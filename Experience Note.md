- langchain原理：
	- 由**models、Agent、Chain、Indexes、Memory、prompt、shema**组成
		- models：LLMs、chatModels、Text Embedding models
			- chatModels：存在**聊天消息的标准接口**，支持AImessage、HumanMessage、Systemmessage、ChatMessage格式
			- Embedding models：高维的离散数据转化为低维连续向量，接着就可以计算相似度、检索等相关操作
			- LLMs：langchain支持大部分常见的LLMs
		- Prompt：见[[prompt engineering]]
		- Indexes：对文档进行结构化，由**Document Loaders、Text Splitters、Vectorstores、Retrievers**组成
		- chains：将多个组件组合在一起，完成更加复杂的任务
		- Memory：记忆组件，可以将提问与回答保存下来，由**ConversationBufferMemory（聊天记录保存在内存中）、ConversationBufferWindowMemory（在第一个基础加入窗口参数，会保存k个聊天记录）、ConversationTokenBufferMemory（在内存中保存最近交互缓冲区，使用token长度确定何时刷新）、ConversationSummaryMemory（保存一个用户和模型聊天内容摘要）、ConversationSummaryBufferMemory(存储一个用户和模型聊天并使用token何时刷新)、VectorStoreRetrieverMemory（所有之前对话通过向量方式存储到向量数据库，每一轮新对话根据输入信息匹配最相似k组对话）**
		- Agents：代理可以访问一套工具，并根据用户输入确定要使用哪些工具。简单的理解为他可以动态的帮我们选择和调用 chain 或者已有的工具。
			- Action Agents：每一个时间步，之前动作的输出决定下一个动作
			- Plan-and-execute agents：预先决定操作顺序，执行所有操作
- LLM复读机问题：**生成文本时重复之前的内容**
	- 原因：存在多种原因，**模型对过去信息过度依赖、模型在处理长序列时的注意力机制失效**。
	- 解决方法：**数据增强**（通过增加训练数据的多样性、复杂性）、**模型改进**、**生成策略**（采用多样性策略，**抽样生成、引入随机性**）
- 大模型处理更长文本：**模型架构、加入内存机制（外部记忆/缓存）、分块（长文本分割更小的部分，之后分别处理）**
- 节省模型内存的方法：
	- **模型剪纸**：移除模型冗余结构和参数、减少内存占用
	- **知识蒸馏**：使用一个**大型教师模型**指导**小型学生模型**，学习大型教师模型，减少内存占用。
	- **量化**：高精度转低精度
	- **模型并行**：将大型模型分割多个设备上进行训练和推理
	- **数据并行**：将训练数据分割到多个设备上，每个设备训练模型的一个副本，减少单个设备的内存需求。
	- **动态批处理**：根据可用内存动态调整批量大小，以适应内存限制。
- linux docket和windows docket：
	- 在linux docket可以直接运行，**无需额外的虚拟化**
	- docket的文件系统使用的是linux上的文件系统，能高效的共享文件。
	- 在Linux上启动速度更快、资源开销更低
	- linux兼容性较windows好很多
	- 适用于大多数基于docket的开发、测试和生产环境
- 划分词的方式：
	- 基于规则的分词：根据语言语法规则和词典进行分词，但覆盖率有限，可能会出现未登录的词
	- 基于统计的分词：利用大量的语料库，通过统计词语共现频率和统计模型来划分词
	- 基于机器学习的分词：利用机器学习模型，根据大量的标注数据来训练自动进行分词
	- 基于深度学习的分词：通过构建神经网络，从数据中学习更为复杂的词语边界信息
	- 基于子词单元的方法：子词可以是词的片段或是更细粒度的单位，减少未登录词问题。
- python的数据类型：int、float、bool、str、list、tuple、set、dict、NoneType
- 多线程和多进程：**多线程**可以在同一进程并发执行多个线程，线程共享内存资源，适合I/O密集型任务，但python通过GIL限制，并不能真正并行。**多进程**通过独立的进程并行执行，每个进程都有独立的内存空间，适合CPU密集型任务，能够充分利用多核CPU，进行真正的并行计算。
- 图片存在鱼眼效果，如何解决：
	- opencv校正，`cv2.undistort`，可以通过相机的内参和畸变系数来进行校正
	- 图片处理软件
	- 投影：因为鱼眼效果只是将直线投影成弧形，但**每个像素的色彩和亮度等信息并没有发生变化**，通过几何投影的方式，将鱼眼图片的坐标重新映射到一个校正后的投影平面上。
- 多线程资源竞争问题、资源共享问题：
	- 资源竞争：多个线程在访问或修改共享资源时，因为线程执行的不确定性，导致程序异常或结果不正确的现象，简单来说就是**多个线程试图同时操作相同的资源**。
		- 多线程并发访问共享资源是发生的原因
		- 通过**互斥锁（确保只有一个线程能够访问共享资源）、信号量等解决**
		- 通过**避免共享资源的直接访问、使用线程安全的数据结构（队列、Hashmap.......）**
	- 资源共享：多个线程同时需要访问某些共享资源如何安全、正确的管理这些资源，确保资源不会因为竞争导致死锁。
		- 通过**读写锁（多个线程可以同时访问，但是只允许一个线程修改）、条件变量**进行解决
		- 通过**资源分片（将共享资源尽量划分多个独立资源，避免多个线程抢夺资源）、减少资源访问次数（尽量减少对共享资源的频繁读写操作，减少出现竞争的可能性）。**
	- 线程通信手段：
		- 共享内存（共享数据）：共享某一个部分内存，访问同一个内存上的数据，**需要互斥锁保证数据一致性，避免竞态条件**
		- 信号量：用于控制访问共享资源的线程数量，一个计数器，用于控制进入临界区的线程数
		- 条件变量：允许线程在满足条件之前阻塞，当另一个线程修改并满足条件，可以通知其他线程
		- 消息队列：线程通过消息队列发送和接收消息
		- 管道：进程间通信方式，也可以用于线程，通过管道传递数据，单向通信方式
		- 事件：通过触发和等待事件来实现线程同步。一些线程在某些条件下触发事件，而另一些线程则在等待该事件的发生。
		- socket
	- 线程和进程之间区别：
		- 资源独立性：进程是资源分配最小单位，每个进程有独立的内存空间，线程是cpu调度最小单位，多个线程共享进程的内存和资源。
		- 通信方式：进程间通信（IPC）复杂，需要使用管道、消息队列、共享内存等；线程间通信较简单，因为它们共享内存。
		- 开销：进程切换开销大，因为要保存和切换整个进程的上下文；线程切换开销较小，因为线程间共享内存，切换时只需切换线程上下文。
		- 崩溃影响：一个进程崩溃不会影响其他进程；但一个线程崩溃可能会导致整个进程崩溃。
	- 同一进程中的多个线程那些资源会共享：
		- 共享资源：**进程的内存空间、文件描述符、进程代码段、全局变量和局部变量、信号处理器、进程的工作目录、用户ID、组ID**
		- 独立资源：**线程的栈（每个线程都独立的栈空间）、寄存器状态（每个线程有独立的cpu寄存器（程序计数器、栈指针））、线程的私有局部变量（局部变量在栈上、线程私有）、线程特有的数据**，PS：但可以通过不安全的指针进行操作。
	- AB两个数，不使用if判断大小：
		- 三目运算符
		- 利用数学运算：![[Pasted image 20250318124923.png|400]]
		- 利用位运算：![[Pasted image 20250318124957.png|400]]
	- LLM不重新训练适应新的数据：都是通过**微调**实现
		- 提示学习：对于NLP模型来说，可以通过prompt engineering适配新数据
		- 参数高效微调（PEFT）：在不更新模型所有参数的前提下，仅调整一小部分参数来适应新任务，如在模型的部分层添加少量可学习参数。
		- 迁移学习：基于已有的大模型，在新任务的数据上微调较高层的少量参数，甚至不调整参数，直接用大模型作为特征提取器。
			- ![[Pasted image 20250318125309.png|400]]
		- 增量学习：通过连续学习新数据，模型能够逐步适应变化，避免遗忘以前的知识。
			- ![[Pasted image 20250318125333.png|400]]
---
deepseek：强化学习+冷启动+多阶段训练，知识蒸馏
- 强化学习：GRPO的强化学习算法
	- 训练不稳定，收敛困难，推理能力受限于训练数据，强化学习的奖励机制会被ai利用，导致泛化能力降低，计算成本高
- 冷启动：通过大模型生成的数据或者收集高质量数据，进行监督微调（SFT），学习基础的内容。
- 知识蒸馏：小模型从大模型学习的技术。
	- 大模型生成高质量数据
	- 小模型学习大模型输出，监督微调（SFT）模型大模型推理过程
	- 优化训练策略
- in-context learning：提供一些问题-答案（示例），让模型根据这些示例中的规律进行推断，并应用这些规律来解答新的问题。