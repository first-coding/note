- **线性模型**：试图学的一个通过属性的线性组合来进行预测的函数。
	- $$f(x)=ω_1x_1+ω_2x_2+...+ω_dx_d+b$$
	- 向量形式：$$f(x)=ω^Tx+b$$
	- 许多功能强大的非线性模型可以在线性模型的基础上通过引入层级结构或高维映射可以得到。
	- **ω可以理解为权值（即每个属性在预测中的重要性）**，因为存在着这个原因，所以线性模型有很好的可解释性。
	- **线性回归试图学得$$f(x_i)=ωx_i+b$$，使得$$f(x_i)=y_i$$，在这个公式中，f(x_i)相当于预测值，y_i相当于真实标签。
		- 衡量这个线性回归的最常用的是[[基础知识#^5c4e9a|均方误差]]。**基于均方误差最小化来进行模型求解的方法称为：最小二乘法。** 
			- **最小二乘法对于线性回归而言就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。**
			- 根据上面的$$f(x_i)=ωx_i+b$$而言，我们需要求出ω和b。使得在ω和b确定f(x_i)的情况下，均方误差最小。这个过程称为**线性回归模型的最小二乘“参数估计”**。
				- **单个线性回归的参数估计的求法同样适用于多元线性回归**
		- **PS：均方误差不止用于线性回归。**
		- sigmoid函数：形状类似S的函数。
- **决策树**：是什么？原理是什么？怎么判断性能如何？又有哪些新名词。
	- 一种常见的监督学习的机器学习算法，一种树形结构，每一个叶子节点代表一个类别，每一条分支代表一种类型的判断方法，根节点代表了样本全集。
	- **什么是最优划分属性？如何划分？**
		- 最优划分属性就是根据每个特征，来进行划分，每次划分后所包含的样本最好属于同一类别（即结点纯度）
		- 结点的纯度通过**信息熵**来衡量。$$Ent(D)=-\sum_{k=1}^{|y|} p_klog_2p_k$$
		- **当信息熵越小的时候，纯度越高。越大的时候，纯度越低**。
		- 那么如何提高划分属性的纯度（即如何找出划分后，纯度提升最大的属性）。通过计算**信息增益**$$Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$$来判断。（D^v代表第v个分支结点包含了D中所有在属性a上的取值为a^v的样本,D代表样本集，V可以理解为有多少种在同一属性中的取值）
			- ![[Pasted image 20240226170438.png]]
			- eg：对于以上的数据集而言，以计算色泽的信息增益为例，
				- 色泽存在3个可能的取值，{青绿，乌黑，浅白}。即有三个子集（即存在三个分支），分别为D为根节点的信息熵，D1(青绿),D2(乌黑),D3(浅白)
				- 在表中，色泽为青绿的有{1,4,6,10,13,17}，正例（即好瓜）的比例是3/6，反例（即坏瓜）的比例是3/6。对于D2，D3也是同理。
				-  $$Ent(D)=-(\frac{8}{17}log_2\frac{8}{17}+\frac{9}{17}log_2\frac{9}{17})=0.998$$这是数据D的信息熵
				-  $$Ent(D^1)=-(\frac{3}{6}log_2\frac{3}{6}+\frac{3}{6}log_2\frac{3}{6})=1.000$$
				-  $$Ent(D^2)=-(\frac{4}{6}log_2\frac{4}{6}+\frac{2}{6}log_2\frac{2}{6})=0.918$$
				-  $$Ent(D^3)=-(\frac{1}{5}log_2\frac{1}{5}+\frac{4}{5}log_2\frac{4}{5})=0.722$$
				-  属性为色泽的信息增益为：$$Gain(D,color)=0.998-(\frac{6}{17}*1.000+\frac{6}{17}*0.918+\frac{5}{17}*0.722)=0.109$$
				-  同理也可以算出其他属性的信息增益。然后通过信息增益由大到小进行选择构成决策树。
			-  **信息增益越大，使用该属性来进行划分所获得的纯度提升越大，即信息熵的降低越快。**
			-  通过信息增益来选择划分属性的方法，即**ID3决策树学习算法。**
			-  另一种是C4.5决策树算法，通过计算增益率，ID3决策树学习算法会对**取值数目较多的属性有所偏好**。
				-  增益率：$$GainRatio(D,a)=\frac{Gain(D,a)}{IV(a)}$$
				-  $$IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$
				-  称为属性a的固有值，取值数目越多（V越大），IV(a）越大。**增益率对取值数目少的属性有所偏好**。所以不是使用增益率来划分属性，而是通过启发式，**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。** 
				-  另一种是**CART决策树**，使用基尼指数来选择。
					-  **基尼值**：$$Gini(D)=\sum_{k=1}^{|y|}\sum_{k'\neq{k}}p_kp_{k'}=1-\sum_{k=1}^{|y|}p_k^2$$
					-  基尼值反应了数据集D中随机抽取两个样本，其类别标记不一致的概率。**Gini越小。数据集D的纯度越高**
					- 属性a的基尼指数为 $$GiniIndex(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$$
					-  **通过基尼指数最小的作为最优划分属性**
	-  **剪枝处理是什么？用来干什么？如果有类型有多少种？各种类型怎么做？**
		-  **剪枝是防止决策树算法过拟合的一种手段。**
		-  剪枝有两种基本策略
			-  **预剪枝**：在决策树生成过程中，对划分的属性进行估计，如果划分不能带来泛化性能提升，停止划分将当前结点标记作为叶结点。
				- 但是如果在这基础上的划分后面可以提供性能的话，然后预剪枝又禁止展开分支，会给决策树带来**欠拟合**的风险。
			-  **后剪枝**：在生成一棵完整的决策树，自底向上对非叶节点进行考察，如果该节点的子树换为叶节点决策树泛化性能能提升，将子树替换为叶结点。
				- 因为后剪枝是先生成了决策树，然后在自底向上的对非叶子节点进行考察，所以训练时间比未剪枝和预剪枝要大得多。
				- 后剪枝生成的决策树，**欠拟合风险很小，泛化性能优于预剪枝决策树。**
	-  **如果遇到的是连续值（数字类型）怎么办？因为之前都是离散型的数据（色泽=乌黑......），如果存在缺失值的话怎么办？**
		- **连续值处理**：对n个不同取值，从小到大排序，基于t将数据集D划分为子集D1,D2，D1比t小，D2比t大。
			- 通过对从小到大排序，候选划分点 $$T_a={\frac{a^i+a^{i+1}}{2}1 \leq i \leq n-1}$$
			-  选择最有划分点通过以下公式$$Gain(D,a)=\max_{t∈T_a}Gain(D,a,t)=\max_{t∈T_a}Ent(D)-\sum_{λ∈{-,+}}\frac{|D_t^λ|}{|D|}Ent(D_y^λ)$$通过判断这个连续值在这个划分点的哪一边决定是属于哪个类别。
		- **缺失值处理**：C4.5决策树通过让同一个样本以不同的概率划分到不同的子节点中去。
	- **多变量决策树**：将每个属性看作是坐标空间的一个坐标轴的话，那么每一个样本就是坐标空间的一个数据点。样本分类就是**在不同类样本之间找到分类边界**，决策树形成的分类边界明显的特点就是**轴平行（它的分类边界由若干个与坐标平行的分段组成）**。
- **神经网络**：是什么？原理是什么？如何判断性能？应用场景？
	- **神经网络**：由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互行为。
	- **神经元模型**：是神经网络最基本的成分。当某个神经元的电位超过一个“阈值”就会被激活。
		-  **机器学习中的神经网络模型沿用M-P神经元模型**：神经元就收来自n个其他神经元传递过来的输入信号，输入信号通过带权值进行连接并传递。将神经元接收到的总输入值和阈值进行对比，通过**激活函数（可以理解为是分段函数）** 处理产生神经元的输出。
			- PS：**常用Sigmoid函数作为激活函数**。
		-  **多个神经元通过一定的层次结构连接起来，就是神经网络。**
		- **感知机**：是什么？有什么作用？
			-  **感知机是由两层神经元组成，可以容易的实现逻辑与、或、非运算。**
			-  ****