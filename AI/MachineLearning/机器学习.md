- **线性模型**：试图学的一个通过属性的线性组合来进行预测的函数。
	- $$f(x)=ω_1x_1+ω_2x_2+...+ω_dx_d+b$$
	- 向量形式：$$f(x)=ω^Tx+b$$
	- 许多功能强大的非线性模型可以在线性模型的基础上通过引入层级结构或高维映射可以得到。
	- **ω可以理解为权值（即每个属性在预测中的重要性）**，因为存在着这个原因，所以线性模型有很好的可解释性。
	- **线性回归试图学得$$f(x_i)=ωx_i+b$$，使得$$f(x_i)=y_i$$，在这个公式中，f(x_i)相当于预测值，y_i相当于真实标签。
		- 衡量这个线性回归的最常用的是[[基础知识#^5c4e9a|均方误差]]。**基于均方误差最小化来进行模型求解的方法称为：最小二乘法。** 
			- **最小二乘法对于线性回归而言就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。**
			- 根据上面的$$f(x_i)=ωx_i+b$$而言，我们需要求出ω和b。使得在ω和b确定f(x_i)的情况下，均方误差最小。这个过程称为**线性回归模型的最小二乘“参数估计”**。
				- **单个线性回归的参数估计的求法同样适用于多元线性回归**
		- **PS：均方误差不止用于线性回归。**
		- sigmoid函数：形状类似S的函数。
- **决策树**：是什么？原理是什么？怎么判断性能如何？又有哪些新名词。
	- 一种常见的监督学习的机器学习算法，一种树形结构，每一个叶子节点代表一个类别，每一条分支代表一种类型的判断方法，根节点代表了样本全集。
	- **什么是最优划分属性？如何划分？**
		- 最优划分属性就是根据每个特征，来进行划分，每次划分后所包含的样本最好属于同一类别（即结点纯度）
		- 结点的纯度通过**信息熵**来衡量。$$Ent(D)=-\sum_{k=1}^{|y|} p_klog_2p_k$$
		- **当信息熵越小的时候，纯度越高。越大的时候，纯度越低**。
		- 那么如何提高划分属性的纯度（即如何找出划分后，纯度提升最大的属性）。通过计算**信息增益**$$Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$$来判断。（D^v代表第v个分支结点包含了D中所有在属性a上的取值为a^v的样本,D代表样本集，V可以理解为有多少种在同一属性中的取值）
			- ![[Pasted image 20240226170438.png]]
			- eg：对于以上的数据集而言，以计算色泽的信息增益为例，
				- 色泽存在3个可能的取值，{青绿，乌黑，浅白}。即有三个子集（即存在三个分支），分别为D为根节点的信息熵，D1(青绿),D2(乌黑),D3(浅白)
				- 在表中，色泽为青绿的有{1,4,6,10,13,17}，正例（即好瓜）的比例是3/6，反例（即坏瓜）的比例是3/6。对于D2，D3也是同理。
				-  $$Ent(D)=-(\frac{8}{17}log_2\frac{8}{17}+\frac{9}{17}log_2\frac{9}{17})=0.998$$这是数据D的信息熵
				-  $$Ent(D^1)=-(\frac{3}{6}log_2\frac{3}{6}+\frac{3}{6}log_2\frac{3}{6})=1.000$$
				-  $$Ent(D^2)=-(\frac{4}{6}log_2\frac{4}{6}+\frac{2}{6}log_2\frac{2}{6})=0.918$$
				-  $$Ent(D^3)=-(\frac{1}{5}log_2\frac{1}{5}+\frac{4}{5}log_2\frac{4}{5})=0.722$$
				-  属性为色泽的信息增益为：$$Gain(D,color)=0.998-(\frac{6}{17}*1.000+\frac{6}{17}*0.918+\frac{5}{17}*0.722)=0.109$$
				-  同理也可以算出其他属性的信息增益。然后通过信息增益由大到小进行选择构成决策树。
			-  **信息增益越大，使用该属性来进行划分所获得的纯度提升越大，即信息熵的降低越快。**
			-  通过信息增益来选择划分属性的方法，即**ID3决策树学习算法。**
			-  另一种是C4.5决策树算法，通过计算增益率，ID3决策树学习算法会对**取值数目较多的属性有所偏好**。
				-  增益率：$$GainRatio(D,a)=\frac{Gain(D,a)}{IV(a)}$$
				-  $$IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$
				-  称为属性a的固有值，取值数目越多（V越大），IV(a）越大。**增益率对取值数目少的属性有所偏好**。所以不是使用增益率来划分属性，而是通过启发式，**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。** 
				-  另一种是**CART决策树**，使用基尼指数来选择。
					-  **基尼值**：$$Gini(D)=\sum_{k=1}^{|y|}\sum_{k'\neq{k}}p_kp_{k'}=1-\sum_{k=1}^{|y|}p_k^2$$
					-  基尼值反应了数据集D中随机抽取两个样本，其类别标记不一致的概率。**Gini越小。数据集D的纯度越高**
					- 属性a的基尼指数为 $$GiniIndex(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$$
					-  **通过基尼指数最小的作为最优划分属性**
	-  **剪枝处理是什么？用来干什么？如果有类型有多少种？各种类型怎么做？**
		-  **剪枝是防止决策树算法过拟合的一种手段。**
		-  剪枝有两种基本策略
			-  **预剪枝**：在决策树生成过程中，对划分的属性进行估计，如果划分不能带来泛化性能提升，停止划分将当前结点标记作为叶结点。
				- 但是如果在这基础上的划分后面可以提供性能的话，然后预剪枝又禁止展开分支，会给决策树带来**欠拟合**的风险。
			-  **后剪枝**：在生成一棵完整的决策树，自底向上对非叶节点进行考察，如果该节点的子树换为叶节点决策树泛化性能能提升，将子树替换为叶结点。
				- 因为后剪枝是先生成了决策树，然后在自底向上的对非叶子节点进行考察，所以训练时间比未剪枝和预剪枝要大得多。
				- 后剪枝生成的决策树，**欠拟合风险很小，泛化性能优于预剪枝决策树。**
	-  **如果遇到的是连续值（数字类型）怎么办？因为之前都是离散型的数据（色泽=乌黑......），如果存在缺失值的话怎么办？**
		- **连续值处理**：对n个不同取值，从小到大排序，基于t将数据集D划分为子集D1,D2，D1比t小，D2比t大。
			- 通过对从小到大排序，候选划分点 $$T_a={\frac{a^i+a^{i+1}}{2}1 \leq i \leq n-1}$$
			-  选择最有划分点通过以下公式$$Gain(D,a)=\max_{t∈T_a}Gain(D,a,t)=\max_{t∈T_a}Ent(D)-\sum_{λ∈{-,+}}\frac{|D_t^λ|}{|D|}Ent(D_y^λ)$$通过判断这个连续值在这个划分点的哪一边决定是属于哪个类别。
		- **缺失值处理**：C4.5决策树通过让同一个样本以不同的概率划分到不同的子节点中去。
	- **多变量决策树**：将每个属性看作是坐标空间的一个坐标轴的话，那么每一个样本就是坐标空间的一个数据点。样本分类就是**在不同类样本之间找到分类边界**，决策树形成的分类边界明显的特点就是**轴平行（它的分类边界由若干个与坐标平行的分段组成）**。
- **神经网络**：是什么？原理是什么？如何判断性能？应用场景？[[深度学习（花书）]]
	- **神经网络**：由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互行为。
	- **神经元模型**：是神经网络最基本的成分。当某个神经元的电位超过一个“阈值”就会被激活。
		-  **机器学习中的神经网络模型沿用M-P神经元模型**：神经元就收来自n个其他神经元传递过来的输入信号，输入信号通过带权值进行连接并传递。将神经元接收到的总输入值和阈值进行对比，通过**激活函数（可以理解为是分段函数）** 处理产生神经元的输出。
			-  $$y=f(\sum_{i=1}^nω_ix_i-θ)$$
			- 权值ω（权值）和θ（阈值）可以通过数据集进行学习。
			- PS：**常用Sigmoid函数作为激活函数**。
		-  **多个神经元通过一定的层次结构连接起来，就是神经网络。**
		- **感知机**：是什么？有什么作用？
			-  **感知机是由两层神经元组成，可以容易的实现逻辑与、或、非运算。**
			-  **PS:感知机只有在输出层神经元进行激活函数处理**
			- 感知机可以解决线性可分的问题，当非线性需要使用多层功能神经元。
		- **多层神经元**：
			-  输入层和输出层之间的层称为**隐层/隐含层**，隐含层和输出层神经元都是拥有激活函数的功能神经元。
			-  **每层神经元和下一层神经元全互联，神经元之间不存在同层连接，也不存在跨层连接。** 这种神经网络结构称为**多层前馈神经网络**。
			-  神经网络是一个模型，而算法就是用来训练神经网络的。
		- **神经网络学习算法**：
			- **误差逆传播算法（BP算法）**：大多数的神经网络都是在使用BP算法来进行训练。BP算法基于梯度下降的策略。
			- **全局最小和局部极小**：若用E表示神经网络在训练集上的误差。神经网络训练过程就是可以看作一个参数寻优的过程。在参数空间中，找出一组最优参数使得E最小。
				-  但是会存在这局部最优和全局最优的区别。
				-  寻找E最小的算法就是**梯度下降的算法**。
		- **其他的神经网络**：
			- **RBF网络**
			- **ART网络**
			- **SOM网络**
			- **级联相关网络**
			- **Elman网络**
			- **Boltzmann机**
- **支持向量机（SVM）**：
	- 支持向量机是一种统计学习的方法
	- 支持向量机在分类中，**找到一个超平面将数据进行划分**
		- 对于一个超平面我们可以使用一个$f(x)=ω^Tx+b$表示，**ω是法向量，决定超平面的方向，b为位移向，决定超平面与原点之间的距离**。记为(ω,b)，样本空间**任意点到超平面距离可以写为$r=\frac{|ω^Tx+b|}{||ω||}$**。
		- **假设超平面能将样本正确分类，就会有6-3![[Pasted image 20241031173807.png]]**
		- **支持向量：距离超平面最近的几个训练样本使得上面的式子成立**
		- **间隔：两个异类支持向量到超平面距离之和$γ=\frac{2}{||ω||}$**，**γ越大，模型泛化能力越强。**
		- 即目标转换成了：**满足6-3中约束的参数ω和b，使得γ最大**。即：![[Pasted image 20241031181044.png]]，**最大化γ仅需最大化$||ω||^{-1}$,等价于最大化$||ω||^{2}$**![[Pasted image 20241031181156.png]]
		- 但因为**6.6是一个凸二次规划的问题（详细自己查阅），我们可以直接使用现成的优化计算包求解（但效率较低）**，所以引入**拉格朗日乘子法（求解有约束下最优化问题，具体自己查阅）转化为对偶问题**，最终拉格朗日函数写为：![[Pasted image 20241031181514.png]]
			- ![[Pasted image 20241031181607.png]]		 
			- 但因为6-11是一个**二次规划问题，可以使用二次规划算法求解，但是因开销过大，转为使用SMO算法来进行计算（![[Pasted image 20241031181839.png]]）**
			- **以上步骤简单来说，即求得最大的间隔，然后不断转化，求得所需要的**
	- **以上我们都是建立在训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类**。
		- 存在非线性可分得情况，**即没有一个超平面可以正确的分类**
		- **对于非线性可分的问题，将样本从原始空间映射到更高维的特征空间，使得样本在这个特征空间线性可分。**
		- **如果原始的空间是有限维的，即属性数有限，一定存在一个高维特征空间使样本可分。**
		- 将x进行映射得到**Φ（x）,超平面对应的模型的为$f(x)=ω^TΦ(x)+b$**,**之后与线性可分一样得到函数。**![[Pasted image 20241031190046.png]]
		- **求解图6.21的方程，需要求解映射的x即$Φ(x_i)^TΦ(x_j)$ ，即 $x_i$与$x_j$映射到特征空间的内积$a.b=\sum{^n_{i=1}}a_ib_i$，n是维度**。
		- 但是存在n**很大（即高维甚至无穷维）**，计算较为困难。**为了避开这个计算可以使用以下方法**![[Pasted image 20241031190623.png]]
		- **紧接着上图所说的核函数和映射，通常在现实任务中，我们无法知道映射具体形式和和函数选择**。
		- **那么什么函数可以作为核函数呢**，通过一个**核函数定理可知：一个[[数学基础知识#^bfc4dc|对称函数]]所对应的核矩阵是[[数学基础知识#^219e87|半正定]]的就可以作为核函数使用**
		- **核函数作为一个SVM的最大变数，需要注意一下**
			- 对于不知道**特征映射的形式时，并不知道什么核函数合适**
			- **核函数不合适的话，意味着将样本映射到了一个不合适的特征空间，会导致模型泛化能力不佳**
			- **核函数可以组合得到**
	- **软间隔SVM**：为什么要引入软间隔？以及有什么作用？
		- **现实任务很难确定核函数使得样本在特征空间是线性可分的，即使是线性可分，不知是否是因为过拟合导致的**。
			- **为了解决这个问题，允许SVM在一些样本上出错。**
		- 软间隔：
			- **即允许某些样本不满足约束**![[Pasted image 20241101153248.png]]![[Pasted image 20241101153430.png]]
			- **每个样本都有一个对应的松弛变量，表示该样本不满足约束的程度**
			- 上面的软间隔支持向量机是一个**二次规划的问题**。通过**拉格朗日乘子法**得![[Pasted image 20241101153541.png]]
			- **软间隔支持向量机转化的对偶问题也可以使用SMO来进行求解**
	- **支持向量机回归（SVR）**：
		- 以**样本（x,y）为例**：
			- 传统回归模型：**直接基于模型输出f(x)与真实输出y之间的差别来计算损失**
			- 支持向量机回归（SVR）：**允许一个ε的偏差，只有模型输出f(x)与真实输出y之间的差别大于ε偏差才计算为损失**
		- **与SVM同理对函数f（x）进行转换得到SVR的对偶问题**：![[Pasted image 20241101154321.png]]![[Pasted image 20241101154343.png]]![[Pasted image 20241101154401.png]]![[Pasted image 20241101154410.png]]![[Pasted image 20241101154422.png]]
		- **核方法**： SVR 能够处理非线性关系。通过核函数，可以将**原始输入数据映射到一个高维特征空间**，从而在**高维空间中**找到一个**线性回归模型**。
- **贝叶斯分类器**：
	- **先验概率**：**在观察到数据之前**，根据已有的知识或假设，对某个事件或参数的发生可能性进行的主观评估。
	- **后验概率**：**在观察到数据之后**，结合新信息更新后的概率估计。
	- **$λ_{ij}$**：是将一个**真实标记为$c_j$** 的样本**误分类为$c_i$** 所产生的损失
	- **基于后验概率$P(C_i|x)$可获得将样本x分类为$c_i$所产生的期望损失**，即**在样本x上的条件风险**![[Pasted image 20241108173029.png]]
	- **目标是寻找一个判定准则h:X->y以最小化总体风险**：![[Pasted image 20241108173112.png]]
	- **贝叶斯判定准则**：**最小化总体风险，只需在每个样本上选择那个能使条件风险最小$R(c|x)$的类别损失。**，即：每个样本x，h能**最小化条件风险**，则**总体风险也将被最小化**。即：![[Pasted image 20241108173415.png]]
		- $h^*(x)$是**贝叶斯最优分类器**，对应的**总体风险$R(h^*)$称为贝叶斯风险**，$1-R(h^*)$反应分类器所能达到**最好性能**。**即：通过机器学习所能产生的模型精度的理论上限**。
		- ![[Pasted image 20241108173840.png]]
		- 通过**贝叶斯判定准则来最小化决策风险，首先要得到后验概率$P(C|X)$**。但是**现实任务通常难以直接获得**，从这个角度，**机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率$P(C|X)$**，有两种策略：
			1.  **判别式模型**：给定x，通过**直接建模$P(c|x)$来预测$c$**
			2. **生成式模式**：先对**联合概率分布$P(x,c)$建模**，然后，**获得$P(c|x)$**![[Pasted image 20241108175320.png]]
				- $P(c)$**是类”先验“概率**，$P(x|c)$**是样本x相对应类标记c的类条件概率**，**$P(x)$是用于归一化的“证据”因子。**
				- **对于给定样本x，证据因子与类标记无关系**，即：转换为了**如何求得$P(c)P(X|c)$**,先验概率可以通过数据的概率分布求得，而**类条件概率关于x所有属性的联合概率，难以计算**
			- 常见的策略为：
				- **假定具有某种确定的概率分布公式**
				- **基于训练样本对概率分布公式的参数进行估计**
			- **参数的估计存在两种流派**：
				- **频率主义学派**：概率代表一个事件在**无限次重复实验中发生的比例**，频率主义学派需要大量的实验数据
				- **贝叶斯学派**：概率反映一个人对一件事的主观信念。在根据样本/实验来更新
				- 以一个简单的例子说明：
					- **判断硬币是否公平（即正面朝上和反面朝上的概率是否一样）**
						- **频率主义学派**：假设你做了1000次，结果得到**正面朝上出现了510次，反面出线了490**，那么**正面朝上概率为0.51，而反面为0.49**，所以说这枚硬币相对公平
						- **贝叶斯学派**：贝叶斯学派允许存在一个**先验的概率，即：你认为硬币公平的概率是90%**，接下来假设做了10次，发现7次正面朝上，3次反面朝上。**通过贝叶斯定理更新概率**。得到更新后的概率，**根据这个更新后的概率判断是否公平**
				- **在这里，我们选择频率主义学派，通过[[深度学习（花书）#^3b5a60|极大似然估计(MLE)]]**
		- **朴素贝叶斯分类器**：对于上面的贝叶斯公式来说，**类条件概率是所有属性的联合概率，难以从有限的训练样本直接估计得到**。朴素贝叶斯分类器通过**属性条件独立性假设，即：假设每个属性独立的对分类结果产生影响**，基于这个假设前提：
			- $P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod^{d}_{i=1}P(x_i|c)$
			- **d为属性数目，$x_i$为x在第i个属性上的取值**
			- 对于所有类别来说P(x)相同，**贝叶斯判定准则为**：![[Pasted image 20241110102403.png]]
			- **朴素贝叶斯分类器训练过程即：基于训练集D来估计类先验概率P(c)，并为每个属性估计条件概率$p(x_i|c)$**.![[Pasted image 20241110102923.png]]以一个例子说明：![[Pasted image 20241110102902.png]]![[Pasted image 20241110103022.png]]![[Pasted image 20241110103118.png]]![[Pasted image 20241110103127.png]]
			- **以上的例子，在样本数目足够多的时候才能进行有意义的估计，以及在计算好瓜与否的时候通常取对数将连乘变为连加，避免数值下溢**![[Pasted image 20241110103306.png]]
		- **半朴素贝叶斯分类器**：即对**属性条件独立性假设进行一定放松，大部分与朴素贝叶斯分类器没什么区别**，详见互联网或者西瓜书154
- **集成学习/多分类器系统**：通过构建并结合多个学习器来完成学习任务。
	- **同质**：同一种学习算法结合起来的，**个体学习器也称“基学习器”**。
	- **异质**：集成中的个体学习器由**不同学习算法生成**。
	- **集成学习通过将多个弱学习器结合，可以获得好的性能**
	- **得到泛化性能强的集成，集成中的个体学习器尽可能相互独立**
	- 个体学习器的生成方式分两类：
		- **个体学习器存在强依赖关系、必须串行生成的序列化方法**，Boosting
		- **个体学习器不存在强依赖关系，可以同时生成的并行化方法**，Bagging和“随机森林”
	- **Boosting**：是**一族**可将弱学习器提升为强学习器的算法，工作机制类似：
		- 从初始训练集训练基学习器
		- 通过基学习器表现对训练样本分布进行调整，**使先前基学习器做错的训练样本在后续受到更多关注**
		- 这一族算法有：
			- AdaBoost、GB（梯度提升）、BrownBoost、LogitBoost、LPBoost等
			- **AdaBoost较为常用且著名**：详情移步到西瓜书173
	- **Bagging与随机森林**：
		- 集成中个体学习器尽可能相互独立**现实任务无法做到**，可以通过使基学习器尽可能具有较大的差异。**可以对样本进行采样，得到不同的子集，通过不同子集得到基学习器**。但是会存在数据较为重叠（**即：大部分数据都会集中在某一部分，导致基学习器性能较差** ）
		- Bagging是并行式集成学习方法最著名的代表。可以使用**自助采样法进行采样（即有放回采样，详情见[[基础知识#^79cf4a|自助法]]）**。
	- **随机森林（决策树+Bagging+随机属性选择）**：
		- **随机属性选择**：每棵决策树**随机使用部分特征**，而不是全部特征都用于生成决策树。
		- **简单、容易实现、计算开销相对较小，在许多现实任务中展现出强大的性能，代表集成学习技术水平的方法。**
	- **学习器结合会带来三个好处**：
		 1. 假设空间大，单学习器存在泛化性能不佳
		 2. 单个学习器可能**陷入局部最优，但是这个局部最优的泛化性能可能会差**
		 3. 假设空间会扩大，学习器能学到更好的近似
		- 结合策略分为以下几种：
			- **平均法**：
				- 数值型的输出
- **k近邻算法**：
- **聚类**：是一种无监督学习。

---
- **降维算法**：分为**线性降维和非线性降维**
	- **线性降维**：
		- PCA（主成分分析）：
		- LDA（线性判别分析）：
		- ICA（独立成分分析）：
		- FA（因子分析）：
		- Random Projection（随机投影）：
		- PFA（主特征分析）：
	- **非线性降维**：
		- Kernel PCA（核主成分分析）：
		- t-SNE：
		- 自编码器：
		- MDS（多维尺度法）：
		- LLE（局部线性嵌入）：
		- Isomap：
		- UMAP：
		- FM（因子分解机）：
