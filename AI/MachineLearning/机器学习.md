- **线性模型**：试图学的一个通过属性的线性组合来进行预测的函数。
	- $$f(x)=ω_1x_1+ω_2x_2+...+ω_dx_d+b$$
	- 向量形式：$$f(x)=ω^Tx+b$$
	- 许多功能强大的非线性模型可以在线性模型的基础上通过引入层级结构或高维映射可以得到。
	- **ω可以理解为权值（即每个属性在预测中的重要性）**，因为存在着这个原因，所以线性模型有很好的可解释性。
	- **线性回归试图学得$$f(x_i)=ωx_i+b$$，使得$$f(x_i)=y_i$$，在这个公式中，f(x_i)相当于预测值，y_i相当于真实标签。
		- 衡量这个线性回归的最常用的是[[基础知识#^5c4e9a|均方误差]]。**基于均方误差最小化来进行模型求解的方法称为：最小二乘法。** 
			- **最小二乘法对于线性回归而言就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。**
			- 根据上面的$$f(x_i)=ωx_i+b$$而言，我们需要求出ω和b。使得在ω和b确定f(x_i)的情况下，均方误差最小。这个过程称为**线性回归模型的最小二乘“参数估计”**。
				- **单个线性回归的参数估计的求法同样适用于多元线性回归**
		- **PS：均方误差不止用于线性回归。**
		- sigmoid函数：形状类似S的函数。
- **决策树**：是什么？原理是什么？怎么判断性能如何？又有哪些新名词。
	- 一种常见的监督学习的机器学习算法，一种树形结构，每一个叶子节点代表一个类别，每一条分支代表一种类型的判断方法，根节点代表了样本全集。
	- **什么是最优划分属性？如何划分？**
		- 最优划分属性就是根据每个特征，来进行划分，每次划分后所包含的样本最好属于同一类别（即结点纯度）
		- 结点的纯度通过**信息熵**来衡量。$$Ent(D)=-\sum_{k=1}^{|y|} p_klog_2p_k$$
		- **当信息熵越小的时候，纯度越高。越大的时候，纯度越低**。
		- 那么如何提高划分属性的纯度（即如何找出划分后，纯度提升最大的属性）。通过计算**信息增益**$$Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$$来判断。（D^v代表第v个分支结点包含了D中所有在属性a上的取值为a^v的样本,D代表样本集，V可以理解为有多少种在同一属性中的取值）
			- ![[Pasted image 20240226170438.png]]
			- eg：对于以上的数据集而言，以计算色泽的信息增益为例，
				- 色泽存在3个可能的取值，{青绿，乌黑，浅白}。即有三个子集（即存在三个分支），分别为D为根节点的信息熵，D1(青绿),D2(乌黑),D3(浅白)
				- 在表中，色泽为青绿的有{1,4,6,10,13,17}，正例（即好瓜）的比例是3/6，反例（即坏瓜）的比例是3/6。对于D2，D3也是同理。
				-  $$Ent(D)=-(\frac{8}{17}log_2\frac{8}{17}+\frac{9}{17}log_2\frac{9}{17})=0.998$$这是数据D的信息熵
				-  $$Ent(D^1)=-(\frac{3}{6}log_2\frac{3}{6}+\frac{3}{6}log_2\frac{3}{6})=1.000$$
				-  $$Ent(D^2)=-(\frac{4}{6}log_2\frac{4}{6}+\frac{2}{6}log_2\frac{2}{6})=0.918$$
				-  $$Ent(D^3)=-(\frac{1}{5}log_2\frac{1}{5}+\frac{4}{5}log_2\frac{4}{5})=0.722$$
				-  属性为色泽的信息增益为：$$Gain(D,color)=0.998-(\frac{6}{17}*1.000+\frac{6}{17}*0.918+\frac{5}{17}*0.722)=0.109$$
				-  同理也可以算出其他属性的信息增益。然后通过信息增益由大到小进行选择构成决策树。
			-  **信息增益越大，使用该属性来进行划分所获得的纯度提升越大，即信息熵的降低越快。**
			-  通过信息增益来选择划分属性的方法，即**ID3决策树学习算法。**
			-  另一种是C4.5决策树算法，通过计算增益率，ID3决策树学习算法会对**取值数目较多的属性有所偏好**。
				-  增益率：$$GainRatio(D,a)=\frac{Gain(D,a)}{IV(a)}$$
				-  $$IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$
				-  称为属性a的固有值，取值数目越多（V越大），IV(a）越大。**增益率对取值数目少的属性有所偏好**。所以不是使用增益率来划分属性，而是通过启发式，**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。** 
				-  另一种是**CART决策树**，使用基尼指数来选择。
					-  **基尼值**：$$Gini(D)=\sum_{k=1}^{|y|}\sum_{k'\neq{k}}p_kp_{k'}=1-\sum_{k=1}^{|y|}p_k^2$$
					-  基尼值反应了数据集D中随机抽取两个样本，其类别标记不一致的概率。**Gini越小。数据集D的纯度越高**
					- 属性a的基尼指数为 $$GiniIndex(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$$
					-  **通过基尼指数最小的作为最优划分属性**
	-  **剪枝处理是什么？用来干什么？如果有类型有多少种？各种类型怎么做？**
		-  **剪枝是防止决策树算法过拟合的一种手段。**
		-  剪枝有两种基本策略
			-  **预剪枝**：在决策树生成过程中，对划分的属性进行估计，如果划分不能带来泛化性能提升，停止划分将当前结点标记作为叶结点。
				- 但是如果在这基础上的划分后面可以提供性能的话，然后预剪枝又禁止展开分支，会给决策树带来**欠拟合**的风险。
			-  **后剪枝**：在生成一棵完整的决策树，自底向上对非叶节点进行考察，如果该节点的子树换为叶节点决策树泛化性能能提升，将子树替换为叶结点。
				- 因为后剪枝是先生成了决策树，然后在自底向上的对非叶子节点进行考察，所以训练时间比未剪枝和预剪枝要大得多。
				- 后剪枝生成的决策树，**欠拟合风险很小，泛化性能优于预剪枝决策树。**
	-  **如果遇到的是连续值（数字类型）怎么办？因为之前都是离散型的数据（色泽=乌黑......），如果存在缺失值的话怎么办？**
		- **连续值处理**：对n个不同取值，从小到大排序，基于t将数据集D划分为子集D1,D2，D1比t小，D2比t大。
			- 通过对从小到大排序，候选划分点 $$T_a={\frac{a^i+a^{i+1}}{2}1 \leq i \leq n-1}$$
			-  选择最有划分点通过以下公式$$Gain(D,a)=\max_{t∈T_a}Gain(D,a,t)=\max_{t∈T_a}Ent(D)-\sum_{λ∈{-,+}}\frac{|D_t^λ|}{|D|}Ent(D_y^λ)$$通过判断这个连续值在这个划分点的哪一边决定是属于哪个类别。
		- **缺失值处理**：C4.5决策树通过让同一个样本以不同的概率划分到不同的子节点中去。
	- **多变量决策树**：将每个属性看作是坐标空间的一个坐标轴的话，那么每一个样本就是坐标空间的一个数据点。样本分类就是**在不同类样本之间找到分类边界**，决策树形成的分类边界明显的特点就是**轴平行（它的分类边界由若干个与坐标平行的分段组成）**。
- **神经网络**：是什么？原理是什么？如何判断性能？应用场景？[[深度学习（花书）]]
	- **神经网络**：由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互行为。
	- **神经元模型**：是神经网络最基本的成分。当某个神经元的电位超过一个“阈值”就会被激活。
		-  **机器学习中的神经网络模型沿用M-P神经元模型**：神经元就收来自n个其他神经元传递过来的输入信号，输入信号通过带权值进行连接并传递。将神经元接收到的总输入值和阈值进行对比，通过**激活函数（可以理解为是分段函数）** 处理产生神经元的输出。
			-  $$y=f(\sum_{i=1}^nω_ix_i-θ)$$
			- 权值ω（权值）和θ（阈值）可以通过数据集进行学习。
			- PS：**常用Sigmoid函数作为激活函数**。
		-  **多个神经元通过一定的层次结构连接起来，就是神经网络。**
		- **感知机**：是什么？有什么作用？
			-  **感知机是由两层神经元组成，可以容易的实现逻辑与、或、非运算。**
			-  **PS:感知机只有在输出层神经元进行激活函数处理**
			- 感知机可以解决线性可分的问题，当非线性需要使用多层功能神经元。
		- **多层神经元**：
			-  输入层和输出层之间的层称为**隐层/隐含层**，隐含层和输出层神经元都是拥有激活函数的功能神经元。
			-  **每层神经元和下一层神经元全互联，神经元之间不存在同层连接，也不存在跨层连接。** 这种神经网络结构称为**多层前馈神经网络**。
			-  神经网络是一个模型，而算法就是用来训练神经网络的。
		- **神经网络学习算法**：
			- **误差逆传播算法（BP算法）**：大多数的神经网络都是在使用BP算法来进行训练。BP算法基于梯度下降的策略。
			- **全局最小和局部极小**：若用E表示神经网络在训练集上的误差。神经网络训练过程就是可以看作一个参数寻优的过程。在参数空间中，找出一组最优参数使得E最小。
				-  但是会存在这局部最优和全局最优的区别。
				-  寻找E最小的算法就是**梯度下降的算法**。
		- **其他的神经网络**：
			- **RBF网络**
			- **ART网络**
			- **SOM网络**
			- **级联相关网络**
			- **Elman网络**
			- **Boltzmann机**
- **支持向量机（SVM）**：
	- 支持向量机是一种统计学习的方法
	- 支持向量机在分类中，**找到一个超平面将数据进行划分**
		- 对于一个超平面我们可以使用一个$f(x)=ω^T+b$表示，**ω是法向量，决定超平面的方向，b为位移向，决定超平面与原点之间的距离**。记为(ω,b)，样本空间**任意点到超平面距离可以写为$r=\frac{|ω^Tx+b|}{||ω||}$**。
		- **假设超平面能将样本正确分类，就会有6-3![[Pasted image 20241031173807.png]]**
		- **支持向量：距离超平面最近的几个训练样本使得上面的式子成立**
		- **间隔：两个异类支持向量到超平面距离之和$γ=\frac{2}{||ω||}$**，**γ越大，模型泛化能力越强。**
		- 即目标转换成了：**满足6-3中约束的参数ω和b，使得γ最大**。即：![[Pasted image 20241031181044.png]]，**最大化γ仅需最大化$||ω||^{-1}$,等价于最大化$||ω||^{2}$**![[Pasted image 20241031181156.png]]
		- 但因为**6.6是一个凸二次规划的问题（详细自己查阅），我们可以直接使用现成的优化计算包求解（但效率较低）**，所以引入**拉格朗日乘子法（求解有约束下最优化问题，具体自己查阅）转化为对偶问题**，最终拉格朗日函数写为：![[Pasted image 20241031181514.png]]
			- ![[Pasted image 20241031181607.png]]		 
			- 但因为6-11是一个**二次规划问题，可以使用二次规划算法求解，但是因开销过大，转为使用SMO算法来进行计算（![[Pasted image 20241031181839.png]]）**
			- **以上步骤简单来说，即求得最大的间隔，然后不断转化，求得所需要的**
	- **以上我们都是建立在训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类**。
		- 存在非线性可分得情况，**即没有一个超平面可以正确的分类**
		- **对于非线性可分的问题，将样本从原始空间映射到更高维的特征空间，使得样本在这个特征空间线性可分。**
		- **如果原始的空间是有限维的，即属性数有限，一定存在一个高维特征空间使样本可分。**
		- 将x进行映射得到**Φ（x）,超平面对应的模型的为$f(x)=ω^TΦ(x)+b$**,**之后与线性可分一样得到函数。**![[Pasted image 20241031190046.png]]
		- **求解图6.21的方程，需要求解映射的x即$Φ(x_i)^TΦ(x_j)$ ，即 $x_i$与$x_j$映射到特征空间的内积$a.b=\sum{^n_{i=1}}a_ib_i$，n是维度**。
		- 但是存在n**很大（即高维甚至无穷维）**，计算较为困难。**为了避开这个计算可以使用以下方法**![[Pasted image 20241031190623.png]]
		- **紧接着上图所说的核函数和映射，通常在现实任务中，我们无法知道映射具体形式和和函数选择**。
		- **那么什么函数可以作为核函数呢**，通过一个**核函数定理可知：一个[[数学基础知识#^bfc4dc|对称函数]]所对应的核矩阵是[[数学基础知识#^219e87|半正定]]的就可以作为核函数使用**
		- **核函数作为一个SVM的最大变数，需要注意一下**
			- 对于不知道**特征映射的形式时，并不知道什么核函数合适**
			- **核函数不合适的话，意味着将样本映射到了一个不合适的特征空间，会导致模型泛化能力不佳**
			- **核函数可以组合得到**
- **贝叶斯分类器**：
- **k近邻算法**：
- **集成学习**：通过构建并结合多个学习器来完成学习任务。
- **聚类**：是一种无监督学习。

---
- **降维算法**：分为**线性降维和非线性降维**
	- **线性降维**：
		- PCA（主成分分析）：
		- LDA（线性判别分析）：
		- ICA（独立成分分析）：
		- FA（因子分析）：
		- Random Projection（随机投影）：
		- PFA（主特征分析）：
	- **非线性降维**：
		- Kernel PCA（核主成分分析）：
		- t-SNE：
		- 自编码器：
		- MDS（多维尺度法）：
		- LLE（局部线性嵌入）：
		- Isomap：
		- UMAP：
		- FM（因子分解机）：
