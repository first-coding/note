- 机器学习：类似人通过对经验的利用，对新情况做出有效的决策。在机器学习领域，**经验->数据，学习算法基于数据得出模型（从数据中学到的结果）**
	- 每一条数据就是一条记录，记录的集合就是**数据集**。
	- 一条记录是关于一个事件或对象的描述。
		- eg: 西瓜的根蒂、敲声、色泽就是**属性**。属性上的值就是**属性值**，每一条记录就是对一个西瓜的描述。
		- 属性的值形成的空间就是**属性空间，样本空间**
		- 在**属性空间中**的每一条记录都可以找到对应的点。每个点就是**特征向量**。
	- 从数据中学到模型的过程称为**学习/训练**。
		- 用于学习/训练的数据集就是**训练集**
		- 训练集中的每一条数据就是**训练样本**
		- 特征值得到的是**标签**，有标签的记录是一个**样例（xi,yi）** yi表示xi的标记，y是所有标记集合。称为**标记空间**
	- 连续值和离散值：
		- 连续值：eg:0.95，0.37
		- 离散值：eg:西瓜分类是好瓜还是坏瓜，好瓜或者坏瓜就是离散值
	- 是否有标记信息：监督学习和无监督学习。
	- **泛化能力**：模型适用于新样本的能力。
	- 归纳和演绎：
		- 归纳：从具体事实归结出一般性规律，从样例中学习->规律->归纳学习。
			- 归纳学习：
				- 广义：从样例中学习
				- 狭义：从训练数据中学的概念（概念学习（研究少，应用少））
	- **假设空间**：eg:y=ax+b，a和b存在多种可能的集合就是**假设空间**，学习的目的就是找出最佳的线性代数，即最符合训练数据的a和b。
		- **版本空间**：在有限的训练集中，存在着多个最佳的，这个就是**版本空间**。即存在多个函数与训练的数据相对应。
		- **归纳偏好**：在版本空间中，不同的函数对于新预测的数据得出不同的结果，不同的算法有不同的选择，有两种偏好
			- **尽可能特殊**：适用情况尽可能少
			- **尽可能一般**：使用情况尽可能多
			- 如何选择：奥卡姆剃刀：**若有多个假设与观察一致，选择最简单那个**。

- **模型评估**：
	- **训练误差**：学习器在**训练集**上的误差。
	- **泛化误差**：学习器在**新样本**上的误差。
		- 当训练误差过小，也就是说学习器把训练样本自身的特殊的特点当作所有潜在样本具有的一般特点的时候，会导致泛化性能下降。这种现象叫做**过拟合**。过拟合无法避免，只能缓解。
		- 与之相反，当训练误差很大，也就是对训练样本的一般特点尚未学好的，这种现象叫做**欠拟合**。可以解决。
		- eg：对树叶进行训练，**过拟合**就是误以为树叶必须有锯齿（但是有些树叶没有锯齿）。**欠拟合**就是认为绿色都是树叶。
	- **P和NP问题**：
		- P问题（可解问题）：P代表“多项式时间”，**指的是在多项式时间内能解决的问题**。如果一个算法的运行时间可以用问题的规模多项式去界定，就认为这个算法高效。
		- NP问题（非确定性多项式时间问题）：NP代表“非确定性多项式时间“，指的是在多项式时间内验证一个解是否正确的问题。**如果一个解可以在多项式时间内被验证，虽然找到解的过程可能并不高效，那么这个问题就被认为是NP问题**。至今尚未找到能够在多项式时间内解决所有NP问题的算法。
		- 在计算机科学中，P和NP问题是否等价没有一个确定的答案。
	- 评估方法：通过**测试集**来测试学习器对新样本的判别能力，以测试集上的**测试误差**作为**泛化误差**的近似。但是测试样本尽量不在训练集中出现，未在训练过程中使用过。
		- 一般的**训练集**和**测试集**都是在数据集中进行划分。有三种方法
			- **留出法**：直接将数据集D划分为两个互斥的集合。一个集合训练集，另一个作为测试集。但是**训练/测试集的划分尽可能保持数据分布的一致性**。尽量分层采样，避免出现训练集和测试集都是正例和反例的情况，尽量1：1。单次留出法得出的估计结果不够稳定可靠，在使用留出法时，一般要采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。**常见做法大约2/3~4/5的样本用于训练**。
			- **交叉验证法**：划分子集，k-1个子集训练剩下测试。将数据集D划分为K个大小相似的互斥子集。