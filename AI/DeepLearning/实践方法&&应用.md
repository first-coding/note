### 要想成功做出深度学习的项目，仅局限于存在什么算法和解决原理是不够的。

#在机器学习日常开发中需要决定以下几点： 
	[[#^b1f5db|决定是否收集更多数据]]
	[[实践方法&&应用#^f52938|是否增加/减少模型容量]]
	#是否添加/删除正则化项 
	#是否需要改进模型 

### 推荐实践设计流程：
	确定目标：
		使用什么样的误差度量，并为误差度量指定目标值。这些取决于实际的应用
		搭建一个端到端的工作流程，包括估计合适的性能度量
		搭建系统，并确定性能瓶颈。
			检查哪个部分的性能差于预期，以及是否是因为过拟合、欠拟合，或者数据或软件缺陷造成的。
		根据具体观察反复地进行增量式的改动。
			收集新数据、调整超参数或改进算法。
### [[基础知识#^1aa288|性能度量]]
### 默认的基准模型：
	
	根据问题复杂性选择模型，简单的线性权重就可以解决，使用逻辑回归这一类的就好了，对于图像识别这些用深度学习模型会更还
	根据数据的结构选择一类合适的模型
		固定大小的向量作为输入，使用全连接的前馈网络
		已知拓扑结构（例如图像），可以使用卷积神经网络
		输入或输出是一个序列，可以使用门控循环网络（LSTM/GRU）
	优化器的选择：SGD/Adam......
	正则化，提前终止，Dropout，批标准化等优化的选择。
	根据项目决定是否使用无监督学习。
### 决定是否收集更多数据 ^b1f5db
	判断训练集上的性能可否接受
		模型在训练集上的性能就很差，学习算法无法在训练集中学习出良好的模型，那么没必要收集更多数据
			应该尝试增加更多的网络层或每层增加更多的隐藏单元
			也可以通过调整学习率等超参数的措施改进学习算法
			如果更大的模型和优化算法效果不佳，问题可能和训练集的质量有关
		训练集上性能可以接收，开始度量测试集上的性能。
			如果存在测试集上的性能比训练集要差，那么收集更多的数据是最有效的解决方案之一
				但是要考虑收集更多数据的代价和可行性
			降低模型大小或改进正则化（调整超参数）

### 选择超参数 ^f52938
	手动选择：需要了解超参数做了什么，以及如何设置才能更好泛化
	自动选择：减少了手动选择的问题，但是伴随着更高的计算成本
		自动超参数优化算法：理想的学习算法应该是只需要输入一个数据集，就可以输出学习的函数。
			支持向量机和逻辑回归等这种算法，只需要一到两个超参数需要调整，它们也能表现出不错的性能。
			存在着一些超参数算法。
	网格搜索：对于每个超参数，选择一个有限值集去选择，通过笛卡尔乘积得到一组组的超参数，使用每组超参数去训练，挑选验证集误差最小的超参数作为最好的超参数。
	
	随机搜索：通过随机抽样来搜索参数空间的方法。相对于网格搜索，随机搜索的优点在于它不需要穷尽搜索整个参数空间，因此在参数空间较大时更具有可扩展性和效率。随机搜索可能无法保证找到全局最优解，但通常能在有限时间内找到足够好的解决方案。通过概率分布去选择超参数。
		均匀采样：在给定区间内每个可能数值被选择概率相同。
	贝叶斯优化：是一种基于贝叶斯理论的优化方法，它通过建立一个代理模型（通常是高斯过程或树形模型）来近似目标函数，然后利用后验分布来指导下一步参数选择的过程。贝叶斯优化在进行参数选择时能够利用先前的观察结果，因此通常比随机搜索和网格搜索更有效率。
![[db21bce44cac8f54768f46f7072e1c3.jpg]]
	
### 调试策略
	
	当一个机器学习系统效果不好时，很难判断是算法本身，还是实现的算法出错。
	另一个难点，大部分机器学习模型有多个自适应的部分。
	大部分神经网络的调试策略都是解决两个难题一个或两个：
		设计足够简单的情况，能够提前得到正确结果，判断模型预测是否与之相符
		设计一个测试，独立检查神经网络实现的各个部分。
	一些重要的调试检测：
		可视化计算中模型的行为：
			当训练模型检测图像中的对象时，查看一些模型检测到部分重叠的图像
			训练语音生成模型，试听一些生成的语音样本
		可视化最严重错误
		根据训练和测试误差检测软件
		拟合极小的数据集：
			当训练集有很大误差的时候，我们需要确定是否是真正欠拟合，还是其他问题。
				通常，即使是小模型也可以保证很好拟合一个足够小的数据集。可以用足够小的数据集进行测试。
		比较反向传播导数和数值导数
		监控激活函数值和梯度的直方图
### 应用：
	 CPU和GPU：GPT相当于与CPU有极高的内存带宽，可以并行运算。
	 级联：多个模型按顺序连接在一起
	 对于单台机器来说，计算资源受限。
		 将训练/推断任务分摊到多个机器上进行
			 数据并行：每一个输入的样本都可以在单独的机器运行
			 模型并行：多个机器共同运行一个数据点，每一个机器负责模型的一部分，对于推断和训练，都是可行的。
			 在训练过程中，数据并行因为梯度下降算法原因：第t步的梯度是第t-1步得到参数的函数。是一个完全串行的过程
				 需要用异步随机梯度下降来进行训练。
	模型压缩：用一个更小的模型代替原始耗时的模型，使得存储与评估所需内存与运行时间更少。
		当原始模型的规模很大，我们需要防止过拟合。模型压缩就有存在的作用了。
		在多数情况下，拥有最小泛化误差的模型往往是多个独立训练而成的模型的集成。
	动态数据：模型的结构在运行时会根据输入数据/条件进行调整/变化。
		通过选通器（可以在模型训练/推理动态选择不同信息通道/模块），在给定当前输入的情况下使用几个专家网络的哪一个来计算。
		开关：隐藏单元可以根据具体情况从不同单元接收输入。这种方法可以理解为注意力机制（即加权平均，基于加权平均的注意力机制是平滑、可微的，近似）
### 计算机视觉：
	预处理：通常只需要相对少的预处理。
		标准化（每个像素都在相同并且合理的范围内）
		图像格式为相同的比例（大部分的计算机视觉都需要标准尺寸的图像，通常需要裁剪或者缩放来适应大小）。但是在卷积模型中不一定要统一大小
		数据集增强（看作一种只对训练集做预处理的方式），是减少大多数计算机视觉模型泛化误差的一种极好的方法。
		对比度归一化：对比度指的是图像中亮像素和暗像素之间差异的大小。深度学习中，对比度指的是图像/图像区域中像素的标准差。

$X_{i,j,1}$ 表示第i行第j列红色的强度
$X_{i,j,2}$ 表示第i行第j列绿色的强度
$X_{i,j,3}$ 表示第i行第j列蓝色的强度
整个图像对比度表示为![[Pasted image 20240428154932.png]]![[Pasted image 20240428154958.png]]
	**全局对比度归一化(GCN)**：通过从每个图像中减去其平均值，然后重新缩放使其像素上的标准差等于某个常数s。![[Pasted image 20240428155124.png]]
	**局部对比归一化(LCN)**：确保对比度在每个小窗口上被归一化，而不是整体在图像归一化。
		局部对比度归一化各种定义都是可行的：
			**在所有情况下**，可以通过减去邻近像素的平均值并除以邻近像素的标准差来修改每个像素。
			**在一些情况下**，要计算以当前要修改的像素为中心的矩阵窗口中所有像素的平均值和标准差。
			**在其他情况下**，要修改的像素为中心的高斯权重的加权平均和加权标准差。
	![[Pasted image 20240428155234.png]]
### 语音识别
	将一段包括了自然发音的声学信号投影到对应说话人的词序列上。

**隐马尔可夫模型（HMM）和高斯混合模型（GMM）**：
	GMM对声学特征和音素（根据语音自然属性划分最小语音单位）之间的关系建模
	HMM对音素序列建模。
		首先，HMM生成一个音素序列以及离散的子音素状态。接着，GMM把每一个离散的状态转化为一个简短的声音的信号。
	随着模型的发展，可以通过神经网络代替。用受限玻尔兹曼机无向概率模型。神经网络每一层都是通过训练受限玻尔兹曼机来初始化。

### 自然语言处理（NLP）
	n-gram:是一种简单但有效的统计语言模型。它根据文本中连续出现的n个词（或字符）来预测下一个词（或字符）。比如，对于二元（2-gram）模型，就是考虑相邻的两个词，通过统计它们的出现频率来预测下一个词的可能性。
	
	神经语言模型：用于克服n-gram的维度灾难，它通过输入文本序列来训练神经网络，使得网络能够理解语言的结构和语义，并能够根据上下文生成合理的文本。这种模型通常比传统的n-gram模型更复杂，能够处理更长的上下文信息，从而在语言建模等任务上取得更好的性能。
	
	n-gram和神经语言模型结合：利用n-gram模型来提供神经语言模型的初始输入或上下文信息。
		在使用神经语言模型生成文本时，可以首先使用n-gram模型生成一个初始词序列，然后将这个序列作为神经语言模型的输入，以生成更加连贯和合理的文本。

### 推荐系统：
	协同过滤算法：假设1和2都喜欢项目A,B和C，我们可以推断它1和2具有相同的新区，如果1喜欢D，可以推荐D给2.
	但是存在一个基本限制：当引入新项目或新用户时，缺乏评级历史意味着无法评估其与其他项目或用户的相似性/无法评估新的用户和现有项目的联系。称为冷处理推荐问题。可以通过基于内容的推荐系统解决
		基于内容的推荐系统：引入单个用户和项目的额外信息，可以是用户基本信息或每个项目的特征。
		也可以通过强化学习进行推荐：通过探索与利用，
			探索是采取行动以获得更多的训练数据，如果我们知道给定上下文x，动作a给与我们1的奖励，但是不知道是不是最好的奖励，所以我们会继续采取行动继续。
			利用从目前学到的最好的策略采取动作，也就是我们所知的将获得高奖励的动作。
### 知识表示、推理和回答
	知识表示：通常是通过词嵌入来实现的。词嵌入是将词汇映射到高维向量空间中的表示方法，使得具有相似含义的词在向量空间中距离较近。通过深度学习技术，可以学习到更加丰富和抽象的词嵌入，以更好地表示词汇之间的语义关系。
	
	推理：是利用神经网络模型来推断出给定前提的逻辑结论。
	
	回答：是利用神经网络模型来回答给定问题。这可能涉及到阅读理解任务，即从给定的文本中找到答案，也可能涉及到开放域问答，即从全文中找到答案。深度学习模型可以通过学习大量问答对来理解问题和答案之间的关系，并从文本中提取相关信息来生成答案。

	
