- 数学基础：线性代数，概率论，高等数学（微积分）
- 基本概念/算法：
	- 优化：通过改变自变量（x），最小化或最大化函数f(x)的过程。
	- 在模型优化过程中，**目标函数/准则**：最大化或最小化的函数。当对目标函数进行最小化的时候，我们也称为**代价函数，损失函数，误差函数**。
	- 在对数据集进行建模过程中，**代价函数可以理解为对模型进行性能度量的函数**，**优化模型就是让这个代价函数最小化或者最大化**，**目标函数就是我们学习得到的一个函数，这个函数可以在数据集上有良好的表现。**
	- **激活函数**：可以理解为在神经网络中，**具有一定计算能力的神经元。决定在这个神经元中的输入与输出的关系。** 常见的激活函数有：**Relu，Sigmod等等**。
	- 在优化过程中，就是存在很多的方法，例如**梯度下降，随机梯度下降**等等。
	- **凸函数和非凸函数**：
		- 凸函数：在该区间函数图象上的任意两点所连成的线段上的每一个点都位于函数图象的下方(或上方)。![[Pasted image 20240307183439.png]]
		- 非凸函数：函数在该区间上有多个极值,即系统有多个稳定的平衡态。![[Pasted image 20240307183450.png]]
	- 正则化：可以简单的理解为在损失函数f(x) 中加上了一定的项，从而降低模型过拟合的概率。
	- 饱和函数：当输入值很大或者很小的时候，输出值趋于平缓的函数。这种函数会把梯度变得很小（即参数空间很小，因为趋于平缓没有太大的变化空间）
	- 泛函：一个/多个函数到一个实数的映射，就是相当于说函数是自变量x，映射出来的一个实数就是因变量y。
	- 仿射变换：几何中，对一个向量空间进行一次线性变化，并加上平移变换另一个向量空间
	- 线性变化：指在两个向量空间之间保持向量加法和标量乘法的函数。
	- 协方差矩阵：是衡量多个随机变量之间相关性的指标
	- 非归一化：就是和不等于1的。
- 优化算法和训练算法：![[Drawing 2024-03-08 17.26.19.excalidraw]]优化算法的目标就是让模型更优，那就存在一些算法来进行一系统的操作（即一堆数学方面的转换），就比如**梯度下降算法（不仅是训练算法也是优化算法）**。**正则化**也是一种优化算法，目的是为了防止模型过拟合，但是**正则化并不是训练算法**。
	- **训练算法**：在数据集中进行学习，得到对应的权值ω，然后得到一个模型。使这个模型可以更好的拟合现有的数据集并预测新的数据。
	- **优化算法**：通过训练得出的模型可以并不是最优的模型，我们需要让这个模型更好的拟合现有的数据并预测新的数据。可以简单的理解为将模型的**性能度量**最小化或最大化。
	- 最大似然原理（是一种训练算法同时也是优化算法）：在已知样本的情况下，最合理的参数值是让样本出现概率最大的参数值
		- **传统的方法：直接预测目标变量 y 的完整概率分布。例如，在分类任务中，模型会预测每个类别的数据出现的概率。**
			1. 初始化参数(根据经验，随机化等等方法)
			2. 计算观察数据序列x在参数p下的似然函数P(X|p):
				-  $$P(x|p)=p*p$$
			3. 更新参数p，使得似然函数最大化
				-  $$p=P(X)/P(X|p)$$ 
			4. 重复2和3，知道参数p收敛。
		- **简化方法：预测目标变量 y 在给定输入变量 x 条件下的某种统计量，例如均值、方差或中位数。例如，在回归任务中，模型会预测目标变量的期望值。**
			1. 初始化参数(根据经验，随机化等等方法)
			2. 计算观察数据序列x中所有条件统计量 P(xᵢ | xᵢ₋₁, p)。
			3. 更新参数 p，使得条件统计量 P(xᵢ | xᵢ₋₁, p) 与观测数据序列 X 中的实际条件统计量一致。 
			4. 重复2和3，知道参数p收敛。
		- 最大似然估计：基础最大似然原理的参数估计方法，即可以简单的理解为模型中参数的估计方法（并选择合理的模型参数，并不是没有任何理论依据的进行模型参数的调整）。**可以理解为概率分布下取值的逆运用，概率分布下取值是通过每个位置的概率随机生成样本，而最大似然估计就相当于知道了样本量，从而去求这个概率分布**。
		- 通过最大似然可以导出代价函数
			- 在最大似然中代价函数就是**负对数似然函数（但可以在使用最大似然进行模型的训练的时候，不采用这种代价函数，也可以自己手动选择代价函数）**。
			 1. 定义似然函数（描述特定参数下，观测数据出现概率），**衡量了观测数据与模型的拟合程度**$$L(θ | x,y)=P(y|x,θ)$$其中左边的就是似然函数，θ 是模型参数，x观测的数据（可以理解为特征向量），y即特征向量x的标签。
			 2. 定义对数似然函数$$l(θ | x, y) = log L(θ | x, y)$$
			 3. 定义代价函数，使用**负的对数似然作为代价函数**$$J(θ) = -l(θ | x, y)$$
			 4. 训练模型，通过优化算法（梯度下降）求出代价函数最优解。
			- 可以理解为这个最大似然就是一套体系流程。
			- PS：这个最大似然只是其中的一种训练方法，还存在着**梯度下降，最小二乘法，贝叶斯估计等**
			- 在梯度下降训练算法中，梯度下降可以理解为一种经验算法，需要手动的进行损失函数选择。
- 输出单元：基于不同任务和任务符合的概率分布进行输出层函数的激活函数
	- 用于高斯输出分布的线性单元，高斯分布就是正态分布。
		- 输出单元基于仿射变换的输出单元，称为线性单元
		- 给定特征，线性输出单元层产生一个向量$$y = W^Th+b$$
		- 当存在两个前提条件时：
			1. 模型的输出服从条件高斯分布$$p(y|x)= N(y; μ(x), Σ)$$μ(x)是条件均值，Σ协方差矩阵（高斯分布的协方差矩阵是一个对称矩阵）
			2. 只适用于线性关系的
			- 当存在这两个前提条件时**线性输出层常常用来产生条件高斯分布的均值** 
		- 当存在两个前提条件：
			1. 模型的输出服从条件高斯分布$$p(y|x)= N(y; μ(x), Σ)$$
			2.  误差项 (y - μ(x)) 服从均值为 0 的高斯分布,即模型的误差应该随机分布在 0 的周围，且正负误差出现的概率大致相等。
			- 最大化对数似然函数等价于最小化均方误差。
		- 最大似然框架让学习高斯分布的协方差矩阵更加容易
			- 似然函数：L(μ, Σ) = ∏ᵢ p(xᵢ | μ, Σ)，通过最大似然估计即可求出协方差矩阵Σ。
	- 用于伯努利分布输出分布的sigmoid单元：当需要预测二值型变量y的值。
		- sigmoid输出单元定义为$\hat{y}=σ(ω^Th+b)$ 可以理解为两部分内容，线性层$z=ω^T+b$ 和sigmoid激活函数将z转化为概率。
		- sigmoid函数:$$f(x)=\frac{1}{1+exp(-x)}$$exp是取指数的意思。![[Pasted image 20240310152721.png]]sigmoid函数在变量取绝对值非常大的正值或负值会出现饱和状态，函数图像会变得很平滑，对输入的微小改变不敏感。
		- softplus函数：![[Pasted image 20240310153654.png]]，可以用来产生正态分布的β和σ，因为它的范围是（0，∞）.![[Pasted image 20240310155007.png]]
	- 用于Multinoulli输出分布的softmax的单元：用于一个具有n个可能取值的离散型随机变量的分布。
- 网络模型：
	- 前馈神经网络：也叫多层感知机，由输入层->隐藏层->输出层。隐藏层可以是多层也可以是单层。