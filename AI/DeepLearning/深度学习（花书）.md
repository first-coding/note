- 数学基础：线性代数，概率论，高等数学（微积分）
- 基本概念/算法：
	- 优化：通过改变自变量（x），最小化或最大化函数f(x)的过程。
	- 在模型优化过程中，**目标函数/准则**：最大化或最小化的函数。当对目标函数进行最小化的时候，我们也称为**代价函数，损失函数，误差函数**。
	- 在对数据集进行建模过程中，**代价函数可以理解为对模型进行性能度量的函数**，**优化模型就是让这个代价函数最小化或者最大化**，**目标函数就是我们学习得到的一个函数，这个函数可以在数据集上有良好的表现。**
	- **激活函数**：可以理解为在神经网络中，**具有一定计算能力的神经元。决定在这个神经元中的输入与输出的关系。** 常见的激活函数有：**Relu，Sigmod等等**。
	- 在优化过程中，就是存在很多的方法，例如**梯度下降，随机梯度下降**等等。
	- **凸函数和非凸函数**：
		- 凸函数：在该区间函数图象上的任意两点所连成的线段上的每一个点都位于函数图象的下方(或上方)。![[Pasted image 20240307183439.png]]
		- 非凸函数：函数在该区间上有多个极值,即系统有多个稳定的平衡态。![[Pasted image 20240307183450.png]]
	- 正则化：可以简单的理解为在损失函数f(x) 中加上了一定的项，从而降低模型过拟合的概率。
	- 饱和函数：当输入值很大或者很小的时候，输出值趋于平缓的函数。这种函数会把梯度变得很小（即参数空间很小，因为趋于平缓没有太大的变化空间）
- 优化算法和训练算法：![[Drawing 2024-03-08 17.26.19.excalidraw]]优化算法的目标就是让模型更优，那就存在一些算法来进行一系统的操作（即一堆数学方面的转换），就比如**梯度下降算法（不仅是训练算法也是优化算法）**。**正则化**也是一种优化算法，目的是为了防止模型过拟合，但是**正则化并不是训练算法**。
	- **训练算法**：在数据集中进行学习，得到对应的权值ω，然后得到一个模型。使这个模型可以更好的拟合现有的数据集并预测新的数据。
	- **优化算法**：通过训练得出的模型可以并不是最优的模型，我们需要让这个模型更好的拟合现有的数据并预测新的数据。可以简单的理解为将模型的**性能度量**最小化或最大化。
	- 最大似然原理（是一种训练算法同时也是优化算法）：在已知样本的情况下，最合理的参数值是让样本出现概率最大的参数值
		- **传统的方法：直接预测目标变量 y 的完整概率分布。例如，在分类任务中，模型会预测每个类别的数据出现的概率。**
		- **简化方法：预测目标变量 y 在给定输入变量 x 条件下的某种统计量，例如均值、方差或中位数。例如，在回归任务中，模型会预测目标变量的期望值。**
		- 最大似然估计：基础最大似然原理的参数估计方法，即可以简单的理解为模型中参数的估计方法（并选择合理的模型参数，并不是没有任何理论依据的进行模型参数的调整）。**可以理解为概率分布下取值的逆运用，概率分布下取值是通过每个位置的概率随机生成样本，而最大似然估计就相当于知道了样本量，从而去求这个概率分布**。
		- 通过最大似然可以导出代价函数
			- 在最大似然中代价函数就是**负对数似然函数（但可以在使用最大似然进行模型的训练的时候，不采用这种代价函数，也可以自己手动选择代价函数）**。
			 1. 定义似然函数（描述特定参数下，观测数据出现概率），**衡量了观测数据与模型的拟合程度**$$L(θ | x,y)=P(y|x,θ)$$其中左边的就是似然函数，θ 是模型参数，x观测的数据（可以理解为特征向量），y即特征向量x的标签。
			 2. 定义对数似然函数$$l(θ | x, y) = log L(θ | x, y)$$
			 3. 定义代价函数，使用**负的对数似然作为代价函数**$$J(θ) = -l(θ | x, y)$$
			 4. 训练模型，通过优化算法（梯度下降）求出代价函数最优解。
			- 可以理解为这个最大似然就是一套体系流程。
			- PS：这个最大似然只是其中的一种训练方法，还存在着**梯度下降，最小二乘法，贝叶斯估计等**
			- 在梯度下降训练算法中，梯度下降可以理解为一种经验算法，需要手动的进行损失函数选择。
- 网络模型：
	- 前馈神经网络：也叫多层感知机，由输入层->隐藏层->输出层。隐藏层可以是多层也可以是单层。