- 数学基础：线性代数，概率论，高等数学（微积分）
- 基本概念/算法：
	- 优化：通过改变自变量（x），最小化或最大化函数f(x)的过程。
	- 在模型优化过程中，**目标函数/准则**：最大化或最小化的函数。当对目标函数进行最小化的时候，我们也称为**代价函数，损失函数，误差函数**。
	- 在对数据集进行建模过程中，**代价函数可以理解为对模型进行性能度量的函数**，**优化模型就是让这个代价函数最小化或者最大化**，**目标函数就是我们学习得到的一个函数，这个函数可以在数据集上有良好的表现。**
	- **激活函数**：可以理解为在神经网络中，**具有一定计算能力的神经元。决定在这个神经元中的输入与输出的关系。即输入一个值，通过这个具有激活函数的神经元之后的输出是什么**， 常见的激活函数有：**Relu，Sigmoid等等**。
		- **PS：sigmoid函数是一个大类，形似S的函数，函数连续，可导，有界。**，logistic sigmoid只是其中一种。
	- 在优化过程中，就是存在很多的方法，例如**梯度下降，随机梯度下降**等等。
	- **凸和非凸：
		- 凸函数：在该区间函数图象上的任意两点所连成的线段上的每一个点都位于函数图象的下方(或上方)。![[Pasted image 20240307183439.png]]
		- 非凸函数：函数在该区间上有多个极值,即系统有多个稳定的平衡态。![[Pasted image 20240307183450.png]]
		- 凸优化问题：目标函数和约束条件满足凸性质情况下的优化，凸函数的优化问题通常相对容易解决。
			- 目标函数是凸函数，即函数二阶导数为正
			- 约束条件是凸集：凸集（集合中任意两点之间的连线上的所有点也属于该集合）
			- 优化变量定义域是凸集：优化问题中涉及的变量所在的定义域也是凸集。
	- **正则化**：可以简单的理解为在损失函数f(x) 中加上了一定的项，从而降低模型过拟合的概率。
	- **饱和函数**：当输入值很大或者很小的时候，输出值趋于平缓的函数。这种函数会把梯度变得很小（即参数空间很小，因为趋于平缓没有太大的变化空间）
	- **泛函**：一个/多个函数到一个实数的映射，就是相当于说函数是自变量x，映射出来的一个实数就是因变量y。
	- **仿射变换**：几何中，对一个向量空间进行一次线性变化，并加上平移变换另一个向量空间
	- **线性变化**：指在两个向量空间之间保持向量加法和标量乘法的函数。
	- **协方差矩阵**：是衡量多个随机变量之间相关性的指标
	- **非归一化**：就是和不等于1的。
	- **异方差模型**：模型对不同的x值预测出y不同的方差，在传统的统计学中，假设输出值的方差是恒定的，即与输入值没关系。
	- **高斯混合**：多个高斯分布的线性组合。
		- ps：高斯混合输出在语音生成模型和物理运动中特别有效。
	- 对于函数f(x)具有左导数和右导数：
		- 左导数：紧邻在x左边的函数的斜率
		- 右导数：紧邻在x右边的函数的斜率
		- 可微：当函数在x处左导数和右导数都有定义并且相等，函数在x点处可微。
- **优化算法和训练算法**：
	- ![[Drawing 2024-03-08 17.26.19.excalidraw]]优化算法的目标就是让模型更优，那就存在一些算法来进行一系统的操作（即一堆数学方面的转换），就比如**梯度下降算法（不仅是训练算法也是优化算法）**。**正则化**也是一种优化算法，目的是为了防止模型过拟合，但是**正则化并不是训练算法**。
	- **训练算法**：在数据集中进行学习，得到对应的权值ω，然后得到一个模型。使这个模型可以更好的拟合现有的数据集并预测新的数据。
	- **优化算法**：通过训练得出的模型可以并不是最优的模型，我们需要让这个模型更好的拟合现有的数据并预测新的数据。可以简单的理解为将模型的**性能度量**最小化或最大化。
	- **最大似然原理**（是一种训练算法同时也是优化算法）：在已知样本的情况下，最合理的参数值是让样本出现概率最大的参数值
		- **传统的方法：直接预测目标变量 y 的完整概率分布。例如，在分类任务中，模型会预测每个类别的数据出现的概率。**
			1. 初始化参数(根据经验，随机化等等方法)
			2. 计算观察数据序列x在参数p下的似然函数P(X|p):
				-  $$P(x|p)=p*p$$
			3. 更新参数p，使得似然函数最大化
				-  $$p=P(X)/P(X|p)$$ 
			4. 重复2和3，知道参数p收敛。
		- **简化方法：预测目标变量 y 在给定输入变量 x 条件下的某种统计量，例如均值、方差或中位数。例如，在回归任务中，模型会预测目标变量的期望值。**
			1. 初始化参数(根据经验，随机化等等方法)
			2. 计算观察数据序列x中所有条件统计量 P(xᵢ | xᵢ₋₁, p)。
			3. 更新参数 p，使得条件统计量 P(xᵢ | xᵢ₋₁, p) 与观测数据序列 X 中的实际条件统计量一致。 
			4. 重复2和3，知道参数p收敛。
		- **最大似然估计**：基础最大似然原理的参数估计方法，即可以简单的理解为模型中参数的估计方法（并选择合理的模型参数，并不是没有任何理论依据的进行模型参数的调整）。**可以理解为概率分布下取值的逆运用，概率分布下取值是通过每个位置的概率随机生成样本，而最大似然估计就相当于知道了样本量，从而去求这个概率分布**。
		- **通过最大似然可以导出代价函数**
			- 在最大似然中代价函数就是**负对数似然函数（但可以在使用最大似然进行模型的训练的时候，不采用这种代价函数，也可以自己手动选择代价函数）**。
			 1. 定义似然函数（描述特定参数下，观测数据出现概率），**衡量了观测数据与模型的拟合程度**$$L(θ | x,y)=P(y|x,θ)$$其中左边的就是似然函数，θ 是模型参数，x观测的数据（可以理解为特征向量），y即特征向量x的标签。
			 2. 定义对数似然函数$$l(θ | x, y) = log L(θ | x, y)$$
			 3. 定义代价函数，使用**负的对数似然作为代价函数**$$J(θ) = -l(θ | x, y)$$
			 4. 训练模型，通过优化算法（梯度下降）求出代价函数最优解。
			- 可以理解为这个最大似然就是一套体系流程。
			- PS：这个最大似然只是其中的一种训练方法，还存在着**梯度下降，最小二乘法，贝叶斯估计等**
			- 在梯度下降训练算法中，梯度下降可以理解为一种经验算法，需要手动的进行损失函数选择。
- **输出单元**：基于不同任务和任务符合的概率分布进行输出层函数的激活函数
	- 用于高斯输出分布的线性单元，高斯分布就是正态分布。
		- 输出单元基于仿射变换的输出单元，称为线性单元
		- 给定特征，线性输出单元层产生一个向量$$y = W^Th+b$$
		- 当存在两个前提条件时：
			1. 模型的输出服从条件高斯分布$$p(y|x)= N(y; μ(x), Σ)$$μ(x)是条件均值，Σ协方差矩阵（高斯分布的协方差矩阵是一个对称矩阵）
			2. 只适用于线性关系的
			- 当存在这两个前提条件时**线性输出层常常用来产生条件高斯分布的均值** 
		- 当存在两个前提条件：
			1. 模型的输出服从条件高斯分布$$p(y|x)= N(y; μ(x), Σ)$$
			2.  误差项 (y - μ(x)) 服从均值为 0 的高斯分布,即模型的误差应该随机分布在 0 的周围，且正负误差出现的概率大致相等。
			- 最大化对数似然函数等价于最小化均方误差。
		- 最大似然框架让学习高斯分布的协方差矩阵更加容易
			- 似然函数：L(μ, Σ) = ∏ᵢ p(xᵢ | μ, Σ)，通过最大似然估计即可求出协方差矩阵Σ。
	- 用于伯努利分布输出分布的sigmoid单元：当需要预测二值型变量y的值。 ^800fd7
		- sigmoid输出单元定义为$\hat{y}=σ(ω^Th+b)$ 可以理解为两部分内容，线性层$z=ω^T+b$ 和sigmoid激活函数将z转化为概率。
		- logistic sigmoid函数:$$f(x)=\frac{1}{1+exp(-x)}$$exp是取指数的意思。即$exp(x)=e^x$![[Pasted image 20240310152721.png]]sigmoid函数在变量取绝对值非常大的正值或负值会出现饱和状态，函数图像会变得很平滑，对输入的微小改变不敏感。
		- softplus函数：![[Pasted image 20240310153654.png]]，可以用来产生正态分布的β和σ，因为它的范围是（0，∞）.![[Pasted image 20240310155007.png]]
	- 用于Multinoulli输出分布的softmax的单元：用于一个具有n个可能取值的离散型随机变量的分布。
		- 最常用在分类器的输出，来表示n个不同类上的概率分布。
		- 比较少见的是softmax函数在模型内部使用。 
		- 可以通过sigmoid函数控制的Bernoulli分布去进行推广到**存在n个值的离散型变量的情况**。
			- **确保每个元素$\hat{y_i}=p(y=i|x)$每个$\hat{y_i}$元素介于0和1之间，还要确保全部$\hat{y_i}$ 的和为1 **
			- 可以通过先对一个线性函数$z=W^Th+b$，$z_i=logP(y=i|x)$，在通过softmax函数对z指数化和归一化得到$\hat{y}$，得到$sofxmax(z)_i=\frac{exp(z_i)}{\sum_jexp(z_j)}$  exp代表指数化。
	- 除了常见的线性，simoid和softmax输出单元最常见，还存在着其他的输出单元，**可以通过最大似然原则进行几乎任何种类的输出层设计一个好的代价函数提供指导**。
- **隐藏单元**：如何选择隐藏单元类型？（隐藏单元在模型的隐藏层中，即隐藏层中的激活函数怎么选择）
	- ReLU：$f(x)=max(0,x)$ ，当输入x为正数，输出本身x，否则就为0。是隐藏单元极好的默认选择，也通常用在隐藏单元中。
		- PS：ReLu在x等于0处不可微，因为可微才可以通过基于梯度的学习算法进行训练或优化。**但是因为神经网络训练算法通常不会达到代价函数的局部最小值**，而只是显著减少它的值。
	- Relu扩展：基于$z_i<0$时使用一个**非零的斜率**$α_i:h_i=g(z,α)_i=max(0,z_i)+αmin(0,z_i)$ 
		- **绝对值整流**：固定$α_i=-1$得到g(z)=|z|。
			- ![[Pasted image 20240314171617.png]]
		- **渗漏ReLu**：将$α_i$固定成一个类似0.01的小值.![[Pasted image 20240314171827.png]]
		- **PReLu**：将$α_i$作为学习参数。
			- ![[Pasted image 20240314171948.png]]这里的α不是固定的值，而是可以通过训练集数据拟合得到的，每个神经元的α可能不一样。
		- **maxout单元**：$f(x)=max(z_1,z_2,...,z_k)$  $z_i=ω_i^T+b_i$  k是maxout单元划分的组数 ，$ω_i$是权重向量，$b_i$是偏置。**Maxout单元的输出 f(x) 是经过这些线性变换后的最大值，因此可以看作是对输入x 的非线性映射。** 通常作为隐藏层的激活函数。
			- maxout单元具有冗余可以抵抗**灾难遗忘和噪声**。
			- **灾难性遗忘**：是指当模型接收到**新数据或进行新任务**时，会**忘记之前学习到的知识或特征**。这可能会导致模型性能下降，因为模型过于专注于新数据或任务，而忽略了以前学到的有用信息。
	- logistic sigmoid与双曲正切函数：
		- logistic sigmoid激活函数见$g(z)=α(z)$[[深度学习（花书）#^800fd7|logistic sigmoid]]
			- sigmoid单元在大部分定义域内都饱和，仅当z接近0时，才对输入敏感。因为大部分定义域内都饱和，所以梯度学习非常困难（所以不鼓励作为前馈网络中的隐藏单元的激活函数，一般用于作为输出单元）。但是在一些特别的循环网络，很多概率模型，自编码器的一些额外要求**不能使用分段线性激活函数**，这时候sigmoid单元具有吸引力。
		- 双曲正切函数：$g(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$ 标准形式，为了突出双曲正切函数性质也可以写成$tanh(z)=2α(2z)-1$
		- 在一定条件下，双曲正切激活函数会比logistic sigmoid更好。
		- 其他隐藏单元：
			- **径向基函数（RBF）**：$RBF(x)=exp(-\frac{(x-c)^2}{2σ^2})$ **c的值是中心点，σ是宽度，x是输入值。**
			- **softplus函数**：$g(a)=log(1+e^a)$ ,实际是对ReLu函数的一种平滑近似，可导，连续。
			- **硬双曲正切函数**：$g(a)=max(-1,min(1,a))$ 
			- 存在很多其他的激活函数，性能表现都良好，但是只有存在着比现有的激活函数要好才会进行发布。
- PS：**无论是隐藏单元和隐藏单元的激活函数都是根据需求而选择，并不存在一种对任何情况都适合的激活函数。
- **架构**：网络的整体结构（应该有多少个单元，这些单元之间怎么连接）
	- **更深层的网络通常能够对每一层使用更少的单元数和更少的参数，并且更容易泛化到测试集，但是通常难以优化**。
	- **万能近似定理**：一个前馈神经网络，如果具有**线性输出层**和至少一层具有任何一种 **“挤压“性质的激活函数**（**logistic sigmoid激活函数**）的**隐藏层**，给予网络足够数量的隐藏单元，可以以**任意精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数**，关于Borel可测只需要知道，定义在$R^n$的有界闭集上的任意连续函数是Borel可测的，所以可以用神经网络来近似。
		- 万能近似定理意味着：无论我们试图学习什么函数，一个大的MLP（多层感知机）一定能够表示这个函数。（**可以保证MLP可以表示这个函数，但是无法保证训练算法可以学习到这个函数** ）
			1. 用于训练的优化算法可能找不到用于期望函数的参数值。
			2. 训练算法可能由于过拟合选择错误的函数。
		- 总的来说，可以用前馈神经网络表示任何函数，但是网络层可能特别大无法实现，还可能无法正确学习和泛化。**大多数情况都是使用更深的模型去减少期望函数所需要的神经元的数量，减少泛化误差**。
		- **存在一种函数族，当神经网络的深度大于d的时候，就可以无限的近似**，但是当**小于或等于d**的时候就需要n的指数级的隐藏单元。
			1. 不与连续可微的神经网络类似的机器学习模型中出现
			2. 关于逻辑门电路
			3. 具有非负权重的线性阈值单元
			4. 连续值激活的网络
		- 大部分情况下，神经网络的深度越深，在合适的任务和数据集中，准确率会越高。
- **反向传播和其他微分算法**：
	- **前向传播**：将输入数据通过神经网络各层次的计算，得到模型的输出。
	- **计算代价函数**：使用模型的输出和真实标签的差异来计算代价函数J(θ) 的值，代表了模型的预测误差。
	- **反向传播**：根据代价函数的值，使用反向传播算法来计算梯度，并利用**梯度下降等优化算法更新神经网络参数θ来减小代价函数的值**，从而优化模型性能。
		- **反向传播仅仅用于计算梯度的方法**，（计算出来的梯度可以用另一种训练算法**比如梯度下降**，通过使用梯度来进行学习）
			- **计算图**：将计算过程化成图像的方法。可以更准确的描述一些算法。
		- **即一种计算导数的链式法则的算法，使用高效的特定运算顺序**
			- 通过已知的微积分求导的链式法则将变量转成向量就变成![[Pasted image 20240317152805.png]]**Jacobian matrix**:![[Pasted image 20240317152910.png]]
			- 可以知道，向量x的梯度可以通过Jacobian matrix$\frac{α_y}{α_x}$和梯度相乘得到。**反向传播算法就是每一个这样的Jacobian和梯度的乘积操作组成。**
			- ![[Drawing 2024-03-17 15.41.15.excalidraw]]
			- **反向传播算法可以自动生成梯度，当梯度计算图过大的时候，就会方便许多。**
	- 其他微分：
		- **自动微分**：如何通过算法方式进行导数计算，（反向传播算法只是其中一种）
			- **反向模式累加**：
				- 反向模式累加从函数的输出变量开始，逐步计算中间变量相对于输出变量的导数，直到得到输入变量的导数。
				- 这种方法对于有多个输入的函数比较适用，因为每个输入的导数可以从输出开始反向传播计算。
			- **前向模式累加**：
				- 前向模式累加从函数的输入变量开始，逐步计算中间变量的导数，直到得到最终输出变量的导数。
				- 这种方法对于有多个输出的函数比较适用，因为每个输出的导数都可以从头开始独立计算。
		- **高阶微分**：
			- Hessian矩阵：![[Pasted image 20240317160006.png]]
			- 典型的深度学习方法时使用**Krylov方法**：
				- 用于执行各种操作的一组迭代技术。（**操作包括像近似求解矩阵的逆或者近似矩阵的特征值/特征向量等**）
- 网络模型：
	- 前馈神经网络：也叫多层感知机，由输入层->隐藏层->输出层。隐藏层可以是多层也可以是单层。
		- **无反馈连接**：数据只在神经网络向前传播，没有反馈连接形成循环结构。
		- **单向传播**：输入数据经过一系列的层和激活函数处理后，最终产生输出，没有跨层的连接反馈。
		- **层级结构**：网络通常由输入层、隐藏层（可以有多层）和输出层组成，层与层之间全连接连接也可以是部分连接。
		- **静态**：网络的输出只取决于当前的输入，不考虑过去的历史输入或网络的状态。**网络的每一次计算都是独立的，不会受到之前计算的影响。**
	- 混合密度网络：将高斯混合作为输出的神经网络。