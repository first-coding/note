- #线性因子模型：
	- 线性因子：是指**影响观察到的变量之间关系的潜在因素或潜在变量**。
	- 可以理解为一种建模工具
	- **数据降维/特征学习**
	- **数据生成/重构**
	- **异常检测和数据清洗**：
	- **预测与推荐**
	- **数据压缩/可视化**
- #自编码器：  ^4e5970
	- 传统的自编码器：
		- 神经网络的一种，**经过训练可以尝试将输入复制到输出**，但是如果**输入输出一样那就没意义**，
			- 我们通常**在自编码器强加一些约束，只能近似的复制**。
		- 由两部分组成
			- **h=f(x)的编码器**和**生成重构的r=g(h)解码器**组成。
	- 现代自编码器：
		- 将**确定的函数h=f(x)** 这种推广为**随机映射$P_{encoder}(h|x)$和$p_{decoder}(x|h)$**。
	- 分类：
		- **欠完备自编码器**：得到有用特征，我们将**h的维度比x小**，但是**需要限制容量，如果太大，那么就可能出现学习到有用的信息。**
		- **正则自编码器**：**自编码器的损失函数**中**加入额外的正则项**，以**约束模型参数的大小或模型的复杂度**，以**防止过拟合**
			- **稀疏自编码器**：在损失函数加上**稀疏乘法Ω(h)**，$L(x,g(f(x)))+Ω(h)$ ,一般用于**学习特征，以便用于分类这样的任务** ^2d9e76
			- **去噪自编码器**：**接收损坏数据作为输入**，并训练来预测**原始未被损坏数据**作为输出的**自编码器**
				- 根据以下过程，**从训练数据对$(x,\widetilde{X})$ 中学习重构分布**
					- 训练数据采一个训练样本x
					- 从$C(\widetilde{X}|X=x)$采一个损坏样本$\widetilde{X}$
					- 将$x,\widetilde{x}$ 作为训练样本估计**自编码器的重构分布**。
			- **收缩自编码器**：和[[深度学习（花书二）#^2d9e76|稀疏自编码器]]一样，都是在损失函数中加入**惩罚项**实现额外的约束。收缩自编码器$Ω(h)=λ||\frac{αf(x)}{αx}||^2_F$ ，是**平方Frobenius范数（元素平方之和）**
		- **参数化自编码器**：编码器和解码器都是由**参数化的函数表示**
			- **变分自编码器**：编码器和解码器都是由**参数化的神经网络**表示的，并且模型的训练过程基于**最大化变分下界**（Variational Lower Bound）。
	- 作用：**数据压缩与降维**，**去噪**，**特征学习**，**生成模型（变分自编码器和GAN等自编码器的变体可以生成新的数据样本）**，**数据重建**。
- #表示学习
	- #无监督 
		- 没有标签，即没有对应y对应输入。
		- **贪心逐层无监督预训练**：这个被用于规避监督问题中深度神经网络难以联合多层的问题。
			- 通过逐层训练模型，每一层的训练都是无监督的，也就是没有使用标注的数据。在训练过程中，模型会逐渐学习到数据中的特征，并将这些特征编码成更高层次的表示。
			- **贪心**：是因为是一个贪心算法，**意味着可以独立地优化解决方案的每一个部分，每一步解决一个部分**。
			- **逐层**：独立的解决方案是**网络层**。**每一次都是处理一层网络，保持前面的网络层不变**
			- **无监督**：每一层用**无监督表示学习算法训练**。
			- **预训练**：**在联合训练算法精调所有层之前第一步**。
			- **何时使用/有效原因**：无监督预训练基于两个想法
				- **利用深度神经网络对初始参数的选择可以对模型有着显著的正则化效果（在较小程度上，改进优化）**。
				- **利用更一般的想法：学习输入分布有助于学习从输入到输出的映射**
				- 对于**词嵌入**，无监督训练在**处理单词特别有用**。对于**处理图像则差点**。
			- **存在两个缺点**：
				- 使用了**两个单独的训练阶段，具有许多超参数，但是效果需要之后才能度量，难以提前预测**。
				- **每个阶段都有各自的超参数，第二阶段的性能通常不能用第一阶段来进行预测**。
			- **大多数的算法已经不使用无监督与训练，除了在one-hot向量的自然表示不能传达相似的信息的地方使用**。
			- 基于这个技术，已经推广到[[深度学习（花书）#^21258f|监督预训练]]。
	- #迁移学习
		- 将知识从一个任务或领域迁移到另一个任务或领域，**关注在不同任务或领域之间共享知识，以改善目标任务的性能**
		- #领域自适应
			- 迁移学习特定情况，专注解决**源和目标之间的分布差异，关注如果让模型在目标上性能更好，不需要在目标域上有标记的数据**。
		- #概念漂移
			- 指**数据分布或数据生成过程随时间发生变化**，从而导致**训练模型的性能下降**的现象。
		- #一次学习和零次学习
			- **只有一个标注样本**，一次学习
				- 一次学习是有可能的，**通过一个标注样本推断表示空间中聚集在周围的样本标签**。
			- **没有任何标注样本**，零次学习
				- 只有**在训练时使用了额外信息**，零次学习才有可能。**通常认为存在三个随机变量(x,y,T),x为输入，y为输出，T即描述任务的附加随机变量**.
				- **多模态学习**：利用多种不同类型的数据（模态），进行联合建模和学习的方法
	- #表示学习的一个重要问题：
		- **什么原因能够使一个表示比另一个表示更好**。
			- 一个假设：理想表示中的特征对应到观察数据的潜在成因。
			- **如果表示向量h表示观察值x的很多潜在因素，并且输出向量y最为重要的原因之一，那么h预测y很容易。**
				- 当如果p(x)是均匀分布的，则无助于学习p（y|x）.
				- 当如果是混合分布，混合分量清晰的分开，建模p（x）可以精确指出每个分量的位置，那**每个类一个标注样本的训练集可以精确学习p(y|x)**。
				- 但是**什么能将p（y|x）和p（x）关联在一起**。
					- 如果y和x的成因之一非常相关，边缘概率p（x）和条件概率p（y|x）会紧密关联。无监督表示学习可以会和半监督一样有用。
					- 假设y和x的成因之一，h表示所有这些成因。但是在现实中无法实现（**不可能捕获影响观察的所有或大多数变化因素** ）。现实中人们只会察觉和进行任务相关的内容。**根据这点，我们需要确定编码什么**。
						- 同时使用无监督学习和监督学习信号，**从而使得模型捕获最相关的变动因素**。
						- 使用纯无监督学习学习更大规模表示。
					- 另一个思路就是，选择一个影响最大的潜在因素。
	- #分布式表示
		- **分布式表示**：通过**多个特征共同表示一个概念**。每个特征单独可能没有明确的意义，但是组合起来可以描述复杂的概念。
			- 特点：
				1. **多维特征**：一个输入数据被映射到一个高维的向量空间，其中每个维度都是一个特征。
				2. **稀疏性**：表示通常是稀疏的，即大多数特征的值可能接近零。
				3.  **共享表示**：相似的输入数据在表示空间中会有相似的表示。这种共享的特性使得模型可以更好地泛化到未见过的数据。
				4. **非线性特性**：通过深度神经网络的层层映射，可以捕捉数据中的非线性关系和复杂特性。
		- **非分布式表示**：指**一个概念由单个特征或少数几个特征明确表示**，**每个特征都有明确的意义。**
			- 特点：
				1. **独立特征**：每个特征独立地表示一个概念或属性，没有其他特征参与。
				2. **低维度**：通常表示维度较低，每个特征都有明确的物理或语义解释。
				3. **固定特性**：表示通常是固定的，没有通过学习来调整或优化。
				4. **有限表达能力**：无法有效捕捉数据中的复杂关系和高阶特性。
		- 分布式表示具有更强的表达能力，**可以捕获数据的高阶特性和非线性关系**
		- **训练与优化**：
			-  分布式表示通过学习过程优化和调整。
			- 非分布式表示通常是固定的，不通过学习来优化。