- #线性因子模型：
	- 线性因子：是指**影响观察到的变量之间关系的潜在因素或潜在变量**。
	- 可以理解为一种建模工具
	- **数据降维/特征学习**
	- **数据生成/重构**
	- **异常检测和数据清洗**：
	- **预测与推荐**
	- **数据压缩/可视化**
- #自编码器：  ^4e5970
	- 传统的自编码器：
		- 神经网络的一种，**经过训练可以尝试将输入复制到输出**，但是如果**输入输出一样那就没意义**，
			- 我们通常**在自编码器强加一些约束，只能近似的复制**。
		- 由两部分组成
			- **h=f(x)的编码器**和**生成重构的r=g(h)解码器**组成。
	- 现代自编码器：
		- 将**确定的函数h=f(x)** 这种推广为**随机映射$P_{encoder}(h|x)$和$p_{decoder}(x|h)$**。
	- 分类：
		- **欠完备自编码器**：得到有用特征，我们将**h的维度比x小**，但是**需要限制容量，如果太大，那么就可能出现学习到有用的信息。**
		- **正则自编码器**：**自编码器的损失函数**中**加入额外的正则项**，以**约束模型参数的大小或模型的复杂度**，以**防止过拟合**
			- **稀疏自编码器**：在损失函数加上**稀疏乘法Ω(h)**，$L(x,g(f(x)))+Ω(h)$ ,一般用于**学习特征，以便用于分类这样的任务** ^2d9e76
			- **去噪自编码器**：**接收损坏数据作为输入**，并训练来预测**原始未被损坏数据**作为输出的**自编码器**
				- 根据以下过程，**从训练数据对$(x,\widetilde{X})$ 中学习重构分布**
					- 训练数据采一个训练样本x
					- 从$C(\widetilde{X}|X=x)$采一个损坏样本$\widetilde{X}$
					- 将$x,\widetilde{x}$ 作为训练样本估计**自编码器的重构分布**。
			- **收缩自编码器**：和[[深度学习（花书二）#^2d9e76|稀疏自编码器]]一样，都是在损失函数中加入**惩罚项**实现额外的约束。收缩自编码器$Ω(h)=λ||\frac{αf(x)}{αx}||^2_F$ ，是**平方Frobenius范数（元素平方之和）**
		- **参数化自编码器**：编码器和解码器都是由**参数化的函数表示**
			- **变分自编码器**：编码器和解码器都是由**参数化的神经网络**表示的，并且模型的训练过程基于**最大化变分下界**（Variational Lower Bound）。
	- 作用：**数据压缩与降维**，**去噪**，**特征学习**，**生成模型（变分自编码器和GAN等自编码器的变体可以生成新的数据样本）**，**数据重建**。
- #表示学习
	- #无监督 
		- 没有标签，即没有对应y对应输入。
		- **贪心逐层无监督预训练**：这个被用于规避监督问题中深度神经网络难以联合多层的问题。
			- 通过逐层训练模型，每一层的训练都是无监督的，也就是没有使用标注的数据。在训练过程中，模型会逐渐学习到数据中的特征，并将这些特征编码成更高层次的表示。
			- **贪心**：是因为是一个贪心算法，**意味着可以独立地优化解决方案的每一个部分，每一步解决一个部分**。
			- **逐层**：独立的解决方案是**网络层**。**每一次都是处理一层网络，保持前面的网络层不变**
			- **无监督**：每一层用**无监督表示学习算法训练**。
			- **预训练**：**在联合训练算法精调所有层之前第一步**。
			- **何时使用/有效原因**：无监督预训练基于两个想法
				- **利用深度神经网络对初始参数的选择可以对模型有着显著的正则化效果（在较小程度上，改进优化）**。
				- **利用更一般的想法：学习输入分布有助于学习从输入到输出的映射**
				- 对于**词嵌入**，无监督训练在**处理单词特别有用**。对于**处理图像则差点**。
			- **存在两个缺点**：
				- 使用了**两个单独的训练阶段，具有许多超参数，但是效果需要之后才能度量，难以提前预测**。
				- **每个阶段都有各自的超参数，第二阶段的性能通常不能用第一阶段来进行预测**。
			- **大多数的算法已经不使用无监督与训练，除了在one-hot向量的自然表示不能传达相似的信息的地方使用**。
			- 基于这个技术，已经推广到[[深度学习（花书）#^21258f|监督预训练]]。
	- #迁移学习
		- 将知识从一个任务或领域迁移到另一个任务或领域，**关注在不同任务或领域之间共享知识，以改善目标任务的性能**
		- #领域自适应
			- 迁移学习特定情况，专注解决**源和目标之间的分布差异，关注如果让模型在目标上性能更好，不需要在目标域上有标记的数据**。
		- #概念漂移
			- 指**数据分布或数据生成过程随时间发生变化**，从而导致**训练模型的性能下降**的现象。
		- #一次学习和零次学习
			- **只有一个标注样本**，一次学习
				- 一次学习是有可能的，**通过一个标注样本推断表示空间中聚集在周围的样本标签**。
			- **没有任何标注样本**，零次学习
				- 只有**在训练时使用了额外信息**，零次学习才有可能。**通常认为存在三个随机变量(x,y,T),x为输入，y为输出，T即描述任务的附加随机变量**.
				- **多模态学习**：利用多种不同类型的数据（模态），进行联合建模和学习的方法
	- #表示学习的一个重要问题：
		- **什么原因能够使一个表示比另一个表示更好**。
			- 一个假设：理想表示中的特征对应到观察数据的潜在成因。
			- **如果表示向量h表示观察值x的很多潜在因素，并且输出向量y最为重要的原因之一，那么h预测y很容易。**
				- 当如果p(x)是均匀分布的，则无助于学习p（y|x）.
				- 当如果是混合分布，混合分量清晰的分开，建模p（x）可以精确指出每个分量的位置，那**每个类一个标注样本的训练集可以精确学习p(y|x)**。
				- 但是**什么能将p（y|x）和p（x）关联在一起**。
					- 如果y和x的成因之一非常相关，边缘概率p（x）和条件概率p（y|x）会紧密关联。无监督表示学习可以会和半监督一样有用。
					- 假设y和x的成因之一，h表示所有这些成因。但是在现实中无法实现（**不可能捕获影响观察的所有或大多数变化因素** ）。现实中人们只会察觉和进行任务相关的内容。**根据这点，我们需要确定编码什么**。
						- 同时使用无监督学习和监督学习信号，**从而使得模型捕获最相关的变动因素**。
						- 使用纯无监督学习学习更大规模表示。
					- 另一个思路就是，选择一个影响最大的潜在因素。
	- #分布式表示
		- **分布式表示**：通过**多个特征共同表示一个概念**。每个特征单独可能没有明确的意义，但是组合起来可以描述复杂的概念。
			- 特点：2
				1. **多维特征**：一个输入数据被映射到一个高维的向量空间，其中每个维度都是一个特征。
				2. **稀疏性**：表示通常是稀疏的，即大多数特征的值可能接近零。
				3.  **共享表示**：相似的输入数据在表示空间中会有相似的表示。这种共享的特性使得模型可以更好地泛化到未见过的数据。
				4. **非线性特性**：通过深度神经网络的层层映射，可以捕捉数据中的非线性关系和复杂特性。
		- **非分布式表示**：指**一个概念由单个特征或少数几个特征明确表示**，**每个特征都有明确的意义。**
			- 特点：
				1. **独立特征**：每个特征独立地表示一个概念或属性，没有其他特征参与。
				2. **低维度**：通常表示维度较低，每个特征都有明确的物理或语义解释。
				3. **固定特性**：表示通常是固定的，没有通过学习来调整或优化。
				4. **有限表达能力**：无法有效捕捉数据中的复杂关系和高阶特性。
		- 分布式表示具有更强的表达能力，**可以捕获数据的高阶特性和非线性关系**
		- **训练与优化**：
			-  分布式表示通过学习过程优化和调整。
			- 非分布式表示通常是固定的，不通过学习来优化。
- #结构化概率模型
	- **使用图来描述概率分布中随机变量之间的直接相互作用，从而描述一个概率分布**。
		- 难点在于：**如何判断那些变量之间存在直接的相互作用关系**。
	- 结构化概率模型主要优点**可以显著降低表示概率分布、学习和推断的成本。** 对于有向模型还可以被加速。
	- 环：由无向边连接的变量序列，并且满足序列中的**最后一个变量连接回序列中的第一个变量**。
	- 弦：定义环序列中任意两个非连接变量之间的连接。
	- #非结构建模挑战：
		- 对于一些问题**我们可以忽略部分输入只需要关键的地方。**
		- 对于另一些问题**需要对输入数据整个结构完整理解，不能忽略部分的输入**。
			- **估计密度函数**：输入x，返回一个对数据生成分布的真实密度函数p(x)的估计。
			- **去噪**：输入受损/观察有误的输入数据$\widetilde{x}$ ,返回对原始的真实x的估计。
			- **缺失值的填补**：输入x的某些元素，返回一个x一些或者全部未观察值的估计或者概率分布。
			- **采样**：从概率分布p(x)中抽取新的样本，对于一些需求来说，语音合成，**模型需要多个输出以及对输入整体的良好建模**。
		- **当我们对有n个离散变量并且每个变量可以取k个值得x**，最简单得建模就是存储一个可以查询表格，纪录每一种可能值得概率，**需要$k^n$参数**。但是因为**内存大小，高效性以及运行时间来说**所以现实不可行。
		- 所以**结构化概率模型为随机变量之间的直接作用提供一个正式得建模框架**
			- **大大减少了模型的参数个数**，以至于只需要更少的数据就能进行有效的估计。
			- **更小的模型大大减小了在模型存储、推断以及采样时的计算开销**.
	- #图描述模型结构 
		- 图(graph)可以很好的表示随机变量之间的作用关系**其中每个节点(node)代表了一个随机变量，而每条边(edge)代表了两个随机变量之间有直接作用，而连接不相邻节点的路径(path)就代表了简介相互作用**。分两种
			- **有向图**：也叫贝叶斯网络，边有方向，从一个结点指向另一个结点，**方向表示条件概率分布**。eg：a指向b，**代表b的概率分布依赖于a**。![[Pasted image 20240702143408.png]]**优点是可以通过一个简单高效的过程从模型所表示的联合分布中产生样本，这个过程称为原始采样（只适用于有向图）**
				- 有向图用于信息流动方向比较明确的问题，对于因果不明确的问题，需要用到无向图
			- **无向图**：也被叫做**马尔科夫随机场(Markowv random fields)** **或**马尔科夫网络(Markov Network)。**边没有方向，不代表条件概率**。![[Pasted image 20240702143613.png]]无向图可以通过转换成有向图进行采样，**也可以通过Gibbs采样**。
				- 无向模型所有的理论结果依赖于**对于所有的x，$\widetilde{p}(x)>0$的假设**。满足这个条件的一个模型为![[Pasted image 20240702143923.png]]保证所有状态的概率大于零。**服从这个公式的任意分布都是玻尔兹曼分布**。
				- 因子图：在于将复杂的概率分布分解成多个简单的因子函数的乘积。
			- **分离和d-分离**：
				- 分离：**如果图结构显示给定变量集S的情况下变量集A和变量集B无关，我们声称给定变量集S时，变量集A和变量集B是分离的**
				- d-分离：**如果图结构显示给定变量集S时，变量集A和变量集B无关，那么我们认为给定变量集S时，变量集A d-分离于变量集B**
			- **有向模型和无向模型可以进行进行转换**，每个概率分布可以由有向模型或无向模型，**在最坏的时候，可以使用完全图表示任何分布。**![[Pasted image 20240702163426.png]]
			- 当模型**不包含任何潜变量的时候**，在有向图和无向图之中就需要大量父节点和非常大的团，会导致计算成本呈指数级上升。
			- 传统图模型和非传统图模型：
				- 传统图模型：有向图和无向图
					- 解释性强，能够明确表达变量之间的依赖关系，**但是难以处理大规模数据和复杂的非线性关系**
				- 非传统图模型：图神经网络（GNN）（通过神经网络的方法对图数据进行建模和学习）和消息传递神经网络（特定的GNN，通过节点之间的信息传递实现图的学习）
					- 解释性差，**能够处理大规模数据和学习复杂的非线性关系，有较强的泛化能力。**
			- 结构学习：需要连接那些紧密相关的变量，并忽略其他变量的作用。
			- **深度概率模型**：结合深度学习与概率建模的模型，常见的有**VAE（变分自编码器）、GAN（生成对抗网络）、VI（变分推断）等**，主要应用于**生成图像、文本、音频，半监督学习与无监督学习，异常检测中**。