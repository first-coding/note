- #线性因子模型：
	- 可以理解为一种建模工具
	- **数据降维/特征学习**
	- **数据生成/重构**
	- **异常检测和数据清洗**：
	- **预测与推荐**
	- **数据压缩/可视化**
- #自编码器：  ^4e5970
	- 传统的自编码器：
		- 神经网络的一种，**经过训练可以尝试将输入复制到输出**，但是如果**输入输出一样那就没意义**，
			- 我们通常**在自编码器强加一些约束，只能近似的复制**。
		- 由两部分组成
			- **h=f(x)的编码器**和**生成重构的r=g(h)解码器**组成。
	- 现代自编码器：
		- 将**确定的函数h=f(x)** 这种推广为**随机映射$P_{encoder}(h|x)$和$p_{decoder}(x|h)$**。
	- 分类：
		- **欠完备自编码器**：得到有用特征，我们将**h的维度比x小**，但是**需要限制容量，如果太大，那么就可能出现学习到有用的信息。**
		- **正则自编码器**：**自编码器的损失函数**中**加入额外的正则项**，以**约束模型参数的大小或模型的复杂度**，以**防止过拟合**
			- **稀疏自编码器**：在损失函数加上**稀疏乘法Ω(h)**，$L(x,g(f(x)))+Ω(h)$ ,一般用于**学习特征，以便用于分类这样的任务** ^2d9e76
			- **去噪自编码器**：**接收损坏数据作为输入**，并训练来预测**原始未被损坏数据**作为输出的**自编码器**
				- 根据以下过程，**从训练数据对$(x,\widetilde{X})$ 中学习重构分布**
					- 训练数据采一个训练样本x
					- 从$C(\widetilde{X}|X=x)$采一个损坏样本$\widetilde{X}$
					- 将$x,\widetilde{x}$ 作为训练样本估计**自编码器的重构分布**。
			- **收缩自编码器**：和[[深度学习（花书二）#^2d9e76|稀疏自编码器]]一样，都是在损失函数中加入**惩罚项**实现额外的约束。收缩自编码器$Ω(h)=λ||\frac{αf(x)}{αx}||^2_F$ ，是**平方Frobenius范数（元素平方之和）**
		- **参数化自编码器**：编码器和解码器都是由**参数化的函数表示**
			- **变分自编码器**：编码器和解码器都是由**参数化的神经网络**表示的，并且模型的训练过程基于**最大化变分下界**（Variational Lower Bound）。
	- 作用：**数据压缩与降维**，**去噪**，**特征学习**，**生成模型（变分自编码器和GAN等自编码器的变体可以生成新的数据样本）**，**数据重建**。
- #无监督学习： 
	- 没有标签，即没有对应y对应输入。
	- **贪心逐层无监督预训练**：这个被用于规避监督问题中深度神经网络难以联合多层的问题。
		- 通过逐层训练模型，每一层的训练都是无监督的，也就是没有使用标注的数据。在训练过程中，模型会逐渐学习到数据中的特征，并将这些特征编码成更高层次的表示。
		- **贪心**：是因为是一个贪心算法，**意味着可以独立地优化解决方案的每一个部分，每一步解决一个部分**。
		- **逐层**：独立的解决方案是**网络层**。**每一次都是处理一层网络，保持前面的网络层不变**
		- **无监督**：每一层用**无监督表示学习算法训练**。
		- **预训练**：**在联合训练算法精调所有层之前第一步**。
		- **何时使用/有效原因**：无监督预训练基于两个想法
			- **利用深度神经网络对初始参数的选择可以对模型有着显著的正则化效果（在较小程度上，改进优化）**。
			- **利用更一般的想法：学习输入分布有助于学习从输入到输出的映射**
			- 对于**词嵌入**，无监督训练在**处理单词特别有用**。对于**处理图像则差点**。