- #线性因子模型：
	- 线性因子：是指**影响观察到的变量之间关系的潜在因素或潜在变量**。
	- 可以理解为一种建模工具
	- **数据降维/特征学习**
	- **数据生成/重构**
	- **异常检测和数据清洗**：
	- **预测与推荐**
	- **数据压缩/可视化**
- #自编码器：  ^4e5970
	- 传统的自编码器：
		- 神经网络的一种，**经过训练可以尝试将输入复制到输出**，但是如果**输入输出一样那就没意义**，
			- 我们通常**在自编码器强加一些约束，只能近似的复制**。
		- 由两部分组成
			- **h=f(x)的编码器**和**生成重构的r=g(h)解码器**组成。
	- 现代自编码器：
		- 将**确定的函数h=f(x)** 这种推广为**随机映射$P_{encoder}(h|x)$和$p_{decoder}(x|h)$**。
	- 分类：
		- **欠完备自编码器**：得到有用特征，我们将**h的维度比x小**，但是**需要限制容量，如果太大，那么就可能出现学习到有用的信息。**
		- **正则自编码器**：**自编码器的损失函数**中**加入额外的正则项**，以**约束模型参数的大小或模型的复杂度**，以**防止过拟合**
			- **稀疏自编码器**：在损失函数加上**稀疏乘法Ω(h)**，$L(x,g(f(x)))+Ω(h)$ ,一般用于**学习特征，以便用于分类这样的任务** ^2d9e76
			- **去噪自编码器**：**接收损坏数据作为输入**，并训练来预测**原始未被损坏数据**作为输出的**自编码器**
				- 根据以下过程，**从训练数据对$(x,\widetilde{X})$ 中学习重构分布**
					- 训练数据采一个训练样本x
					- 从$C(\widetilde{X}|X=x)$采一个损坏样本$\widetilde{X}$
					- 将$x,\widetilde{x}$ 作为训练样本估计**自编码器的重构分布**。
			- **收缩自编码器**：和[[深度学习（花书二）#^2d9e76|稀疏自编码器]]一样，都是在损失函数中加入**惩罚项**实现额外的约束。收缩自编码器$Ω(h)=λ||\frac{αf(x)}{αx}||^2_F$ ，是**平方Frobenius范数（元素平方之和）**
		- **参数化自编码器**：编码器和解码器都是由**参数化的函数表示**
			- **变分自编码器**：编码器和解码器都是由**参数化的神经网络**表示的，并且模型的训练过程基于**最大化变分下界**（Variational Lower Bound）。
	- 作用：**数据压缩与降维**，**去噪**，**特征学习**，**生成模型（变分自编码器和GAN等自编码器的变体可以生成新的数据样本）**，**数据重建**。
- #表示学习
	- #无监督 
		- 没有标签，即没有对应y对应输入。
		- **贪心逐层无监督预训练**：这个被用于规避监督问题中深度神经网络难以联合多层的问题。
			- 通过逐层训练模型，每一层的训练都是无监督的，也就是没有使用标注的数据。在训练过程中，模型会逐渐学习到数据中的特征，并将这些特征编码成更高层次的表示。
			- **贪心**：是因为是一个贪心算法，**意味着可以独立地优化解决方案的每一个部分，每一步解决一个部分**。
			- **逐层**：独立的解决方案是**网络层**。**每一次都是处理一层网络，保持前面的网络层不变**
			- **无监督**：每一层用**无监督表示学习算法训练**。
			- **预训练**：**在联合训练算法精调所有层之前第一步**。
			- **何时使用/有效原因**：无监督预训练基于两个想法
				- **利用深度神经网络对初始参数的选择可以对模型有着显著的正则化效果（在较小程度上，改进优化）**。
				- **利用更一般的想法：学习输入分布有助于学习从输入到输出的映射**
				- 对于**词嵌入**，无监督训练在**处理单词特别有用**。对于**处理图像则差点**。
			- **存在两个缺点**：
				- 使用了**两个单独的训练阶段，具有许多超参数，但是效果需要之后才能度量，难以提前预测**。
				- **每个阶段都有各自的超参数，第二阶段的性能通常不能用第一阶段来进行预测**。
			- **大多数的算法已经不使用无监督与训练，除了在one-hot向量的自然表示不能传达相似的信息的地方使用**。
			- 基于这个技术，已经推广到[[深度学习（花书）#^21258f|监督预训练]]。
	- #迁移学习
		- 将知识从一个任务或领域迁移到另一个任务或领域，**关注在不同任务或领域之间共享知识，以改善目标任务的性能**
		- #领域自适应
			- 迁移学习特定情况，专注解决**源和目标之间的分布差异，关注如果让模型在目标上性能更好，不需要在目标域上有标记的数据**。
		- #概念漂移
			- 指**数据分布或数据生成过程随时间发生变化**，从而导致**训练模型的性能下降**的现象。
		- #一次学习和零次学习
			- **只有一个标注样本**，一次学习
				- 一次学习是有可能的，**通过一个标注样本推断表示空间中聚集在周围的样本标签**。
			- **没有任何标注样本**，零次学习
				- 只有**在训练时使用了额外信息**，零次学习才有可能。**通常认为存在三个随机变量(x,y,T),x为输入，y为输出，T即描述任务的附加随机变量**.
				- **多模态学习**：利用多种不同类型的数据（模态），进行联合建模和学习的方法
	- #表示学习的一个重要问题：
		- **什么原因能够使一个表示比另一个表示更好**。
			- 一个假设：理想表示中的特征对应到观察数据的潜在成因。
			- **如果表示向量h表示观察值x的很多潜在因素，并且输出向量y最为重要的原因之一，那么h预测y很容易。**
				- 当如果p(x)是均匀分布的，则无助于学习p（y|x）.
				- 当如果是混合分布，混合分量清晰的分开，建模p（x）可以精确指出每个分量的位置，那**每个类一个标注样本的训练集可以精确学习p(y|x)**。
				- 但是**什么能将p（y|x）和p（x）关联在一起**。
					- 如果y和x的成因之一非常相关，边缘概率p（x）和条件概率p（y|x）会紧密关联。无监督表示学习可以会和半监督一样有用。
					- 假设y和x的成因之一，h表示所有这些成因。但是在现实中无法实现（**不可能捕获影响观察的所有或大多数变化因素** ）。现实中人们只会察觉和进行任务相关的内容。**根据这点，我们需要确定编码什么**。
						- 同时使用无监督学习和监督学习信号，**从而使得模型捕获最相关的变动因素**。
						- 使用纯无监督学习学习更大规模表示。
					- 另一个思路就是，选择一个影响最大的潜在因素。
	- #分布式表示
		- **分布式表示**：通过**多个特征共同表示一个概念**。每个特征单独可能没有明确的意义，但是组合起来可以描述复杂的概念。
			- 特点：2
				1. **多维特征**：一个输入数据被映射到一个高维的向量空间，其中每个维度都是一个特征。
				2. **稀疏性**：表示通常是稀疏的，即大多数特征的值可能接近零。
				3.  **共享表示**：相似的输入数据在表示空间中会有相似的表示。这种共享的特性使得模型可以更好地泛化到未见过的数据。
				4. **非线性特性**：通过深度神经网络的层层映射，可以捕捉数据中的非线性关系和复杂特性。
		- **非分布式表示**：指**一个概念由单个特征或少数几个特征明确表示**，**每个特征都有明确的意义。**
			- 特点：
				1. **独立特征**：每个特征独立地表示一个概念或属性，没有其他特征参与。
				2. **低维度**：通常表示维度较低，每个特征都有明确的物理或语义解释。
				3. **固定特性**：表示通常是固定的，没有通过学习来调整或优化。
				4. **有限表达能力**：无法有效捕捉数据中的复杂关系和高阶特性。
		- 分布式表示具有更强的表达能力，**可以捕获数据的高阶特性和非线性关系**
		- **训练与优化**：
			-  分布式表示通过学习过程优化和调整。
			- 非分布式表示通常是固定的，不通过学习来优化。
- #结构化概率模型 ^298aaf
	- **使用图来描述概率分布中随机变量之间的直接相互作用，从而描述一个概率分布**。
		- 难点在于：**如何判断那些变量之间存在直接的相互作用关系**。
	- 结构化概率模型主要优点**可以显著降低表示概率分布、学习和推断的成本。** 对于有向模型还可以被加速。
	- 环：由无向边连接的变量序列，并且满足序列中的**最后一个变量连接回序列中的第一个变量**。
	- 弦：定义环序列中任意两个非连接变量之间的连接。
	- #非结构建模挑战：
		- 对于一些问题**我们可以忽略部分输入只需要关键的地方。**
		- 对于另一些问题**需要对输入数据整个结构完整理解，不能忽略部分的输入**。
			- **估计密度函数**：输入x，返回一个对数据生成分布的真实密度函数p(x)的估计。
			- **去噪**：输入受损/观察有误的输入数据$\widetilde{x}$ ,返回对原始的真实x的估计。
			- **缺失值的填补**：输入x的某些元素，返回一个x一些或者全部未观察值的估计或者概率分布。
			- **采样**：从概率分布p(x)中抽取新的样本，对于一些需求来说，语音合成，**模型需要多个输出以及对输入整体的良好建模**。
		- **当我们对有n个离散变量并且每个变量可以取k个值得x**，最简单得建模就是存储一个可以查询表格，纪录每一种可能值得概率，**需要$k^n$参数**。但是因为**内存大小，高效性以及运行时间来说**所以现实不可行。
		- 所以**结构化概率模型为随机变量之间的直接作用提供一个正式得建模框架**
			- **大大减少了模型的参数个数**，以至于只需要更少的数据就能进行有效的估计。
			- **更小的模型大大减小了在模型存储、推断以及采样时的计算开销**.
	- #图描述模型结构 
		- 图(graph)可以很好的表示随机变量之间的作用关系**其中每个节点(node)代表了一个随机变量，而每条边(edge)代表了两个随机变量之间有直接作用，而连接不相邻节点的路径(path)就代表了简介相互作用**。分两种
			- **有向图**：也叫贝叶斯网络，边有方向，从一个结点指向另一个结点，**方向表示条件概率分布**。eg：a指向b，**代表b的概率分布依赖于a**。![[Pasted image 20240702143408.png]]**优点是可以通过一个简单高效的过程从模型所表示的联合分布中产生样本，这个过程称为原始采样（只适用于有向图）**
				- 有向图用于信息流动方向比较明确的问题，对于因果不明确的问题，需要用到无向图
			- **无向图**：也被叫做**马尔科夫随机场(Markowv random fields)** **或**马尔科夫网络(Markov Network)。**边没有方向，不代表条件概率**。![[Pasted image 20240702143613.png]]无向图可以通过转换成有向图进行采样，**也可以通过Gibbs采样**。
				- 无向模型所有的理论结果依赖于**对于所有的x，$\widetilde{p}(x)>0$的假设**。满足这个条件的一个模型为![[Pasted image 20240702143923.png]]保证所有状态的概率大于零。**服从这个公式的任意分布都是玻尔兹曼分布**。
				- 因子图：在于将复杂的概率分布分解成多个简单的因子函数的乘积。
			- **分离和d-分离**：
				- 分离：**如果图结构显示给定变量集S的情况下变量集A和变量集B无关，我们声称给定变量集S时，变量集A和变量集B是分离的**
				- d-分离：**如果图结构显示给定变量集S时，变量集A和变量集B无关，那么我们认为给定变量集S时，变量集A d-分离于变量集B**
			- **有向模型和无向模型可以进行进行转换**，每个概率分布可以由有向模型或无向模型，**在最坏的时候，可以使用完全图表示任何分布。**![[Pasted image 20240702163426.png]]
			- 当模型**不包含任何潜变量的时候**，在有向图和无向图之中就需要大量父节点和非常大的团，会导致计算成本呈指数级上升。
			- 传统图模型和非传统图模型：
				- 传统图模型：有向图和无向图
					- 解释性强，能够明确表达变量之间的依赖关系，**但是难以处理大规模数据和复杂的非线性关系**
				- 非传统图模型：图神经网络（GNN）（通过神经网络的方法对图数据进行建模和学习）和消息传递神经网络（特定的GNN，通过节点之间的信息传递实现图的学习）
					- 解释性差，**能够处理大规模数据和学习复杂的非线性关系，有较强的泛化能力。**
			- 结构学习：需要连接那些紧密相关的变量，并忽略其他变量的作用。
			- **深度概率模型**：结合深度学习与概率建模的模型，常见的有**VAE（变分自编码器）、GAN（生成对抗网络）、VI（变分推断）等**，主要应用于**生成图像、文本、音频，半监督学习与无监督学习，异常检测中**。
- #蒙特卡罗方法：
	- 随机算法粗略可以分为两类：
		- **Las Vegas算法**：返回一个精确的答案（正确或者失败），占用随机量的计算资源
		- **蒙特卡罗算法**：返回具有随机大小的错误，花费更多的计算资源。
		- **在任意固定的计算资源下，蒙特卡罗算法可以得到一个近似解**。
	- #为什么需要采样： 有很多原因我们希望在**某个分布中采样**
		- 需要以较小的代价**近似**许多项的和或者某个积分时，采样是一种很灵活的选择。
		- 需要加速一些很费时却易于处理的求和估计。
	- #蒙特卡罗采样基础：当无法精确计算和或者积分（例如，和具有指数数量个项，无法被精确简化） 可以用**蒙特卡罗采样近似**，把和或者积分视作**某分布下的期望**，通过估计**对应的平均值来近似这个期望**。 ^77b896
		- ![[Pasted image 20240703101648.png]]
		- 在求和的时候，p是一个关于随机变量x的概率分布，求积分时，p是概率密度函数
		- **我们可以通过从p中抽取n个样本来近似s来得到一个经验平均值**：
			- ![[Pasted image 20240703101804.png]]
				- 这个近似合理性可以通过**大数定理（如果样本$x^{(i)}$是独立同分布的，那么其平均值几乎必然收敛到期望值）** 和**中心极限定理（$\hat s_n$的分布收敛到s为均值以$\frac{Var[f(x)]}{n}$为方差的正态分布，使得我们可以利用正态分布的累计来估计$\hat s_n$的置信区间）** 来证明。
					- ![[Pasted image 20240703102730.png]]Var（x）表示方差。
			- 但都有一个前提条件：**可以轻易的在基准分布p(x)中采样**。当这个前提条件不存在的时候（即无法从p中采样时），就存在两个方法：[[深度学习（花书二）#^f6f2af|重要采样]]和[[深度学习（花书二）#^4d2068|马尔可夫链蒙特卡罗方法]]
		- #重要采样 ^f6f2af：
			- 对于p(x)和f(x)如何选取并不唯一，我们总是分解为$p(x)f(x)=q(x)\frac{p(x)f(x)}{q(x)}$ ,可以看成$\frac{p(x)f(x)}{q(x)}=f(x)$我们可以从分布q(x)中取样之后求出f(x)的期望，通过![[Pasted image 20240703112439.png]]可以求出。重要采样的估计：![[Pasted image 20240703112627.png]]
			- 什么时候可以使用重要采样，相对于p（x），q（x）更容易采样，或者q(x)预测的方差更小。
		- #马尔可夫链蒙特卡尔方法（构建一个收敛到目标分布的估计序列） ^4d2068：
			- **因为目标分布$p_{model}(x)$的复杂度，难以采样，以及无向模型的特殊性（目标分布由一个没有特定方向的概率图模型表示。以及规范化常数 ZZZ 的计算往往涉及到高维积分或者求和，因此直接计算并不现实，这导致无法使用简单的方法从 $p_{\text{model}}(x)$中采样。）**,所以我们使用近似采样的方法，引入**马尔可夫链**
			- 马尔可夫链：**是一种数学工具，用于描述序列数据的状态转移，无后效性（当前的状态只依赖于前一个状态）** 简单来说分三步
				1. 从一个随机的值x出发
				2. 随着时间推移，反复的更新状态x'
				3. 最终，x成为了一个从p(x)中抽出的（非常接近）比较一般的样本。
				- 正式的定义就是**由随机状态x和转移分布$T(x'|x)$定义，转移分布是一个概率分布，表示给定状态x的情况随机转移到x'的概率。马尔可夫链就是根据转移分布采样出来的x'来更新x**。
				- 马尔可夫链有两种，离散和连续的，当x为连续，期望对应积分，x为离散，对应的时求和。
				- **当马尔可夫链**达到了均衡分布的时候，就结束了，达到这个均衡分布所需的时间就是**混合时间**。但是我们无法预知，所以我们通常估计时间，然后**通过判断生成的样本和样本之间的相关性来判定是否达到达到了均衡分布。**，理想状态下，马尔可夫链采样采出的连续样本之间时完全独立的。
				- 但是马尔可夫链大多数情况都是难以混合的，生成的样本会具有很强的相关性，特别是在高维的情况下，这种现象称为**慢混合或者是混合失败**。
					- 峰值：目标分布中的**局部最大值或者局部密度较高**的区域。具体来说，峰值表示**概率分布函数中局部集中**的区域，通常与**目标分布中概率密度较高的区域相对应。**
					- 挑战：当**目标分布具有多个峰值**时，MCMC方法可能会在**局部峰值附近徘徊，而难以跨越能量较高的隔离区域，从而影响到样本的收敛速度和质量**。使得传统的MCMC方法在收敛到全局分布上表现不佳
					- 通过**回火**来帮助不同峰值之间更好的混合。
						- **回火**：是一种渐进调整目标分布的过程，通过逐**步改变分布的形状**，从而促进MCMC算法更有效地探索整个分布空间，尤其是在处理多峰分布时特别有用。
							- **帮助马尔可夫链在复杂的目标分布中更好地探索和抽样。**
							- 通过温度调度逐步降低温度，从高温阶段开始，马尔可夫链更容易跨越不同能量障碍。
						- **回火转移**：结合重要性采样和回火技术，用于估计复杂分布的归一化常数或进行积分计算。
							- 通过从简单分布到目标分布的渐进过程，提高采样效率并减少估计的方差。
						- **并行回火**：多条马尔可夫链并行运行，在不同温度下进行抽样，周期性地交换样本，以提高跨越能量障碍的概率。
							- 加速MCMC方法的收敛速度，特别是在处理高维或复杂分布时尤为有效。
	- #GIbbs采样
		- 前面说到了马尔可夫链，但是转移分布T如何选择。
			- 通过已经学习到的$p_{model}$中推导T
				- Gibbs采样：用于从**复杂的多维概率分布中**采样。它通过**逐一采样每个变量的条件分布**来构造一个马尔可夫链，使得生成的样本可以逐渐逼近目标分布。![[Pasted image 20240703145423.png]]
			- 直接使用参数描述T，然后学习参数。
- #配分函数：一个概率分布的所有可能状态总和，用于将未归一化的分布进行归一化
	- 对于连续变量：配分函数积分：![[Pasted image 20240704152923.png]]
	- 对于离散变量：配分函数求和：![[Pasted image 20240704153002.png]]
	- 正相：表示模型对实际数据响应/行为，关注如何让模型尽可能拟合真实数据的分布。
	- 负相：模型对其自身生成的数据的响应/行为。
	- 通过**概率分布/配分函数**可以得到正则化的概率分布
		- 但是对于大部分模型来说，积分和求和难以处理，所以**如果有效的近似求出配分函数或者如何绕过这个计算的问题**。
		- 近似求出（通过估计配分函数的梯度）：
			- 最大似然法：需要求模型概率分布相对于参数α的对数似然![[Pasted image 20240704153448.png]]
				- 分解出两项，正相和负相。但是负相并不好求出。
				- 通过配分函数定义可以将**负相转换为期望形式**，然后通过[[深度学习（花书二）#^77b896]]方法近似求出。
				- 思路分为三步：
					- ![[Pasted image 20240704153812.png]]
					- ![[Pasted image 20240704153825.png]]
					- 但是这种方法，**每次都需要重新随机初始化马尔可夫链，达到平稳需要花很长时间**，为了加速，引出**对比散度**
					- **对比散度**：利用minibatch真实的数据样本来初始化马尔可夫链。
						- 最初模型和数据分布存在较大差异，但是若干次后，模型分布就会趋近于数据分布了。
						- 但是如果模型分布**过度偏离数据分布**，负相计算误差过大，为了改进引入**持续对比散度/随机最大似然**
					- **随机最大似然**：对于每个minibatch，马尔可夫链不需要重新初始化，延续上一个minibatch的马尔可夫链状态。
						- 因为，**假定两次步进之间模型差异较小**，所以可以延续上一次minibatch的马尔可夫链的状态。
				- 基于MCMC从模型中抽取样本的方法都可以于MCMC的变体一起使用。**eg：随机最大似然可以于增强MCMC的技术（回火）来改进**。
		- 绕过求值问题：大部分通过一个：**无向模型概率中很容易计算概率的比率（因为配分函数同时出现在比率的分子和分母中，可以相互抵消）**
			- **伪似然**：![[Pasted image 20240705111534.png]]
			- **得分匹配**：最小化模型对数密度和数据对数密度关于输入的导数之间的平方差期望。![[Pasted image 20240705111545.png]]
				- **去噪得分估计**：在实践中，通常不能获取真实的数据概率分布。所以我们需要用**去噪得分估计**![[Pasted image 20240705112518.png]]
			- **比率匹配**：![[Pasted image 20240705111623.png]]
			- **噪声对比估计(NCE)**：用某个变量来近似配分函数![[Pasted image 20240705115254.png]]c是$-logZ(θ)$的近似，模型使用相同算法估计/学习θ和c。
				- 当转化这种形式后，我们**无法再用最大似然估计方法**，因为它会选择将 𝑐 设置的无限大，而不是将其定为一个合理的概率分布。
				- 所以**噪声对比估计**算法将将无监督学习的估计 $𝑝(\hat{𝑥})$ 的问题转化为一个**概率二分类器的监督学习问题**
				- **分为两类**：一类是**模型产生数据**，另一类是新引入的**噪声分布$p_{noise}(\hat{x})$** 。该**噪声分布易于评估与取样**。现在，我们就可以构建一个新的分类模型，**其输入为 $\hat{x}$ ，二元分类标记为 𝑦 **。![[Pasted image 20240705120202.png]]
				- 当NCE应用在**许多随机变量**问题，效率会很低，当逻辑回归分类器认为一个变量取值不可能时，就会拒绝。**当$p_{model}$学习到基本的边缘统计后，学习进程就会变得很慢**这会导致如果在进行学习人脸的时候，当学习到一个部分特征的时候，就不会去学习其他地方的特征。
				- **NCE的基本思想是好的模型可以分辨实际数据与噪声，但在对抗生成网络中，好的生成模型可以产生使分类器无法分辨的样本。**
	- 当我们需要直接配分函数来求得**归一化后的概率分布**，这个时候如何得到**配分函数的值**？
		- 可以将配分函数转成比率
		- 通过估计比率求出
		- 其中一种方法**使用蒙特卡罗方法（简单重要采样）**
			- **假设要求的配分函数是**![[Pasted image 20240705154137.png]]但是不好处理这个配分函数，转换为![[Pasted image 20240705154204.png]]
			- **利用重要采样**：![[Pasted image 20240705154227.png]]
			- 这种方法**只能用于分布$p_0$接近$p_1$的时候才能有效地估计配分函数**，但是大部分情况$p_1$是很复杂的（通过是多个峰值），并且定义在高维中，很难求出一个易解的$p_0$。简单来说就是**很难找到一个简单的分布𝑝0既方便计算有能接近𝑝1分布，这个时候 𝑍1 的方差很大，无法有效的近似。** 这时候可以通过两种方法来解决**[[深度学习（花书二）#^e26a1c|退火重要采样]]和[[深度学习（花书二）#^3b482b|桥式采样]]**，通过**引入缩小$p_0$和$p_1$之间的差距的中间分布，来解决$p_0$远离$p_1$的问题。**
		- **退火重要采样**：在$D_{KL}(p_0||P_1)$很大情况(**KL代表经验分布与模型分布之间的差异**，![[Pasted image 20240705164559.png]]
		- 基本思想是用**一系列的中间分布来连接$p_0$** 和$P_1$之间差距，即用一系列分布 $P_{n_0},...,p_{n_n}$其中![[Pasted image 20240705163657.png]]第一个分布为 $P_0$，最后一个分布为$P_1$，由此![[Pasted image 20240705163752.png]]如果我们保证$p_{n_j}$和$p_{n_{j+1}}$足够接近，可以使用重要采样来近似每个![[Pasted image 20240705163903.png]]，最终可以得到![[Pasted image 20240705163914.png]]。实际操作中，**通常使用加权几何平均来得到中间的概率分布**![[Pasted image 20240705164013.png]]^e26a1c
			- 为了中间分布中采样，**定义了一组马尔可夫链转移函数$T_{n_j}(x'|x)$，定义了给定x转移到x‘的条件概率分布**，转移算子$T_{n_j}(x'|x)$定义如下![[Pasted image 20240705164301.png]]，保持$P_{n_{j}}{x}$不变。
			- 退火重要采样是**估计无向概率模型的配分函数的最常用方法**。
		- **桥式采样**：通过单个中间分布$P_*$ 来过渡$P_0$和$P_1$![[Pasted image 20240705171906.png]],如果$P_*$ 选取合适（**$P_0$和$P_1$均有较大的重合**），可以在$P_0$和$P_1$之间构建一个沟通的桥梁。**最优的桥式结构是![[Pasted image 20240705172055.png]]**，**但是实际不知道r**，**我们通常都是选择一个估计值，然后通过迭代来更新r的值**。^3b482b
		- **这两个算法中，如果$D_{KL}(P_0||P_1)$不太大的话可以使用桥式采样，但是如果过于大的话，桥式采样无法连接。只能用退火重要采样。**
		- 但在实际训练中。**退火重要采样计算代价高，训练中不实用**。
		- PS：**研究者将桥式采样，退火重要采样，并行回火进行组合得到另一个算法。（更多详细请阅读文献）**