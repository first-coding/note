- #线性因子模型：
	- 线性因子：是指**影响观察到的变量之间关系的潜在因素或潜在变量**。
	- 可以理解为一种建模工具
	- **数据降维/特征学习**
	- **数据生成/重构**
	- **异常检测和数据清洗**：
	- **预测与推荐**
	- **数据压缩/可视化**
- #自编码器：  ^4e5970
	- 传统的自编码器：
		- 神经网络的一种，**经过训练可以尝试将输入复制到输出**，但是如果**输入输出一样那就没意义**，
			- 我们通常**在自编码器强加一些约束，只能近似的复制**。
		- 由两部分组成
			- **h=f(x)的编码器**和**生成重构的r=g(h)解码器**组成。
	- 现代自编码器：
		- 将**确定的函数h=f(x)** 这种推广为**随机映射$P_{encoder}(h|x)$和$p_{decoder}(x|h)$**。
	- 分类：
		- **欠完备自编码器**：得到有用特征，我们将**h的维度比x小**，但是**需要限制容量，如果太大，那么就可能出现学习到有用的信息。**
		- **正则自编码器**：**自编码器的损失函数**中**加入额外的正则项**，以**约束模型参数的大小或模型的复杂度**，以**防止过拟合**
			- **稀疏自编码器**：在损失函数加上**稀疏乘法Ω(h)**，$L(x,g(f(x)))+Ω(h)$ ,一般用于**学习特征，以便用于分类这样的任务** ^2d9e76
			- **去噪自编码器**：**接收损坏数据作为输入**，并训练来预测**原始未被损坏数据**作为输出的**自编码器**
				- 根据以下过程，**从训练数据对$(x,\widetilde{X})$ 中学习重构分布**
					- 训练数据采一个训练样本x
					- 从$C(\widetilde{X}|X=x)$采一个损坏样本$\widetilde{X}$
					- 将$x,\widetilde{x}$ 作为训练样本估计**自编码器的重构分布**。
			- **收缩自编码器**：和[[深度学习（花书二）#^2d9e76|稀疏自编码器]]一样，都是在损失函数中加入**惩罚项**实现额外的约束。收缩自编码器$Ω(h)=λ||\frac{αf(x)}{αx}||^2_F$ ，是**平方Frobenius范数（元素平方之和）**
		- **参数化自编码器**：编码器和解码器都是由**参数化的函数表示**
			- **变分自编码器**：编码器和解码器都是由**参数化的神经网络**表示的，并且模型的训练过程基于**最大化变分下界**（Variational Lower Bound）。
	- 作用：**数据压缩与降维**，**去噪**，**特征学习**，**生成模型（变分自编码器和GAN等自编码器的变体可以生成新的数据样本）**，**数据重建**。
- #表示学习
	- #无监督 
		- 没有标签，即没有对应y对应输入。
		- **贪心逐层无监督预训练**：这个被用于规避监督问题中深度神经网络难以联合多层的问题。
			- 通过逐层训练模型，每一层的训练都是无监督的，也就是没有使用标注的数据。在训练过程中，模型会逐渐学习到数据中的特征，并将这些特征编码成更高层次的表示。
			- **贪心**：是因为是一个贪心算法，**意味着可以独立地优化解决方案的每一个部分，每一步解决一个部分**。
			- **逐层**：独立的解决方案是**网络层**。**每一次都是处理一层网络，保持前面的网络层不变**
			- **无监督**：每一层用**无监督表示学习算法训练**。
			- **预训练**：**在联合训练算法精调所有层之前第一步**。
			- **何时使用/有效原因**：无监督预训练基于两个想法
				- **利用深度神经网络对初始参数的选择可以对模型有着显著的正则化效果（在较小程度上，改进优化）**。
				- **利用更一般的想法：学习输入分布有助于学习从输入到输出的映射**
				- 对于**词嵌入**，无监督训练在**处理单词特别有用**。对于**处理图像则差点**。
			- **存在两个缺点**：
				- 使用了**两个单独的训练阶段，具有许多超参数，但是效果需要之后才能度量，难以提前预测**。
				- **每个阶段都有各自的超参数，第二阶段的性能通常不能用第一阶段来进行预测**。
			- **大多数的算法已经不使用无监督与训练，除了在one-hot向量的自然表示不能传达相似的信息的地方使用**。
			- 基于这个技术，已经推广到[[深度学习（花书）#^21258f|监督预训练]]。
	- #迁移学习
		- 将知识从一个任务或领域迁移到另一个任务或领域，**关注在不同任务或领域之间共享知识，以改善目标任务的性能**
		- #领域自适应
			- 迁移学习特定情况，专注解决**源和目标之间的分布差异，关注如果让模型在目标上性能更好，不需要在目标域上有标记的数据**。
		- #概念漂移
			- 指**数据分布或数据生成过程随时间发生变化**，从而导致**训练模型的性能下降**的现象。
		- #一次学习和零次学习
			- **只有一个标注样本**，一次学习
				- 一次学习是有可能的，**通过一个标注样本推断表示空间中聚集在周围的样本标签**。
			- **没有任何标注样本**，零次学习
				- 只有**在训练时使用了额外信息**，零次学习才有可能。**通常认为存在三个随机变量(x,y,T),x为输入，y为输出，T即描述任务的附加随机变量**.
				- **多模态学习**：利用多种不同类型的数据（模态），进行联合建模和学习的方法
	- #表示学习的一个重要问题：
		- **什么原因能够使一个表示比另一个表示更好**。
			- 一个假设：理想表示中的特征对应到观察数据的潜在成因。
			- **如果表示向量h表示观察值x的很多潜在因素，并且输出向量y最为重要的原因之一，那么h预测y很容易。**
				- 当如果p(x)是均匀分布的，则无助于学习p（y|x）.
				- 当如果是混合分布，混合分量清晰的分开，建模p（x）可以精确指出每个分量的位置，那**每个类一个标注样本的训练集可以精确学习p(y|x)**。
				- 但是**什么能将p（y|x）和p（x）关联在一起**。
					- 如果y和x的成因之一非常相关，边缘概率p（x）和条件概率p（y|x）会紧密关联。无监督表示学习可以会和半监督一样有用。
					- 假设y和x的成因之一，h表示所有这些成因。但是在现实中无法实现（**不可能捕获影响观察的所有或大多数变化因素** ）。现实中人们只会察觉和进行任务相关的内容。**根据这点，我们需要确定编码什么**。
						- 同时使用无监督学习和监督学习信号，**从而使得模型捕获最相关的变动因素**。
						- 使用纯无监督学习学习更大规模表示。
					- 另一个思路就是，选择一个影响最大的潜在因素。
	- #分布式表示
		- **分布式表示**：通过**多个特征共同表示一个概念**。每个特征单独可能没有明确的意义，但是组合起来可以描述复杂的概念。
			- 特点：2
				1. **多维特征**：一个输入数据被映射到一个高维的向量空间，其中每个维度都是一个特征。
				2. **稀疏性**：表示通常是稀疏的，即大多数特征的值可能接近零。
				3.  **共享表示**：相似的输入数据在表示空间中会有相似的表示。这种共享的特性使得模型可以更好地泛化到未见过的数据。
				4. **非线性特性**：通过深度神经网络的层层映射，可以捕捉数据中的非线性关系和复杂特性。
		- **非分布式表示**：指**一个概念由单个特征或少数几个特征明确表示**，**每个特征都有明确的意义。**
			- 特点：
				1. **独立特征**：每个特征独立地表示一个概念或属性，没有其他特征参与。
				2. **低维度**：通常表示维度较低，每个特征都有明确的物理或语义解释。
				3. **固定特性**：表示通常是固定的，没有通过学习来调整或优化。
				4. **有限表达能力**：无法有效捕捉数据中的复杂关系和高阶特性。
		- 分布式表示具有更强的表达能力，**可以捕获数据的高阶特性和非线性关系**
		- **训练与优化**：
			-  分布式表示通过学习过程优化和调整。
			- 非分布式表示通常是固定的，不通过学习来优化。
- #结构化概率模型 ^298aaf
	- **使用图来描述概率分布中随机变量之间的直接相互作用，从而描述一个概率分布**。
		- 难点在于：**如何判断那些变量之间存在直接的相互作用关系**。
	- 结构化概率模型主要优点**可以显著降低表示概率分布、学习和推断的成本。** 对于有向模型还可以被加速。
	- 环：由无向边连接的变量序列，并且满足序列中的**最后一个变量连接回序列中的第一个变量**。
	- 弦：定义环序列中任意两个非连接变量之间的连接。
	- #非结构建模挑战：
		- 对于一些问题**我们可以忽略部分输入只需要关键的地方。**
		- 对于另一些问题**需要对输入数据整个结构完整理解，不能忽略部分的输入**。
			- **估计密度函数**：输入x，返回一个对数据生成分布的真实密度函数p(x)的估计。
			- **去噪**：输入受损/观察有误的输入数据$\widetilde{x}$ ,返回对原始的真实x的估计。
			- **缺失值的填补**：输入x的某些元素，返回一个x一些或者全部未观察值的估计或者概率分布。
			- **采样**：从概率分布p(x)中抽取新的样本，对于一些需求来说，语音合成，**模型需要多个输出以及对输入整体的良好建模**。
		- **当我们对有n个离散变量并且每个变量可以取k个值得x**，最简单得建模就是存储一个可以查询表格，纪录每一种可能值得概率，**需要$k^n$参数**。但是因为**内存大小，高效性以及运行时间来说**所以现实不可行。
		- 所以**结构化概率模型为随机变量之间的直接作用提供一个正式得建模框架**
			- **大大减少了模型的参数个数**，以至于只需要更少的数据就能进行有效的估计。
			- **更小的模型大大减小了在模型存储、推断以及采样时的计算开销**.
	- #图描述模型结构 
		- 图(graph)可以很好的表示随机变量之间的作用关系**其中每个节点(node)代表了一个随机变量，而每条边(edge)代表了两个随机变量之间有直接作用，而连接不相邻节点的路径(path)就代表了简介相互作用**。分两种
			- **有向图**：也叫贝叶斯网络，边有方向，从一个结点指向另一个结点，**方向表示条件概率分布**。eg：a指向b，**代表b的概率分布依赖于a**。![[Pasted image 20240702143408.png]]**优点是可以通过一个简单高效的过程从模型所表示的联合分布中产生样本，这个过程称为原始采样（只适用于有向图）**
				- 有向图用于信息流动方向比较明确的问题，对于因果不明确的问题，需要用到无向图
			- **无向图**：也被叫做**马尔科夫随机场(Markowv random fields)** **或**马尔科夫网络(Markov Network)。**边没有方向，不代表条件概率**。![[Pasted image 20240702143613.png]]无向图可以通过转换成有向图进行采样，**也可以通过Gibbs采样**。
				- 无向模型所有的理论结果依赖于**对于所有的x，$\widetilde{p}(x)>0$的假设**。满足这个条件的一个模型为![[Pasted image 20240702143923.png]]保证所有状态的概率大于零。**服从这个公式的任意分布都是玻尔兹曼分布**。
				- 因子图：在于将复杂的概率分布分解成多个简单的因子函数的乘积。
			- **分离和d-分离**：
				- 分离：**如果图结构显示给定变量集S的情况下变量集A和变量集B无关，我们声称给定变量集S时，变量集A和变量集B是分离的**
				- d-分离：**如果图结构显示给定变量集S时，变量集A和变量集B无关，那么我们认为给定变量集S时，变量集A d-分离于变量集B**
			- **有向模型和无向模型可以进行进行转换**，每个概率分布可以由有向模型或无向模型，**在最坏的时候，可以使用完全图表示任何分布。**![[Pasted image 20240702163426.png]]
			- 当模型**不包含任何潜变量的时候**，在有向图和无向图之中就需要大量父节点和非常大的团，会导致计算成本呈指数级上升。
			- 传统图模型和非传统图模型：
				- 传统图模型：有向图和无向图
					- 解释性强，能够明确表达变量之间的依赖关系，**但是难以处理大规模数据和复杂的非线性关系**
				- 非传统图模型：图神经网络（GNN）（通过神经网络的方法对图数据进行建模和学习）和消息传递神经网络（特定的GNN，通过节点之间的信息传递实现图的学习）
					- 解释性差，**能够处理大规模数据和学习复杂的非线性关系，有较强的泛化能力。**
			- 结构学习：需要连接那些紧密相关的变量，并忽略其他变量的作用。
			- **深度概率模型**：结合深度学习与概率建模的模型，常见的有**VAE（变分自编码器）、GAN（生成对抗网络）、VI（变分推断）等**，主要应用于**生成图像、文本、音频，半监督学习与无监督学习，异常检测中**。
- #蒙特卡罗方法：
	- 随机算法粗略可以分为两类：
		- **Las Vegas算法**：返回一个精确的答案（正确或者失败），占用随机量的计算资源
		- **蒙特卡罗算法**：返回具有随机大小的错误，花费更多的计算资源。
		- **在任意固定的计算资源下，蒙特卡罗算法可以得到一个近似解**。
	- #为什么需要采样： 有很多原因我们希望在**某个分布中采样**
		- 需要以较小的代价**近似**许多项的和或者某个积分时，采样是一种很灵活的选择。
		- 需要加速一些很费时却易于处理的求和估计。
	- #蒙特卡罗采样基础：当无法精确计算和或者积分（例如，和具有指数数量个项，无法被精确简化） 可以用**蒙特卡罗采样近似**，把和或者积分视作**某分布下的期望**，通过估计**对应的平均值来近似这个期望**。
		- ![[Pasted image 20240703101648.png]]
		- 在求和的时候，p是一个关于随机变量x的概率分布，求积分时，p是概率密度函数
		- **我们可以通过从p中抽取n个样本来近似s来得到一个经验平均值**：
			- ![[Pasted image 20240703101804.png]]
				- 这个近似合理性可以通过**大数定理（如果样本$x^{(i)}$是独立同分布的，那么其平均值几乎必然收敛到期望值）** 和**中心极限定理（$\hat s_n$的分布收敛到s为均值以$\frac{Var[f(x)]}{n}$为方差的正态分布，使得我们可以利用正态分布的累计来估计$\hat s_n$的置信区间）** 来证明。
					- ![[Pasted image 20240703102730.png]]Var（x）表示方差。
			- 但都有一个前提条件：**可以轻易的在基准分布p(x)中采样**。当这个前提条件不存在的时候（即无法从p中采样时），就存在两个方法：[[深度学习（花书二）#^f6f2af|重要采样]]和[[深度学习（花书二）#^4d2068|马尔可夫链蒙特卡罗方法]]
		- #重要采样 ^f6f2af：
			- 对于p(x)和f(x)如何选取并不唯一，我们总是分解为$p(x)f(x)=q(x)\frac{p(x)f(x)}{q(x)}$ ,可以看成$\frac{p(x)f(x)}{q(x)}=f(x)$我们可以从分布q(x)中取样之后求出f(x)的期望，通过![[Pasted image 20240703112439.png]]可以求出。重要采样的估计：![[Pasted image 20240703112627.png]]
			- 什么时候可以使用重要采样，相对于p（x），q（x）更容易采样，或者q(x)预测的方差更小。
		- #马尔可夫链蒙特卡尔方法（构建一个收敛到目标分布的估计序列） ^4d2068：
			- **因为目标分布$p_{model}(x)$的复杂度，难以采样，以及无向模型的特殊性（目标分布由一个没有特定方向的概率图模型表示。以及规范化常数 ZZZ 的计算往往涉及到高维积分或者求和，因此直接计算并不现实，这导致无法使用简单的方法从 $p_{\text{model}}(x)$中采样。）**,所以我们使用近似采样的方法，引入**马尔可夫链**
			- 马尔可夫链：**是一种数学工具，用于描述序列数据的状态转移，无后效性（当前的状态只依赖于前一个状态）** 简单来说分三步
				1. 从一个随机的值x出发
				2. 随着时间推移，反复的更新状态x'
				3. 最终，x成为了一个从p(x)中抽出的（非常接近）比较一般的样本。
				- 正式的定义就是**由随机状态x和转移分布$T(x'|x)$定义，转移分布是一个概率分布，表示给定状态x的情况随机转移到x'的概率。马尔可夫链就是根据转移分布采样出来的x'来更新x**。
				- 马尔可夫链有两种，离散和连续的，当x为连续，期望对应积分，x为离散，对应的时求和。
				- **当马尔可夫链**达到了均衡分布的时候，就结束了，达到这个均衡分布所需的时间就是**混合时间**。但是我们无法预知，所以我们通常估计时间，然后**通过判断生成的样本和样本之间的相关性来判定是否达到达到了均衡分布。**，理想状态下，马尔可夫链采样采出的连续样本之间时完全独立的。
				- 但是马尔可夫链大多数情况都是难以混合的，生成的样本会具有很强的相关性，特别是在高维的情况下，这种现象称为**慢混合或者是混合失败**。
					- 峰值：目标分布中的**局部最大值或者局部密度较高**的区域。具体来说，峰值表示**概率分布函数中局部集中**的区域，通常与**目标分布中概率密度较高的区域相对应。**
					- 挑战：当**目标分布具有多个峰值**时，MCMC方法可能会在**局部峰值附近徘徊，而难以跨越能量较高的隔离区域，从而影响到样本的收敛速度和质量**。使得传统的MCMC方法在收敛到全局分布上表现不佳
					- 通过**回火**来帮助不同峰值之间更好的混合。
						- **回火**：是一种渐进调整目标分布的过程，通过逐**步改变分布的形状**，从而促进MCMC算法更有效地探索整个分布空间，尤其是在处理多峰分布时特别有用。
							- **帮助马尔可夫链在复杂的目标分布中更好地探索和抽样。**
							- 通过温度调度逐步降低温度，从高温阶段开始，马尔可夫链更容易跨越不同能量障碍。
						- **回火转移**：结合重要性采样和回火技术，用于估计复杂分布的归一化常数或进行积分计算。
							- 通过从简单分布到目标分布的渐进过程，提高采样效率并减少估计的方差。
						- **并行回火**：多条马尔可夫链并行运行，在不同温度下进行抽样，周期性地交换样本，以提高跨越能量障碍的概率。
							- 加速MCMC方法的收敛速度，特别是在处理高维或复杂分布时尤为有效。
	- #GIbbs采样
		- 前面说到了马尔可夫链，但是转移分布T如何选择。
			- 通过已经学习到的$p_{model}$中推导T
				- Gibbs采样：用于从**复杂的多维概率分布中**采样。它通过**逐一采样每个变量的条件分布**来构造一个马尔可夫链，使得生成的样本可以逐渐逼近目标分布。![[Pasted image 20240703145423.png]]
			- 直接使用参数描述T，然后学习参数。