- **展开计算图**：
	- $s^{(t)}=f(s^{(t-1)};θ)$ 
		- $s^{(t)}$ 称为系统的状态。
		- $s^{(3)}=f(s^{(2)};θ)=f(f(s^{(1)};θ);θ)$ 
		- ![[Pasted image 20240420145334.png]]
		- ![[Pasted image 20240420145835.png]]
	- $s^{(t)}=f(s^{(t-1)},x^{(t)};θ)$ 
		- 循环神经网络可以通过许多不同方式建立，**所有涉及循环的函数都可以视为一个循环神经网络**。
		- $h^{(t)}=f(h^{(t-1)},x^{(t)};θ)$ 
			- 通过上面这个式子或类似公式定义隐藏单元的值。
			- 存在两个方式可以进行绘制
				- **在模型的物理实现中存在的部分赋予一个节点，生物神经网络**。
				- **绘制展开的计算图**，存在两个主要优点
					- 无论序列的长度，学成的模型始终具有相同的输入大小（**指定的是一种状态到另一种状态的转移，而不是在可变长度的历史状态操作**）。
					- 每个时间步使用相同参数的相同转移函数f。
-  **循环神经网络：一类用于处理序列数据的神经网络。**
	- 专门用于处理序列$x^{(1)},...,x^{(T)}$ 的神经网络
		- 循环网络可以拓展到更长的序列。
		- 大多数循环网络可以处理可变长度的序列。
	- **循环神经网络重要的设计存在以下几种**：
		- **每个时间步都有输出**，并且**隐藏单元之间有循环连接**的循环网络。
			- ![[Pasted image 20240420150730.png]]
				- L：是损失衡量每个o与相应得训练目标y的距离。
				- x：输入序列。
				- U，W，V是权重矩阵。
				- o：是通过x值输入序列映射的输出值o
					- 使用softmax输出，$\hat{y}=softmax(o)$ 
				- **$a^{(t)}=b+Wh^{(t-1)}+Ux^{(t)}$ ，输入单元的输出，隐藏单元的输入。**
				- **$h^{(t)}=tanh(a^{(t)})$ ，通过双曲正切激活函数（tanh简写(th)），隐藏层输出。**
				- **$o^{(t)}=c+Vh^{(t)}$ ，是输出层的输入。**
				- **$\hat{y}^{(t)}=softmax(o^{(t)})$ 获得标准化后概率的输出向量$\hat{y}$ **
			- 任何图灵可计算的函数都可以通过这样的一个有限维的循环网络计算。
		- **每个时间步都产生一个输出**，只有**当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络**
			- ![[Pasted image 20240420151413.png]]
				- 不能模拟通用图灵机，因**缺少隐藏到隐藏的循环**
				- 可以**并行化训练**，任何基于比较时刻t的预测和时刻t的训练目标的损失函数所有时间步都解耦了。**即在各时刻t分别计算梯度。**
				- 唯一的循环时在输出到隐藏层的反馈机制中。
				- $o^{(t)}$是输出，$y^{(t)}$ 是目标，$L^{t}$ 是损失。
			- **导师驱动过程**：一种训练技术，适用于**输出与下一时间步隐藏状态存在链接的RNN**，即上图这种类型的![[Pasted image 20240423160400.png]]
				- 导师驱动过程不再使用最大似然准则（**基于观察到的数据来估计模型的参数，使得给定观察到的数据样本的情况下，所估计的参数值使得观察到这些数据的概率最大化。**），**而是在t+1时刻接收真实值$y^{(t)}$ 作为输入。** 最大条件似然准则如下：![[Pasted image 20240423160528.png]]
				- **开环**：网络输出（或输出分布的样本）反馈作为输入。
					- 这种情况下，使用导师驱动过程就会导致**训练的输入和测试的输入存在很大的不同**。
		- **隐藏单元之间存在循环连接**，但**读取整个序列后产生单个输出的循环网络**。
			- ![[Pasted image 20240420151634.png]]
				- 在序列结束时具有**单个输出**。可以**用于概括序列并产生进一步处理的固定大小表示**。
	- 通过[[深度学习（花书）#^7d08ea|反向传播]]并结合通用的**基于梯度**的技术即可训练RNN。
		- **通过时间反向传播（BPTT）**：
			- 是反向传播算法在RNN中的拓展通过时间上展开的网络结构，将损失函数相对于隐藏状态和网络参数的梯度累积起来。
			- 专门用于序列数据的处理。