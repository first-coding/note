1. **LORA**：
	- 只训练极小的参数，同时保持全量微调所能达到的性能。**通过在某些特定层/所有层添加两个低秩矩阵（矩阵的秩度量的矩阵行列之间的相关性）。**
	- 简单来说，**冻结一个预训练模型的矩阵参数，并选择用A和B矩阵来替代，在下游任务时只更新A和B**。![[Pasted image 20241018172824.png]]
	- 用数学来说，**通过修改线性层中的权重矩阵**，$$(ω+Δω)x=ωx+ABx$$其中，**ω为原始权重矩阵，Δω=AB**。即将**Δω进行低维分解**
	- 实现流程：
		1. 将$Y=ωx$转化为$Y=(ω+Δω)x$，**Δω是微调得到的结果**
		2. Δω进行**低维分解**$Δω=AB$
		3. 运用数据，训练出A和B即可得到Δω，**在推理的过程，直接把Δω加到ω上去，即无额外的成本**
	- **如果需要LORA适配不同的场景，可以尝试在进行LORA微调**
	- LORA存在多种不同的变体，**LORA-FA,VeRA,Delta-LoRA,QLoRA.......**
	-  https://github.com/microsoft/LoRA
1. **Adapter**：
	- 在预训练模型**每一层（或者某些层）** 添加Adapter模块。**微调时冻结预训练模型主体，由Adapter模块学习特定下游任务的知识**，Adapter**只对新插入的Adapter层进行微调，而不是微调整个模型**。**显著减少了参数量和计算资源**
	- Adapter模块结构：
		- ![[Pasted image 20241018175436.png]]
		- Adapter由以下部分组成
			- **降维层（Down_projection）**：通过一个线性层对输入维度降低到较小的维度(**通常是瓶颈维度** )，**以减少计算量**
			- **非线性激活函数（Nonlinearity）**：降维后，**应用非线性激活函数来增强模型的表达能力**
			- **升维层（Up_projection）**：通过一个线性层将降维后的向量升维到原始维度
			- **残差连接（Residual connection）**：Adapter层通常与主模型采用残差连接，**即输入的特征不仅经过Adapter层，与原始特征进行相加**，**为了保持原模型的性能稳定**
	- 实现流程：
		- **冻结预训练模型的所有权重**：模型的大部分参数不会更新，以减少计算开销
		- **插入Adapter层**：在模型的不同层插入Adapter层
		- **微调Adapter层**：仅对这些插入的 Adapter 层的参数进行微调，学习特定任务的特征。
		- **保留Adapter层**：微调完成后，Adapter 层可以在不同的任务中共享使用，这使得一个模型可以方便地扩展到多个任务，而不需要重新训练整个模型。
	- **Adapter存在多个变体，Houlsby Adapter，Compacter，Parallel Adapter......**
1. **Prefix-tuning**：
2. **P-tuning**：
3. **Prompt-tuning**：
4. **全量微调**：