- 基础概念：
	- **实变函数**：**以实数作为自变量的函数**叫做实变函数 ^159ef4
	- **Toeplitz矩阵**：矩阵**每一行中的元素**都与**上一行对应位置平移一个单位的元素相同**。
	- **循环矩阵**：其列向量 / 行向量的每个元素都是前一个列向量 / 行向量个元素循环右移一个位置的结果。![[Pasted image 20240415170312.png]]
	- **分块矩阵**：将一个**大矩阵分解成多个较小的子矩阵**，并按照**一定规则排列组合**得到的**矩阵结构**。这种分块可以是按行分块、按列分块或者更复杂的方式。
	- **分块循环矩阵**：分块循环矩阵是指将**整个矩阵分成多个块**，每个**块内的元素是循环的**，即矩阵的每个块都是一个循环矩阵。![[Pasted image 20240415170614.png]]
	- **双重分块循环矩阵**：是在分块循环矩阵的基础上再进行一次分块。它将每个分块循环矩阵再次分成更小的块，并且这些小块内的元素同样是循环的。
	- **上采样**：增加数据的分辨率或尺寸。
	- **下采样**：降低数据的分辨率或尺寸。
- ###### **卷积神经网络（CNN）**：专门用于处理具有类似网格结构的数据的神经网络。
	- #网络结构的数据
		- **时间序列（看作在时间轴有规律地采样形成的一维网格）**
		- **图像数据（看作二维的像素网格）**
	- #卷积数学运算：是一种特殊的线性运算。
		- 卷积是对两个[[卷积神经网络#^159ef4|实变函数]]的一种数学运算。
		- 用一个例子来说明卷积运算
			- 用激光传感器来追踪一艘飞船位置，对于每个时刻t都存在对应的位置**即x(t)**，但是存在某些噪声影响，我们需要对他进行**加权平均处理ω(a)**，**时间之间越靠近测量结果（就是飞船位置）就越相关**，a即测量结果和当前时刻的时间间隔。即得到**卷积积分**$$s(t)=\int{x(a)ω(t-a)}da\quad\quad\quad\quad(9.1)$$
			- **卷积运算用 * 表示**：$s(t)=(x*ω)(t)$，在这个公式中，ω必须是一个**有效的概率密度函数**，当参数为负值，ω必须为0（这个限制是基于上面的例子）。
			- 在例子中，不存在每个时刻的位置瞬间反馈过来的情况，所以**我们定义每一秒获取一个位置**这样得到的是离散型的数据，t只能取整数值，x和ω定义在整数时刻上，得到**离散形式的卷积**：$$s(t)=(x*ω)(t)=\sum_{a=-\infty}^{\infty}x(a)ω(t-a)$$**离散形式的卷积可以看作矩阵的乘法**，但存在限制，**一些元素被限制必须和另外一些元素相等**。**卷积通常对应一个非常稀疏的矩阵（一个几乎所有元素都为零的矩阵）**
			- 在深度学习/机器学习中，通常会**一次在多个维度进行卷积运算**，比如对一张二维图片I作为输入，我们也会用一个二维的核K：$$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(m,n)K(i-m,j-n)$$因为卷积是可交换的，基于**我们对核相对输入进行翻转**得到$$S(i,j)=(K*I)(i,j)=\sum_m\sum_nI(i-m,j-n)K(m,n)$$
			- 在大多数的神经网络库中都会实现一个函数**互相关函数（和卷积运算几乎一样，但是没有对核进行翻转），通常机器学习库实现的就是这个，然后叫做卷积**：$$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(i+m,j+n)K(m,n)$$
		- **满足上面9.1的积分式的任意函数就是卷积**。
		- 在卷积网络中，卷积的第一个参数（**函数x**）在上面的例子为**输入**，第二个参数（**函数ω**）叫做**核函数**，输出可以称为**特征映射**。
		- ![[Pasted image 20240415171041.png]]
		- 具有多种卷积计算方式，比如**二维FUll卷积、二维Same卷积.......**
	- #卷积通过三种思想改进机器学习系统：
		- #稀疏交互：
			- 传统神经网络使用**矩阵乘法**建立输入与输出的关系。即**每一个输入单元都会与每一个输出单元进行全连接**。
			- 卷积神经网络**神经元只与输入数据的一个局部区域连接**，而不是与整个输入层的所有神经元连接。**即不是每一个输入单元都与每一个输出单元进行交互**，而是通过**滤波器（卷积核）的滑动来实现局部连接。**![[Pasted image 20240415172349.png]]
		- #参数共享： 
			- 一个模型的多个函数使用相同的参数，用传统的神经网络和卷积神经网络相比。**输入同样的图像，全连接层的网络每一个神经元都会进行计算，然后使用不同的参数。卷积神经网络通过n * n的卷积核，每个图像都用相同的卷积核的参数进行计算**。用一个例子说明
				- 对于相同的4 * 4的输入图像
					- 传统神经网络，有16个输入神经元，5个隐藏神经元，那么就存在着16 * 5=80个参数。
					- 卷积神经网络，用3 * 3的卷积核处理，每个位置都是用同样的卷积核上的参数进行处理。
		- #等变表示： 
			- **如果一个函数满足输入改变，输出也以同样的方式改变这一性质，就说是等变的。**
			- **函数f(x)与g(x)满足f(g(x))=g(f(x))，那么就是f(x)对于变换g具有等变性。**
		- 卷积提供一种可以处理大小可变的输入方法。
		- **卷积神经网络**部分可以**解决那些不被传统（固定大小）矩阵乘法定义的神经网络处理的特殊数据**。
	- #池化：
		- 用于**减少特征图的空间尺寸**，并**提取特征**的一种技术。通常应用在**卷积层之后**，通过在特征图的**局部区域上执行池化操作来减少特征图的尺寸**。常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。
			- 最大池化：每个池化窗口（通常是2x2的窗口）**在特征图上滑动**，对每个窗口中的**元素取最大值作为池化后的值**。这样可以**保留图像中最显著的特征**，**同时减少特征图的维度**。最大池化常用于提取图像的纹理特征。
			- 平均池化：每个池化窗口**对应的区域内的元素取平均值作为池化后的值**。平均池化通常用于降低特征图的维度，同时保留图像中的整体信息。
	- #卷积网络典型有三级：
		- 卷积级：并行计算多个卷积产生一组线性激活响应。
		- 探测级：每一个线性激活响应通过一个非线性的激活函数，eg：Relu。
		- 池化级：调整输出。
		- ![[Pasted image 20240415182839.png]]
			- 有两组常用术语，左边卷积网络视为**少量相对复杂的层，每层有很多级**，在这组中，**核张量与网络层之间存在一一对应的关系**。右边卷积网络视为更多数量的简单层，这意味着不是每一层都有参数。
	- #基本卷积函数的变体： 在神经网络中，通常不是使用标准的离散卷积运算，实际应用函数存在一定区别。
		- 神经网络中的卷积，**通常说的是由多个并行卷积组成的运算**，因为**具有单个核的卷积只能提取一种类型的特征**。我们希望网络的每一层都可以在多个位置提取多个类型的特征。
		- 核翻转不一定保证网络的线性运算可以交换。**只有当其中每个运算的输出和输入具有相同的通道数时，多通道的运算才是可交换的。**
		- 例子：假设我们有4维的核张量k，每个元素为$k_{i,j,k,l}$ ，**表示输出中处于通道i的一个单元和输入中处于通道j的一个单元连接强度，并且输入和输出单元之间有k行l列的偏置。**
			- 假定输入由观测数据**V**组成，每个元素是$V_{i,j,k}$，表示处在第i通道中第j行第k列的值$$Z_{i,j,k}=\sum_{l,m,n}V_{l,j+m-1,k+n-1}K_{i,l,m,n}$$这里求和是指**所有有效的张量索引的值进行求和**。
			- 但是可以跳过核中一些位置减少计算开销（**会导致提取特征没有之前好**）。这一过程即**全卷积函数输出的下采样**。如果在输出每个方向间隔s像素进行采样那么定义一个下采样卷积函数C$$Z_{i,j,k}=c(K,V,s)_{i,j,k}=\sum_{l,m,n}[V_{l,(j-1*s+m),(k-1)*s+n,}K_{i,l,m,n}]$$s是下采样卷积的移动的步幅。
	- 卷积网络指**至少在网络的一层**中使用**卷积运算**来替换**一般的矩阵乘法**运算的神经网络。