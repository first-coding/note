- 模型压缩目的：**减低模型计算复杂度、存储开销、能耗、推理速度**
- 实现途径：减少参数量、减少计算量、减少存储空间、提升推理速度
- 技术分类：
	- **知识蒸馏**：通过大模型（教师模型）指导小模型（学生模型）训练。
	- **模型量化**：旨在减少模型参数的表示精度来降低模型的复杂度和计算存储空间
		- 量化过程将**高精度的浮点数映射转换为低精度的整数**
		- 公式：$scale=\frac{max-min}{q_{max}-q_{min}}$  $zero-point=round(q_{min}-\frac{min}{scale})$ $q=round(x/scale+zero-point)$
	- **模型剪枝**：网络模型存在大量冗余的参数，大量神经元激活值趋近于0，去除这些神经元并不会影响模型的表达能力，这种情况是**过参数化**，解决方法就是**模型剪枝**
		- **什么是模型剪枝**：是一种减少深度学习模型冗余参数的技术，通过移除那些对模型输出影响不大的参数、神经元、通道或层，来减小模型的大小，降低计算量和存储需求。
		- 剪枝分为两类：
			- 结构化剪枝：基于通道、卷积核或层的剪枝，生成一个仍然是稠密矩阵的模型，便于硬件加速。
				- **通道剪枝**：在卷积神经网络（CNN）中，通过剪掉不重要的卷积通道来减少模型计算量。
					- **基于L1/L2范数的通道剪枝**：计算卷积核权重的L1或L2范数，剪掉范数较小的通道。
					- **基于BN层γ参数的剪枝**：利用Batch Normalization中的缩放系数γ，剪掉γ值较小的通道。
				- **卷积核剪枝（Filter Pruning）**：在卷积层中，通过移除不重要的卷积核（Filter），减少模型复杂度。
					-  **基于卷积核L1范数的剪枝**：计算每个卷积核的L1范数，剪掉那些范数较小的卷积核。
					- **基于特征图响应的剪枝**：根据特征图的激活值或响应强度来决定哪些卷积核对最终输出的贡献较小，从而进行剪枝。
				- **网络剪枝（Layer Pruning）**：直接剪掉整个层（如卷积层或全连接层），减少模型的深度和计算开销。
					- 通过重要性分析或网络敏感性评估，剪掉对性能贡献较小的层。
				- **基于知识蒸馏的剪枝（Pruning with Knowledge Distillation）**：在模型蒸馏过程中结合结构化剪枝，将教师模型的知识传递给经过剪枝的学生模型，确保性能不下降。
					- 先训练一个轻量化学生模型，然后对该模型进行通道、卷积核或层的剪枝。
				- **动态剪枝（Dynamic Pruning）**：根据输入数据动态决定剪枝策略，在推理过程中根据输入选择激活哪些通道或层。
					- 使用门控机制或注意力机制，动态决定每次推理时的剪枝策略。
			- 非结构化剪枝：基于权重的剪枝，只剪掉个别权重参数，生成稀疏的模型。
				- **权重剪枝（Weight Pruning）**：对每个单独的权重进行评估，剪掉那些较小或对模型贡献较小的权重。
					- **基于绝对值的剪枝**：剪掉权重值接近零的部分，通常设定一个阈值，移除权重小于该阈值的参数。
					- **基于梯度重要性的剪枝**：通过反向传播中的梯度信息来评估权重的重要性，剪掉对损失函数影响较小的权重。
				- **稀疏正则化剪枝（Sparse Regularization Pruning）**：在训练过程中加入正则化项（如L1正则化），使模型权重自然变得稀疏，然后进行剪枝。
					- **L1/L2正则化**：通过在训练过程中对模型加入L1或L2正则化，驱动部分权重趋近于零，剪掉这些接近零的权重。
					- **Group Lasso**：对权重分组进行稀疏化处理，确保剪枝过程更具系统性。、
				- **基于重要性的剪枝（Importance-based Pruning）**：根据每个权重的重要性进行剪枝，通过敏感性分析、梯度或其他指标评估权重的重要性，移除不重要的权重。
					- **敏感性分析**：评估删除某些权重后对模型精度的影响，剪掉对精度影响较小的权重。
		- 模型剪枝缺点：剪枝后模型精度可能会下降、需要重新训练、硬件加速支持有限
		- **模型剪枝步骤**：
			- **训练原始模型**：首先训练一个性能良好的原始模型。
			- **剪枝策略的设计**：确定剪枝的策略，是基于权重、通道、卷积核还是整个层，选择合适的剪枝算法。
			- **剪枝操作**：按照设计的策略移除模型中的冗余部分，生成剪枝后的模型。
			- **重新训练（微调）**：剪枝后模型通常需要进行重新训练或微调，以恢复或接近原始模型的精度。
			- **部署与评估**：将剪枝后的模型部署到目标平台，并进行实际应用场景下的性能和推理速度评估。
	- **低秩分解**：将模型中的权重矩阵分解为多个低秩矩阵的乘积，用较小的矩阵近似原始矩阵，从而减少模型的参数量和计算量。
		- **SVD（Singular Value Decomposition，奇异值分解）**：将全连接层或卷积层的权重矩阵进行奇异值分解，保留重要的奇异值并忽略小奇异值，降低参数量。
		- **Tensor Decomposition（张量分解）**：用于分解高维卷积核或多维权重张量，减少计算复杂度。
	- **架构搜索**：通过自动化搜索找到更高效的神经网络架构，优化模型的结构，使得模型在性能和效率之间取得平衡。
		- **进化算法**、**强化学习** 等自动化搜索方法，来找到在计算量与精度之间权衡较好的模型架构。
	- **权重共享**：在模型的不同层或神经元之间共享权重参数，从而减少参数量。常用于嵌入层、卷积核等部件中。
		- **Group Convolution（分组卷积）**：通过让卷积核在不同的组之间共享权重，减少参数量和计算量。
		- **HashNet**：对权重进行哈希编码，相同哈希值的权重共享参数。
	- **裁剪输入（Input Feature Reduction）**：减少输入数据的维度或压缩输入特征，通过减少输入数据的大小来减少计算量。
		- **PCA（Principal Component Analysis, 主成分分析）**：将输入数据进行主成分分析，减少数据的维度。
		- **AutoEncoder（自编码器）**：用自编码器对输入数据进行降维，减少不必要的信息。
	- **迁移学习（Transfer Learning）**：使用预训练模型的参数，并在小数据集上进行微调。通过预训练模型来减少训练和参数量。
		- 使用在大规模数据集（如ImageNet）上预训练好的模型，并微调其中的少量层或参数，避免从零开始训练整个模型。
	- **网络蒸馏（Network Slimming）**：通过对网络结构的精简，去掉不必要的层或节点，使网络结构更加紧凑。
		- **Sparse Regularization（稀疏正则化）**：通过训练过程中对某些层的权重施加稀疏正则化，使得部分通道趋向于零，随后移除这些通道。
	- **移动设备优化架构（Mobile-Optimized Architectures）**：设计适合移动设备和嵌入式设备的高效网络结构，使得模型在资源受限的设备上能够快速运行。
		- **MobileNet**、**EfficientNet**：通过引入深度可分离卷积、层次缩放等技术，在减少参数量的同时保持较高的模型性能。