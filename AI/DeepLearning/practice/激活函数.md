- $sigmoid=\frac{1}{1+e^{-x}}$
	- 输出范围：$y ∈ [0,1]$
	- 应用：用于二分类/表示置信度
	- 梯度平滑，防止训练过程出现突变的梯度
	- 容易造成梯度消失，执行指数运算，比较消耗计算机资源。
- $Tanh=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
	- 输出范围：$y∈[-1,1]$
	- 应用：全连接层中，Tanh替代Sigmoid，收敛速度更快/RNN网络，在隐藏层中广泛用于处理序列数据（文本，时间序列），**更好的捕捉序列中的非线性关系**
	- 整个函数以0为中心，比sigmoid函数更好
	- 存在梯度饱和(梯度平缓，会造成梯度消失)的问题，指数运算，消耗计算机资源。
- $ReLu=max(0,x)$
	- 输出范围：$y∈[0,x]$
	- 应用：CNN、全连接层、GAN
	- 避免了sigmoid/Tanh的梯度饱和问题，缓解深层网络的梯度消失，计算高效
	- **当输入为负的时候，梯度为0，之后这个神经元和之后的神经元都为0，含有ReLu的就会死亡，再也不会被激活**
- $Leaky Relu=max(αx,x)$
	- 应用：深层网络训练、GAN、输入数据可能包含显著负值（如归一化后的图像像素值、金融时序数据），Leaky ReLU比ReLU更适用
	- 缓解ReLu在负输入的时候，梯度为0，导致权重无法更新。梯度稳定性。计算高效
	- 性能提升有限、参数敏感α设置不当会影响收敛、导致在复杂分类中效果不好。
- $PRelu=$![[Pasted image 20250221152706.png]]
	- 应用：图像处理、NLP、GAN
	- 解决ReLu神经元死亡的问题，与Leaky Relu区别在于，α是通过学习获得的。
	- 增加α学习参数，**增加训练的计算量，影响实时推理性能**。小数据集上α容易过拟合，需要配合正则化或数据增强。
- ELU=![[Pasted image 20250221153118.png]]
	- 零中心化，负区间饱和输出，均值接近0，加速收敛，避免了Relu负数上死亡的问题。
	- 计算复杂
- SELU=![[Pasted image 20250221153328.png]]
	- 自归一化，自动稳定网络输出的均值和方差
	- 必须使用LeCun正态初始化、输入数据标准化为近似正态分布（不然自归一化机制失效）
- Swish=![[Pasted image 20250221153826.png]]
- Mish=![[Pasted image 20250221153947.png]]
- softmax=![[Pasted image 20250221153957.png]]
	- 常在输出层充当激活函数，将输出映射到0-1区间。**将神经元输出构造成概率分布，用于多分类问题中，Softmax激活函数映射值越大，则真实类别可能性越大**

---
隐藏层：ReLu及其变体
二分类：sigmoid
多分类：softmax
复杂任务探索：Swish、Mish