- 是在预训练语言模型基础上，使用指令+响应格式的数据，通过监督学习的方式，对模型进行进一步微调，使其更多地理解和执行自然语言任务
	- 预训练模型可能只生成一段无关文字，而不是一封信
- 解决的问题是:**让模型从“语言生成器”转变为“听得懂指令、完成具体任务的智能体**
- 有以下几种方法：
	- 人工任务数据：人类编写任务数据
		- 参考案例：
			- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
			- [清华 Belle 数据集](https://github.com/LianjiaTech/BELLE)
			- [FLAN 数据集](https://github.com/google-research/FLAN)
		- 质量高、覆盖面广
		- 适合中英文通用任务
	- 自我指导构造：先用已有模型生成一些指令，再人工或模型润色
		- 扩展速度快、成本低
		- 不完全依赖人工
		- eg：`instruction: "写一个函数判断一个数是否是素数。"output: "def is_prime(n): ..."`
	- 进化指令微调（Evol-Instruct）：通过修改和扩展现有的指令或任务，逐步增加任务的复杂性，生成更多具有挑战性的数据。通常基于 Self-Instruct 的扩展。
		- ![[Pasted image 20250630210934.png]]
	- 多任务整合：把多个公开 NLP 任务转换成统一格式的“指令执行任务”（如翻译、情感分析、常识推理、数学题等）
		- ![[Pasted image 20250630210942.png]]
	- 对话式微调：将模型训练为多轮对话助手，帮助它学习与人类自然互动。这种方法特别适合像 ChatGLM、GPT 系列等对话型 AI。
		- ![[Pasted image 20250630211008.png]]
	- 数学/逻辑任务微调：该方法专门用于训练模型处理需要推理的任务，例如数学题、逻辑推理、常识推理等。这类任务通常需要模型具备较强的推理能力
		- ![[Pasted image 20250630211038.png]]
	- 编程任务微调：训练模型生成代码或解决编程问题，模型学习通过自然语言描述生成代码，或解决编码问题。
		- ![[Pasted image 20250630211132.png]]
	- 应用：问答系统、多轮对话、任务型助手、领域专用模型、工具调用、对齐微调的前置