**什么是知识蒸馏？**
	运用大模型作为教师模型，生成训练数据，教会小模型学会相似的行为，但结构更小、速度更快、资源要求更低。
**为什么需要知识蒸馏？**
	本地部署，算力不足，压缩成小模型便于部署
	专注于特定方向

logits是什么？
	在输出层在映射函数前的输出为logits，映射函数后的输出为soft labels
#蒸馏原理：
	response-based knowledge：student直接模仿teacher最后的预测
	feature-based knowledge：中间层的蒸馏
		**根据蒸馏目标的不同分为几种**
			特征回归、注意力迁移、关系图蒸馏、结构蒸馏（层间蒸馏）
	relation-based knowledge：层之间的蒸馏
#蒸馏方式：
	Offiline Distillation：teacher先训练，student在依据teacher进行训练，但存在着model capacity gap，所以student很难超过teacher
	online Distillation：同时训练
	self-Distillation：不同层、相同层之间作为student或者teacher
#蒸馏算法：
	常用的是Single-teacher distillation，还有很多其他Multi-teacher / Ensemble Distillation、Task-specific distillation等
#loss：
	KL散度（output layer）、MSE(特征回归)、Consine Similarity（注意力蒸馏）..........
#Architecture：
	存在同构、异构情况
#蒸馏应用场景：
	目标检测蒸馏、语义分割蒸馏、NLP蒸馏、强化学习蒸馏、生成模型蒸馏......
#在公式中存在中一个温度T：
	T的大小代表student训练过程对负标签关注程度