-  首先了解，LLM的**生成控制参数**，这些参数**控制模型在生成文本的行为和输出质量**，属于**模型推理阶段的超参数**。常见的有以下这些
	- **Temperature**：控制生成文本的随机性，即**增加其他可能的 token 的权重**
		- 应用：
			- **0.1~0.5**：QA、摘要、事实性回答
			- **0.7~1.0**：创意写作、诗歌生成
	- **Top-P**：和Temperature是同一类型的参数，控制返回结果的确定性。**意味着较高的top-p会使模型考虑更多可能的词语。较低则相反**
		- 应用：
			- **0.1~0.5**：代码生成、技术文档
			- **0.8~1.0**：对话生成、故事创作
	- **一般top-p和Temperature两个调整其中一个即可**
	- **Max-Length**:限制生成文本的最大token数
	- **Stop Sequence**：是一个字符串，指定生成终止的字符串，可以阻止模型生成token
	- **Top-k**：仅从概率最高的前k个词中采样
	- **Repetition Penalty（重复惩罚）**：抑制重复词或短语出现
	- **Presence Penalty & Frequency Penalty**：
		- Presence Penalty：惩罚已出现的词
		- Frequency Penalty：惩罚高频出现的词
		- 对下一个生成的token进行惩罚
	- **Beam width**：束搜索中保留的候选序列数量
	- **Length Penalty（长度惩罚）**：调整束搜索中对生成长度的偏好
		- **>1.0**：鼓励生成长文本
		- **<1.0**：鼓励生成短文本
	- **大部分常用的是Temperature，top-p/k，Max Length**