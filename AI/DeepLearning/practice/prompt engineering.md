-  首先了解，LLM的**生成控制参数**，这些参数**控制模型在生成文本的行为和输出质量**，属于**模型推理阶段的超参数**。常见的有以下这些
	- **Temperature**：控制生成文本的随机性，即**增加其他可能的 token 的权重**
		- 应用：
			- **0.1~0.5**：QA、摘要、事实性回答
			- **0.7~1.0**：创意写作、诗歌生成
	- **Top-P**：和Temperature是同一类型的参数，控制返回结果的确定性。**意味着较高的top-p会使模型考虑更多可能的词语。较低则相反**
		- 应用：
			- **0.1~0.5**：代码生成、技术文档
			- **0.8~1.0**：对话生成、故事创作
	- **一般top-p和Temperature两个调整其中一个即可**
	- **Max-Length**:限制生成文本的最大token数
	- **Stop Sequence**：是一个字符串，指定生成终止的字符串，可以阻止模型生成token
	- **Top-k**：仅从概率最高的前k个词中采样
	- **Repetition Penalty（重复惩罚）**：抑制重复词或短语出现
	- **Presence Penalty & Frequency Penalty**：
		- Presence Penalty：惩罚已出现的词
		- Frequency Penalty：惩罚高频出现的词
		- 对下一个生成的token进行惩罚
	- **Beam width**：束搜索中保留的候选序列数量
	- **Length Penalty（长度惩罚）**：调整束搜索中对生成长度的偏好
		- **>1.0**：鼓励生成长文本
		- **<1.0**：鼓励生成短文本
	- **大部分常用的是Temperature，top-p/k，Max Length**
---
- prompt engineering技术：
	- prompt由**指令、上下文、例子、输入和输出**组成。
	- Zero sample prompt：没有例子，即**直接给指令**
		- `请根据下面的文本生成摘要`
	- Low sample prompt：部分例子，即**给指令和小部分的例子**
		- `分类语句的情感，中性、开心或者悲伤`
		- `例子：我很难过   情感：悲伤   语句：我认为可以   情感：`
	- chain-of-Thought Prompting：链式思考提示，即**将思考的流程显式的列出来**
		- `如果盒子里有五个苹果，给了你三个，盒子还剩多少个`
		- `普通方式：直接说出几个`   `CoT方式：现在盒子里还剩5-3=2` 
		- **CoT可以和zero sample prompt、Low sample prompt这些合在一起使用**
	- self-consistency：发送相同的coT提示，即**发送多次相同的coT提示**
		- ![[Pasted image 20250213181520.png]]
		- ![[Pasted image 20250213181531.png]]
	- 