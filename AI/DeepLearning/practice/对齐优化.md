- 什么是对齐优化：**通过一系列技术手段、确保语言模型在执行任务的时候能够遵循人类的意图**，并与人类的价值观、伦理规范保持一致。对齐优化的目标是让模型的行为更符合人类的期望，确保模型的输出不会产生有害或不符合道德标准的内容

- 目的：
	- 确保安全性：防止生成有害、偏见或歧视性内容
	- 提高任务执行的准确性：使得模型在执行指令时能够遵循特定的任务要求和目标
	- 符合伦理和法律：确保模型的行为符合社会、法律以及道德规范
	- 减少错误输出：避免模型生成的内容因歧义或错误理解而偏离人类预期

- 方法：
	- **监督微调（SFT）**：使用标注数据调整预训练模型行为，“指令-响应”对，学习 **如何在不同的任务上**生成更加符合人类意图的答案
		- 相对简单，需要人工大量标注
	- **强化学习与人类反馈（RLHF）**：模型通过与环境交互来学习最优策略，利用人类的反馈对其生成的结果进行 **奖励和惩罚**。模型生成的输出会被人类标注者评分，模型在接收到反馈后，通过**强化学习算法**来调整其策略。
		- 流程：预训练-人类反馈收集-强化学习优化
		- 通过不断与环境交互学习，逐步改进，但是需要大量的标注和反馈，训练成本高
	- **直接偏好优化（DPO）**：强化学习的变体，该方法模型对比两种候选输出。学习如何 **优先选择人类偏好**的输出。目标是让模型通过直接对比多个候选输出的质量来优化其生成策略
		- 相对于RLHF，要求更加简单高效，但是需要大量的对比反馈样本，容易受训练数据偏差影响
	- **逆向强化学习（IRL）**：是一种强化学习方法，通过观察**人类行为**来推测人类的目标函数或奖励函数。换句话说，模型通过 **模仿人类行为**来学习如何做出符合人类价值观的决策。
		- 通过模仿人类的行为来构建目标函数，能够很好地对齐人类的价值观，但是需要大量的专家行为数据，而且推测出的奖励函数可能不完全精确。
	- **对抗性训练**：对抗性训练通过 **生成对抗样本**来训练模型，使其能够应对可能的攻击或不可靠的输入。这些对抗样本是通过对模型的当前输出进行微小扰动，迫使模型产生错误结果，从而提高模型的鲁棒性。
		- 能够提高模型的鲁棒性，使模型在面对不确定输入时表现更加稳定，但是对抗训练过程复杂且训练时间较长，可能需要额外的资源
	- **去偏见优化**：去偏见优化的目标是消除模型在生成过程中可能存在的**偏见**，比如性别、种族或社会地位等方面的偏见。它通过分析模型输出中的潜在偏见，并调整模型使其在生成内容时更加中立和公正。
		- 能有效提升模型的公平性和透明度，去偏见优化需要精细设计，可能会导致模型性能的下降，特别是在复杂任务中