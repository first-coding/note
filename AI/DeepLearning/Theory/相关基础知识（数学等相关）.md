-  数学基础：线性代数，概率论，高等数学（微积分）
- **推断方法**：是用来求值的，**估计模型的参数值、估计潜变量的值/概率分布的**，常见的有最大似然估计、期望最大化算法、贝叶斯推断、变分推断和马尔可夫链蒙特卡罗采样等。 ^76e196
	- 目的：**通过已有的数据来估计或者推测数据背后的真实模型，推断出未知的参数值或潜在变量。**
	- 核心思想：基于概率推测或估计参数
- **优化算法**：寻找最优解，即找到最好的那个模型(这个模型**能更好的拟合数据和预测数据**)。
	- 核心思想：通过最小化或最大化目标函数，调整模型参数以优化性能
	- 常见优化算法：牛顿法、Adaw、梯度下降等
	- 分两类：**确定性优化算法和随机优化算法**：
		- **确定性优化算法**：通过解析或迭代的方式逐步接近最优解
			- **梯度下降**：
				- **批量梯度下降**
				- **随机梯度下降（SGD）**
				- **动量梯度下降**
				- **Adam优化器**
			- **牛顿法**：利用目标函数的二阶导数（Hessian矩阵）来加速优化过程，但计算复杂度较高，尤其在高维问题中不常用。
		- **随机优化算法**：当目标函数具有多个局部最优值或较为复杂时，确定性算法可能难以找到全局最优解。**这时候随机优化算法提供另一种解决途径**
			- 模拟退火算法
			- 遗传算法
			- 粒子群优化（PSO）
- **推断**：侧重于**估计**模型的参数、潜在变量或概率分布，通常与数据中的不确定性相关。推断不仅关注模型如何拟合数据，还考虑数据背后潜在的生成过程或不确定性。
- **优化**：侧重于**寻找最佳模型参数**，通过优化目标函数来改进模型的预测性能。优化注重的是通过算法找到能够最好地拟合数据的模型。
- **概率模型**：通过概率论来描述数据或现象中的不确定性和随机性的数学工具 ^6e6447
- **大数定理**：如果样本$x^{(i)}$是独立同分布的，那么平均值几乎必然收敛到期望值。 ^8e72e3
- **中心极限定理**：$\hat s_n$的分布收敛到s为均值以$\frac{Var[f(x)]}{n}$为方差的正态分布，使得我们可以利用正态分布的累计来估计$\hat s_n$的置信区间 
	- ![[Pasted image 20240703102730.png]]Var（x）表示方差。^a69853
- **模式崩溃现象**：生成器在训练过程中，生成的样本缺乏多样性，趋向于生成非常相似或几乎相同的输出，导致生成图像不能很好的覆盖真实数据分布，使得生成模型的性能变差。 ^a7f3cf
	- 原因：**生成器和判别器的训练不平衡**、**缺乏适当的正则化**、**优化问题（损失函数设置不当，超参数选择不当）**、**训练不充分或过拟合**
	- 解决方法：修改损失函数、优化器调整、训练策略调整（逐步训练、标签平滑）、多样性奖励、增大生成器的容量、小批量训练。
-  **函数空间的内积**:![[Pasted image 20241031191024.png]]
- **对称函数**：一类在多变量排列顺序下保持不变的函数
	![[Pasted image 20241031191046.png]] ^bfc4dc
- **正定矩阵**：![[Pasted image 20241031191144.png]] ^219e87
	- ![[Pasted image 20241031191156.png]]
- **行列式**：![[Pasted image 20241211171837.png]]![[Pasted image 20241211171844.png]]
- **边缘概率分布可以通过对所有可能的潜变量的联合分布求和得到**
- 对于函数f(x)具有左导数和右导数：
	- 左导数：紧邻在x左边的函数的斜率
	- 右导数：紧邻在x右边的函数的斜率
	- 可微：当函数在x处左导数和右导数都有定义并且相等，函数在x点处可微。
- **高斯混合**：多个高斯分布的线性组合。
	- ps：高斯混合输出在语音生成模型和物理运动中特别有效。
- **凸和非凸**：
	- 凸函数：在该区间函数图象上的任意两点所连成的线段上的每一个点都位于函数图象的下方(或上方)。就存在一个极值点，这个极值点就是最值点![[Pasted image 20240307183439.png]]
	- 非凸函数：函数在该区间上有多个极值,即系统有多个稳定的平衡态。即存在多个极值点。![[Pasted image 20240307183450.png]]
	- **凸优化问题**：目标函数和约束条件满足凸性质情况下的优化，凸函数的优化问题通常相对容易解决。
		- 目标函数是凸函数，即函数二阶导数为正
		- 约束条件是凸集：凸集（集合中任意两点之间的连线上的所有点也属于该集合）
		- 优化变量定义域是凸集：优化问题中涉及的变量所在的定义域也是凸集。
- **泛函**：一个/多个函数到一个实数的映射，就是相当于说函数是自变量x，映射出来的一个实数就是因变量y。
- **仿射变换**：几何中，对一个向量空间进行一次线性变化，并加上平移变换另一个向量空间
- **线性变化**：指在两个向量空间之间保持向量加法和标量乘法的函数。
- **约束优化**：在x的所有可能值中最大化或最小化一个f(x)不是我们所希望的，**x在某些集合S中找f(x)的最大值或最小值**。集合S内的点x被称为**可行点**。
- **算术平均**：是一组数字的总和除以数字的个数。它是最常见的平均数计算方法。算术平均用于描述数据集的集中趋势，它可以很好地反映出数据的总体水平。算术平均计算公式为：$$算术平均=\frac{所有数据总和}{数据的个数}$$
- **几何平均**：是一组数字的乘积的N次根，其中N是数字的个数。几何平均常用于描述一组数据中的相对变化率或增长率。它通常用于处理一些比率、指数或百分比数据，比如利率、收益率等。几何平均计算公式为：![[Pasted image 20240330150726.png]]
- **几何平均和算术平均都是描述一组数据中心趋势的统计量**。
- **范数**：度量向量空间中向量长度。eg：
	- ![[Pasted image 20240411141305.png]]
- **鞍点**：梯度为零的点。
	- 低位空间中，局部极小值很普遍，但是在高位空间中，极小值很罕见，而鞍点很常见。**因为高维空间中鞍点的激增解释了为什么在神经网络训练过程中二阶方法无法成功取代梯度下降**。因为牛顿法目标是寻求梯度为零的点，所以在高维空间会陷入一个鞍点，所以需要**二阶无鞍牛顿法**。
- **正定矩阵**：M为n阶方阵，对任何非零向量Z，存在$z^TMz>0$,M就为正定矩阵。
- **先验概率分布**：在进行观测/实验之前，基于以往知识/经验得到的概率分布。
- **Gabor函数（科普知识）**：![[Pasted image 20240416200000.png]]Gabor函数的特点是它可以在空间域和频率域同时具有良好的局部化特性，这使得它在分析局部特征时非常有用。
- 非线性变换：即变换T不同时满足两个条件 ^1f4000
	- $T(x+y)=T(x)+T(y)$
	- $T(αx)=αT(x)$
	- 更简单的说：不能用一条直线（或平面）完全描述变换