
## 1.模型架构：

模型由多个**编码器和解码器**组成，每个部分由多个层堆叠而成。**编码器负责将输入序列编码成一系列连续的表示**。**解码器根据这些表示生成目标序列**
![[Pasted image 20240525151753.png]]
___
## 2.自注意力机制

![[Pasted image 20240525153412.png]]
self-Attention:
	eg:
	1. **先定义一个输入**
	![[Pasted image 20240525164358.png]]
	2. **初始化权重**
	![[Pasted image 20240525164449.png]]
	3.**计算key**,计算key,query和value矩阵的值，计算的过程也很简单,运用**矩阵乘法**即可： **key = input * w_key; query = input * w_query; value = input * w_value;**
	![[Pasted image 20240525164628.png]]
	4.**计算attention scores**，分别将input1的query[1,0,2]与input1、input2、input3的key的转置[0,1,1]、[4,4,0]、[2,3,1]分别做**点积**。input2、input3的query同样这样做。得到![[Pasted image 20240525171455.png]]
	5.通过对attention scores做**softmax计算**，**这样做的好处是凸显矩阵中最大的值并抑制远低于最大值的其他分量。**![[Pasted image 20240525172610.png]]
	6.通过对**上一步得到的矩阵乘以对应的value**得到alignment vectors![[Pasted image 20240525173603.png]]
	7.**对alignment vectors求和得到output**![[Pasted image 20240525173909.png]]
	self-attention公式：![[Pasted image 20240525174444.png]]
	除以$\sqrt{d_k}$ ，（$\sqrt{d_k}$表示词向量的维度）
	1.是为了防止$QK^T$值过大，导致计算softmax计算时overflow
	2.使用$d_k$ 可以让$QK^T$ 的结果满足期望为0，方差为1的分布
	使用公式计算，$QK^T$看来，**几何角度来说，点积是两个向量的长度与它们夹角余弦的积**，夹角为90，结果为0，**表示两个向量线性无关**。夹角越小，结果越大，**两个向量在方向上相关性越强**。

Transformer的MultiHead通过多个Self-Attention进行组合而成的。即计算每个Self-Attention然后将计算出来的Self-Attention进行组合在一起就成了MultiHead。

Self-Attention作用：**在序列中建立每个位置与其他位置的关联，根据这些关联动态地调整每个位置的表示**。
1. **建立全局依赖关系**：通过计算每个位置与其他位置的关联程度。
2. **捕捉长距离**：**每个位置可以与序列中的所有其他位置进行交互**，self-attention 机制使得模型**能够更好地捕捉序列中的长距离依赖关系**，而无需依赖于固定大小的局部窗口。
3. **动态调整关注**：允许模型**根据输入序列中的不同内容和上下文动态地调整每个位置的关注程度**，从而更好地**适应不同任务和输入序列**的特点。
4. **并行计算**：**每个位置的表示都是根据所有其他位置的信息加权聚合而来的**，高效地进行并行计算，从而提高模型的计算效率和训练速度。
___
one-hot编码：**将字符数据转换成数值编码（没有大小之分）**，eg：我从哪里来--------->one-hot编码：
$$\begin{bmatrix}1&0&0&0&0\\0&1&0&0&0\\0&0&1&0&0\\0&0&0&1&0\\0&0&0&0&1\end{bmatrix} $$
但是one-hot编码会存在几个问题：
1. 大量为0的稀疏矩阵，过度占用资源
2. 特征维度较高，每个特征都彼此独立，训练的数据量高，会导致维度灾难
3. 硬编码，会导致没有相应语义信息
因为这几个问题，引入了Embedding
---
## 3.Embedding

通过将**高维数据映射到低维空间，从而提取数据的有用特征**，将数据转换成计算机能识别的信息，在NLP、CV、推荐系统等都有应用。

Embedding类型：
	Word Embedding：将单词映射到低维空间，**提取单词之间的语义关系**。
	Image Embedding：图像映射到低维空间，**提取图像的特征**。
	User Embedding：将用户行为数据映射到低维向量空间，**表示用户的兴趣和偏好**。

Embedding学习方法：
	Word2vec，GloVe，BERT等。

Transformer的positonal encoding通过公式进行计算,这个是三角位置编码![[Pasted image 20240526155927.png]]
pos 是位置，i 是嵌入维度的索引，dmodel​ 是嵌入的维度。

---
## 4.应用
Transformer首先用在NLP中，在NLP中具有较好的效果，然后在计算机视觉（CV）、推荐系统等地方开始应用。

---
## 5.Vision Transformer
- 是transformer用于图像领域的模型。
- ![[Pasted image 20240630235414.png]]
- 整个流程如下：
	1. vit输入的是图像，分割为若干块(patch_size)。将每个块**通过卷积投影、展平，转置**操作转换为嵌入向量（即超参数中的embed_dim） 
	2. 将多个嵌入向量得到一个(批量大小，分块数量，每个块嵌入维度)
	3. 通过**位置编码**：将输入序列中每个位置的信息编码为向量（简单来说就是将每个块的位置转换为向量）**常用有两种方式，固定的位置编码和可学习得位置编码**。
	4. 将向量和位置编码输入(这个输入才是真正的vit输入)到**多头注意力机制（可以在处理一个位置信息的内容的时候，同时关注其他位置的信息）**
		1. 将嵌入向量投影到Q,K,V空间中，通过W_Q,W_K,W_V进行参数化。
		2. 计算Q与K之间相似度（**点积或者加性注意力机制**）
		3. softmax相似度矩阵，得到**注意力权重矩阵**
		4. 将注意力权重矩阵，进行加权值向量，得到**每个位置注意力加权值**
		5. 将注意力加权值进行池化，将每个注意力头输出通过**拼接或者其他方法**进行汇总，形成完整注意力。
	5. 通过MLP，对注意力进行非线性变换和特征提取。
	6. 归一化和残差连接：每个子层（注意力层和MLP层）输入与输出之间应用归一化与残差，**加快训练和提高性能**。