- Wav2vec2.0是自监督语音预训练模型，**训练过程分为预训练和微调两部分**。
	- 优点：
		- 不依赖于标注数据进行预训练
		- 微调只需要少量数据
		- 表征迁移性强，适用于下游语音任务
	- 模型架构分为三部分：
		- CNN encoder：音频转成潜在特征
		- Transformer：建模上下文关系
		- Quantizer：将encoder输出离散化，用于自监督对比学习
	- 过程：
		1. 输入原始音频，秒数$*$采样率，得到多少个数
		2. 通过feature extractor以及下采样后的latent speech representation，得到n维latent vector(每个是一个特征向量z_t)
		3. 通过12层Transformer，建模时间上下文依赖，输出上下文感知向量c_t
		4. Quantization（用于训练，推理阶段不使用），每个z_t会被Quantization映射为一个离散向量q_t。**模型训练目标是让c_t能预测对q_t(对比学习)**
		5. 微调阶段，c_t送入一个线性分类器，输出 vocab_size 个类（字母/音素/单词） 的概率分布，并使用CTC将概率分布转换为最终文字序列
	- 两个训练过程中：
		- 自监督训练：**让模型学会从上下文中推测被遮盖的未来音频内容**
			- 使用InfoNCE损失进行训练，**就是让masked的预测尽可能接近正确的的quantized表征，同时远离其他时间步的负样本**
			- 随机mask一部分latent表征，用 Transformer 预测它们，预测目标是 quantizer 提供的离散向量
			- 预测未来某一位置的 q 向量
				- 对于某个时间点 t，使用上下文向量 **c_t** 去预测 **q_t+k**（比如未来第3帧）对应的 **离散化编码**
		- 有监督训练：预测出正确的文本序列，不依赖时间对齐
			- 预训练的模型，加入CTC分类头，输入原始音频 → 输出特征 → 再输出每一帧上各个字符（或词素）的概率分布，用**CTC损失**进行训练
		- 对比损失计算，
			  ![[Pasted image 20250615165200.png]]
			  ![[Pasted image 20250615165229.png]]
			  ![[Pasted image 20250615165239.png]]
			  ![[Pasted image 20250615165247.png]]
			  ![[Pasted image 20250615165254.png]]
	