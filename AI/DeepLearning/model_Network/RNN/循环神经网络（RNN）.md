- **展开计算图**：
	- $s^{(t)}=f(s^{(t-1)};θ)$ 
		- $s^{(t)}$ 称为系统的状态。
		- $s^{(3)}=f(s^{(2)};θ)=f(f(s^{(1)};θ);θ)$ 
		- ![[Pasted image 20240420145334.png]]
		- ![[Pasted image 20240420145835.png]]
	- $s^{(t)}=f(s^{(t-1)},x^{(t)};θ)$ 
		- 循环神经网络可以通过许多不同方式建立，**所有涉及循环的函数都可以视为一个循环神经网络**。
		- $h^{(t)}=f(h^{(t-1)},x^{(t)};θ)$ 
			- 通过上面这个式子或类似公式定义隐藏单元的值。
			- 存在两个方式可以进行绘制
				- **在模型的物理实现中存在的部分赋予一个节点，生物神经网络**。
				- **绘制展开的计算图**，存在两个主要优点
					- 无论序列的长度，学成的模型始终具有相同的输入大小（**指定的是一种状态到另一种状态的转移，而不是在可变长度的历史状态操作**）。
					- 每个时间步使用相同参数的相同转移函数f。
-  **循环神经网络：一类用于处理序列数据的神经网络。**
	- 专门用于处理序列$x^{(1)},...,x^{(T)}$ 的神经网络
		- 循环网络可以拓展到更长的序列。
		- 大多数循环网络可以处理可变长度的序列。
	- **循环神经网络重要的设计存在以下几种**：
		- **每个时间步都有输出**，并且**隐藏单元之间有循环连接**的循环网络。
			- ![[Pasted image 20240420150730.png]]
				- L：是损失衡量每个o与相应得训练目标y的距离。
				- x：输入序列。
				- U，W，V是权重矩阵。
				- o：是通过x值输入序列映射的输出值o
					- 使用softmax输出，$\hat{y}=softmax(o)$ 
				- **$a^{(t)}=b+Wh^{(t-1)}+Ux^{(t)}$ ，输入单元的输出，隐藏单元的输入。**
				- **$h^{(t)}=tanh(a^{(t)})$ ，通过双曲正切激活函数（tanh简写(th)），隐藏层输出。**
				- **$o^{(t)}=c+Vh^{(t)}$ ，是输出层的输入。**
				- **$\hat{y}^{(t)}=softmax(o^{(t)})$ 获得标准化后概率的输出向量$\hat{y}$ **
			- 任何图灵可计算的函数都可以通过这样的一个有限维的循环网络计算。
		- **每个时间步都产生一个输出**，只有**当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络**
			- ![[Pasted image 20240420151413.png]]
				- 不能模拟通用图灵机，因**缺少隐藏到隐藏的循环**
				- 可以**并行化训练**，任何基于比较时刻t的预测和时刻t的训练目标的损失函数所有时间步都解耦了。**即在各时刻t分别计算梯度。**
				- 唯一的循环时在输出到隐藏层的反馈机制中。
				- $o^{(t)}$是输出，$y^{(t)}$ 是目标，$L^{t}$ 是损失。
			- **导师驱动过程**：一种训练技术，适用于**输出与下一时间步隐藏状态存在链接的RNN**，即上图这种类型的![[Pasted image 20240423160400.png]]
				- 导师驱动过程不再使用最大似然准则（**基于观察到的数据来估计模型的参数，使得给定观察到的数据样本的情况下，所估计的参数值使得观察到这些数据的概率最大化。**），**而是在t+1时刻接收真实值$y^{(t)}$ 作为输入。** 最大条件似然准则如下：![[Pasted image 20240423160528.png]]
				- **开环**：网络输出（或输出分布的样本）反馈作为输入。
					- 这种情况下，使用导师驱动过程就会导致**训练的输入和测试的输入存在很大的不同**。
		- **隐藏单元之间存在循环连接**，但**读取整个序列后产生单个输出的循环网络**。 ^787f0f
			- ![[Pasted image 20240420151634.png]]
				- 在序列结束时具有**单个输出**。可以**用于概括序列并产生进一步处理的固定大小表示**。
	- 通过[[深度学习（花书）#^7d08ea|反向传播]]并结合通用的**基于梯度**的技术即可训练RNN。
		- **通过时间反向传播（BPTT）**：
			- 是反向传播算法在RNN中的拓展通过时间上展开的网络结构，将损失函数相对于隐藏状态和网络参数的梯度累积起来。
			- 专门用于序列数据的处理。
		- **作为有向图模型的循环网络**：
			- 每个节点代表一个状态或特征，节点之间的边代表它们之间的关系。RNN的循环连接允许信息在节点之间传递，并且通过时间步长往复传递，这使得**模型能够捕捉到时间上的动态特性**。
			- **markov假设**：即图模型只包含从${y^{t-k},.....,y^{(t-1)}}$ 到$y^{(t)}$的边。
			- **将RNN视为定义一个结构为完全图（任意两个顶点之间都有边）的图模型**解释为RNN作为图模型的一种方法。
		- **基于上下文的RNN序列建模**： ^4003ed
			- 只使用单个向量X作为输入，**当x是固定大小的向量的时候**，我们可以简单的将其看成产生y序列RNN的额外输入**将额外输入提供到RNN常见方法**：
				- 在每个时刻作为一个额外输入![[Pasted image 20240425161121.png]]
				- 作为初始状态$h^{(0)}$
				- 结合以上两种
				- ![[Pasted image 20240425161734.png]]
	- **深度循环网络**：
		- 大多数RNN中的计算可以分解为三块参数和变换：
			- 输入到隐藏
			- 前一隐藏状态到下一个隐藏状态
			- 隐藏状态到输出
		- 深度的增加会导致优化困难而损害学习效果。
	- **长期依赖挑战**：指在**时间序列或序列数据**中，某个**时间步的输出与其之前的多个时间步相关联**的现象
		- 由于变深的结构导致模型丧失了学习到先前信息的能力，优化变得困难。
		- 在自然语言处理中，长期依赖问题是指在处理长文本或长句子时，**模型难以捕捉到较远处的依赖关系**，导致**模型性能下降或产生错误的预测**。
		- 根本问题是：经过许多阶段传播后的梯度倾向于消失（**大部分情况**）或爆炸（**很少，但是对优化过程影响很大**）。
		- **处理长期依赖的方法**：
			- 设计工作在多个事件尺度的模型，**让模型在某些部分在细粒度时间尺度上操作并能处理小细节。而其他部分在粗时间尺度上操作并能将遥远过去信息更有效传递过来**，存在多种同时构建**粗细时间粗度的策略**
				- **时间维度的跳跃连接**：增加从**遥远过去的变量**到**目前变量的直接连接**是得到粗时间尺度的一种方法。**但不是所有的长期依赖都可以在这种方式下良好的表示**
				- **渗漏单元和一系列不同时间尺度**：设置**线性自连接单元（神经网络中神经元与自身的连接）**，并且这些连接的权重接近1。
					- **滑动平均值**：从一个有n项的时间序列中来计算多个连续m项序列的平均值。
					- 我们可以用滑动平均值来代替真实参数
					- 通过$μ^{(t)}=αμ^{(t-1)}+(1-α)v^{(t)}$ ，设μ(t)是v(t)累积的一个滑动平均值。即当参数更迭时，我们总是利用μ(t)而不是v(t)。
						- 当α→1时，$μ^{(t)}$几乎**全采用上一个滑动平均值，几乎不发生变化，对历史信息沿用较大**；  
						- 当α→0时，**$μ^{(t)}$几乎全采用算法输出值，历史信息基本被全部丢弃**。
						- **线性自连接**：这些从μ(t−1)连接到μ(t)的方式，称为**线性自连接**
						- **渗漏单元**：α→1的线性自连接隐藏单元。
				- **删除连接**：**多个事件尺度组织RNN状态**，信息在较慢的时间尺度更容易长距离流动。**通过删除长度为一的连接并用更长的连接替换**。
		- **优化长期依赖**：
			- 强非线性函数（许多时间步计算的循环网络），往往倾向于**非常小/非常大幅度的梯度**。导致**网络无法学习有效的参数更新/参数更新过大，训练不稳定**
				- **截断梯度**：
					- 一种通过**参数更新之前，逐元素的截断小批量产生的参数梯度**
					- 一种**在参数更新之前截断梯度g的范数||g||**
						- 通过**限制梯度的大小来防止梯度爆炸**。具体而言，**当梯度的大小超过预先设定的阈值时，就对梯度进行缩放，使其大小不超过阈值，从而保持梯度的稳定性**。
				- **梯度截断有助于处理爆炸的梯度**，但是**无助于**消失的梯度。
			- **引导信息流的正则化**：**是一种特殊形式的正则化**，它**通过设计网络结构或修改损失函数**来引导信息在网络中的流动。例如，可以通**过设计更加平滑的网络结构或添加额外的约束项**来鼓励信息在网络中的良好传播，从而帮助**网络更好地学习和泛化**。
	- **外显记忆**：网络系统可以存储和检索具体事实，也可以利用它们**循序推论**
		- **记忆网络**：这种模型通常由**一个存储器和一组控制器组成**，存储器用于存储信息，而控制器用于读取、写入和操纵存储器中的信息。
		- **神经网络图灵机**：旨在模拟人类的记忆和推理过程。它由一个**可训练的神经网络和一个外部存储器组成**，存储器允许网络读取、写入和擦除信息，并以此来完成各种任务。
		- ![[Pasted image 20240426221329.png]]
- **双向RNN**：即**当前状态**的正确解释与**后面的状态存在关联**，eg：在语音识别中，当前的正确意思和后面几个有关。
	- **结合时间从序列起点开始移动的RNN**和**另一个时间从序列末尾开始移动的RNN**。
	- 双向RNN可以用在图像这种二维数据上，但是计算成本相对于CNN会更高。
	- ![[Pasted image 20240426120825.png]]
- **编码-解码的序列到序列架构**：
	- RNN存在三种：
		- 输入序列映射成固定大小的向量[[#^787f0f]]
		- 固定大小的向量映射成一个序列[[循环神经网络（RNN）#^4003ed]]
		- 输入序列映射到等长的输出序列
	- 编码-解码：将输入序列映射到不一样等长的输出序列。通常可以用在**语音识别、机器翻译或问答**	
		- **编码器/读取器/输入**：RNN处理输入序列，编码器输出上下文C（通常是最终隐藏状态的简单函数），编码器通常以**最后一个状态$h_{n_x}$** 作为输出上下文C的输入和解码器RNN的输入。
		- **解码器/写入器/输出**：RNN以固定长度的向量为条件产生输出序列。
		- ![[Pasted image 20240426120924.png]]
		- 在这种序列架构中，输入x和y的长度可以不同。
		- 存在明显不足：
			- 编码器的输出C维度太小，以至于无法概括一个长序列。
				- 通过将C变成可变长的序列，而不是一个固定大小的向量。
				- **注意力机制**。
- **递归神经网络**：构造为**树状结构**，而不是RNN的链式结构。
	- 用于自然语言处理和计算机视觉
	- **对于长度T的序列，深度可以急剧从T减少为O(logT)**,这有助于解决长期依赖
	- **如何用最佳的方式构造树**：
		- 使用不依赖数据的树结构，比如平衡二叉树
		- 在某些地方，运用外部构建的树结构
			- eg：在自然语言处理中，用句子语法分析树。
	- **回声状态网络**：
		- 递归神经网络的变体，**只训练RNN的输出层**，而将RNN的隐含层视作固定的“回声状态”（echo state），并**随机初始化**。这些回声状态在网络训练前就固定下来，不再更新。这种设计保留了RNN的动态特性，但减少了需要优化的参数数量，从而简化了训练过程。
		- **储层计算**：隐藏单元形成了可能捕获输入历史不同方面的临时特征池。
		- 相对来说，较为冷门
- **门控RNN**：
	- **长短期记忆（LSTM）**：专门设计用于处理和预测序列数据，**相比传统的RNN，LSTM通过引入门控机制**，解决**梯度消失/爆炸**问题，更好的解决**长期依赖关系**。
		- 自循环：**是神经网络一种结构**，用于表示**当前时间步的输入**与**前一个时间步的输出**之间的关系。
			- 通过自循环机制，创建一种能让梯度长时间持续流动的路径，**网络更好的学习长期依赖性**
			- LSTM自循环部分**由“门”的结构来控制**，这些门的开启和关闭都是通过**权重来调节**，这些权重**是根据输入数据的上下文来变化**。![[Pasted image 20240426212631.png]]
		  - 基础结构：**输入门、遗忘门和输出门**，每个门负责控制信息的流动和处理**，通过组合，LSTM能够有效地记忆和使用先前的信息，从而更好地预测未来的序列。**每个门控单元具有自己的权重矩阵和偏置**
		  - 常用于**自然语言处理、时间序列预测、语音识别**。
	  - **门控循环单元（GRU）**：
		  - 基本结构：**更新门和重置门**。
		  - 相比之下，GRU通过两个门的组合来控制信息的流动和更新状态。
		  - **在资源受限的情况下更具优势，参数数量更少**。