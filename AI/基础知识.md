- 机器学习：类似人通过对经验的利用，对新情况做出有效的决策。在机器学习领域，**经验->数据，学习算法基于数据得出模型（从数据中学到的结果）**
	- 每一条数据就是一条记录，记录的集合就是**数据集**。
	- 一条记录是关于一个事件或对象的描述。
		- eg: 西瓜的根蒂、敲声、色泽就是**属性**。属性上的值就是**属性值**，每一条记录就是对一个西瓜的描述。
		- 属性的值形成的空间就是**属性空间，样本空间**
		- 在**属性空间中**的每一条记录都可以找到对应的点。每个点就是**特征向量**。
	- 从数据中学到模型的过程称为**学习/训练**。
		- 用于学习/训练的数据集就是**训练集**
		- 训练集中的每一条数据就是**训练样本**
		- 特征值得到的是**标签**，有标签的记录是一个**样例（xi,yi）** yi表示xi的标记，y是所有标记集合。称为**标记空间**
	- 连续值和离散值：
		- 连续值：eg:0.95，0.37
		- 离散值：eg:西瓜分类是好瓜还是坏瓜，好瓜或者坏瓜就是离散值
	- 是否有标记信息：监督学习和无监督学习。
	- **泛化能力**：模型适用于新样本的能力。
	- 归纳和演绎：
		- 归纳：从具体事实归结出一般性规律，从样例中学习->规律->归纳学习。
			- 归纳学习：
				- 广义：从样例中学习
				- 狭义：从训练数据中学的概念（概念学习（研究少，应用少））
	- **假设空间**：eg:y=ax+b，a和b存在多种可能的集合就是**假设空间**，学习的目的就是找出最佳的线性代数，即最符合训练数据的a和b。
		- **版本空间**：在有限的训练集中，存在着多个最佳的，这个就是**版本空间**。即存在多个函数与训练的数据相对应。
		- **归纳偏好**：在版本空间中，不同的函数对于新预测的数据得出不同的结果，不同的算法有不同的选择，有两种偏好
			- **尽可能特殊**：适用情况尽可能少
			- **尽可能一般**：使用情况尽可能多
			- 如何选择：奥卡姆剃刀：**若有多个假设与观察一致，选择最简单那个**。

- **模型评估**：
	- **训练误差**：学习器在**训练集**上的误差。
	- **泛化误差**：学习器在**新样本**上的误差。
		- 当训练误差过小，也就是说学习器把训练样本自身的特殊的特点当作所有潜在样本具有的一般特点的时候，会导致泛化性能下降。这种现象叫做**过拟合**。过拟合无法避免，只能缓解。
		- 与之相反，当训练误差很大，也就是对训练样本的一般特点尚未学好的，这种现象叫做**欠拟合**。可以解决。
		- eg：对树叶进行训练，**过拟合**就是误以为树叶必须有锯齿（但是有些树叶没有锯齿）。**欠拟合**就是认为绿色都是树叶。
	- **P和NP问题**：
		- P问题（可解问题）：P代表“多项式时间”，**指的是在多项式时间内能解决的问题**。如果一个算法的运行时间可以用问题的规模多项式去界定，就认为这个算法高效。
		- NP问题（非确定性多项式时间问题）：NP代表“非确定性多项式时间“，指的是在多项式时间内验证一个解是否正确的问题。**如果一个解可以在多项式时间内被验证，虽然找到解的过程可能并不高效，那么这个问题就被认为是NP问题**。至今尚未找到能够在多项式时间内解决所有NP问题的算法。
		- 在计算机科学中，P和NP问题是否等价没有一个确定的答案。
	- **评估方法**：通过**测试集**来测试学习器对新样本的判别能力，以测试集上的**测试误差**作为**泛化误差**的近似。但是测试样本尽量不在训练集中出现，未在训练过程中使用过。
		- 一般的**训练集**和**测试集**都是在数据集中进行划分。有三种方法
			- **留出法**：直接将数据集D划分为两个互斥的集合。一个集合训练集，另一个作为测试集。但是**训练/测试集的划分尽可能保持数据分布的一致性**。尽量分层采样，避免出现训练集和测试集都是正例和反例的情况，尽量1：1。单次留出法得出的估计结果不够稳定可靠，在使用留出法时，一般要采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。**常见做法大约2/3~4/5的样本用于训练**。
			- **交叉验证法**：**划分子集，k-1个子集训练剩下测试。** 将数据集D划分为K个大小相似的互斥子集。每个子集都需要尽可能保持数据分布的一致性（从D中通过分层采样得到），然后每次用k-1个子集的并集作为训练集。余下的那个子集作为测试集。
			- **自助法**：抽N个放N个，重复多次。适用于数据集较小的时候。
	- **性能度量**：**衡量模型泛化能力的评价标准**。使用不同的性能度量往往会导致不同的评判结果。**评价一个模型好坏，不仅取决于算法和数据，还和任务需求有关。**
		- 回归任务：
			- 通过**均方误差**衡量模型的泛化能力。也是最常用的性能度量。
				- 均方误差：$$E(f;D)=\frac{1}{m}\sum_{i=1}^m (f(x_i)-y_i)^2$$
				- 对于数据分布D和概率密度函数p(.),均方误差可以描述为
				- $$E(f;D)=\begin{matrix} \int_{x \sim D}^{}(f(x)-y)^2p(x)dx \end{matrix}$$
			- **错误率与精度**：
				- 错误率：分类错误的样本数占样本总数的比例。
				- 精度：分类正确的样本数占总样本数的比例。
			- **查准率，查全率和F1**：
				- $$\begin{matrix}真实情况&正例(预测)&反例(预测)\\正例&TP(真正例)&FN(假反例)\\反例&FP(假正例)&TN(真反例)\\\end{matrix}$$
				- 查准率（P）：模型预测为正类别的样本中，真正是正类别的比例。
					- P=TP/TP+FP
				- 查全率（R）：模型成功捕捉到的正类别样本占所有实际正类别样本的比例。
					- R=TP/TP+FN
				- **查全率和查准率是一对矛盾的，一般来说，查准率高，查全率偏低。查全率高，查准率偏低。**，通常只有简单的任务才有可能出现查全率和查准率都高。
				- 通过不同学习模型在样本上的**P-R图（查全率和查准率）** 可以知道一些信息。
					- 一个学习模型的P-R曲线被另一个学习模型的曲线完全**包住**的话，就可以断言后者的性能优于前者。
					- 如果两个学习模型的P-R曲线发生交叉，难以断言两者孰优孰劣。但是如何一定要进行判断哪个好的话，通过**平衡点**
						- **平衡点（BEP）**：查准率 == 查全率时的取值。平衡点大的就好点。
						- BEP过于简化，最常用的时**F1度量**
						- $$F1=\frac {2×P×R}{P+R}=\frac{2×TP}{样例总数+TP-TN}$$
						- 一般形式为$$F_β=\frac{(1+β)^2×P×R}{(β^2×P)+R}$$
							- 当β=1,和F1相等。当β>1，对查全率有更大影响，当β<1，对查准率有更大影响。
						- 对于有n个二分类混淆矩阵，综合考察查准率和查全率
							- 在每个混淆矩阵中，分别计算出查准率和查全率$$(P_1,R_1),(P_2,R_2),....(P_n,R_n)$$在计算平均值。可以得到
								- **宏查准率（macro-P）**：$$macroP=\frac1n\sum_{i=1}^n P_i$$
								- **宏查全率**：$$macroR=\frac1n\sum_{i=1}^n R_i$$
								- **宏F1**：$$macro-F1=\frac{2×macroP×macroR}{macroP+macroR}$$
							- 也可以将各个混淆矩阵的对应元素进行平均，得到TP，FP，TN，FN的平均值，在通过平均值求出**微查准率(micro-P)，微查全率(micro-R)和微F1(micro-F1)**.
								- **微查准率(micro-P)**:$$microP=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$$
								- **微查全率(micro-R)**:$$microR=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$$
								- **微F1(micro-F1)**:$$microF1=\frac{2×microP×microR}{microP+microR}$$
			- **ROC和AUC**：
				- 大部分学习模型为测试样本产生一个**实值和概率预测**，将产生的值和**分类阈值（threshord）** 进行比较。大于阈值就是正类，否则为反类。
				 -  **真正例率**：$$TPR=\frac{TP}{TP+FN}$$
				- **假正例率**：$$FPR=\frac{FP}{TN+FP}$$
				- 因为只有有限个测试样例来绘制ROC图，所以仅能获得有限个坐标对。
				- ROC曲线是一种以**真正例率（True Positive Rate，也称为召回率）为纵轴**，**假正例率（False Positive Rate）。为横轴**的图形表示。
				- AUC是ROC曲线下的面积，用于量化模型在所有可能分类阈值下的性能。AUC的取值范围在0到1之间，其中1表示模型的性能完美，0.5表示模型性能等同于随机猜测。
					- **一个学习器ROC曲线被另一个学习器的ROC曲线完全包住**，可以说后者的性能优于前者。
					- **一个ROC曲线和另一个学习器的曲线发生交叉**，判断两者孰优孰劣，通过**ROC曲线下的面积，即AUC（Area UnderROC Curve）**。
			- **代价敏感错误率与代价曲线**：
				- 为权衡不同类型错误所造成的不同损失，为错误赋予“**非均等代价**”。
				- 之前我们的错误率是根据错误次数占总次数多少来算的，**已经隐式的假设了均等代价（每次错误所付出的代价是一样的）** 。
				- **但是我们的模型要的是最小化“总体代价”**
				- **在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价。** 而是需要通过**代价曲线（cost curve）** 达到目的。
	- **比较检验**：有了评估方法和性能度量，评估得到的结果如何与泛化能力的评价标准（即性能度量）进行比较？
		- 因为存在三个重要的因素所以比较的方法也较为复杂，哪三个重要的因素呢？
			- **我们需要得到泛化性能，但是评估方法是通过得到测试集的误差近似作为泛化误差，两者的对比结果不一定相同**
			- **评估方面是使用测试集的，所以性能会与测试集本身的选择有很大关系【（使用大小不同的测试集会得到不同的结果）（使用大小相同的测试集，然而测试样例不同也会得到不同结果）】**
			- **很多机器学习算法本身存在一定的随机性（即使用相同的参数设置在同一个测试集上多次运行，结果也会不同）**
		- 存在着以上三个重要的因素，是否存在一个适当的方法对学习器性能进行比较？
			- **统计假设检验**：若在测试集上观察到学习器A比B号，则泛化性能A比B好？以及这个结论的把握有多大？**两种最基本的假设检验，以错误率为性能度量**。
				- **假设检验**：
				- **交叉验证t检验**：
			- **McNemar检验**：
	- **偏差与方差**：